---
title: Arbitrum Full Documentation (Selected Folders)
---

# stylus Folder

## .mdx (cli-tools-overview.md)
---
id: cli-tools-overview
title: CLI Tools (cargo-stylus)
sidebar_label: CLI tools overview
---

The CLI tools provided for Stylus, specifically the `cargo-stylus` tool, are designed to help developers manage, compile, and optimize their Stylus contracts efficiently. This overview provides a summary of the tools available and how to use them effectively.

## Available tools

### 1. Optimize WASM binaries

The `cargo-stylus` tool allows you to optimize WebAssembly (WASM) binaries, ensuring that your contracts are as efficient as possible.

- **[Optimize WASM binaries](/stylus/how-tos/optimizing-binaries.mdx):** Learn how to optimize your WASM binaries for performance and size.

### 2. Debug Stylus transactions

Gain insights into your Stylus contracts by debugging transactions.

- **[Debug Stylus transactions](/stylus/how-tos/debugging-tx.mdx):** A guide to debugging transactions, helping you identify and fix issues.

### 3. Verify contracts

Ensure that your Stylus contracts are correctly verified.

- **[Verify contracts](/stylus/how-tos/verifying-contracts.mdx):** Step-by-step instructions on how to verify your contracts using `cargo-stylus`.

## Source code repository

The source code for `cargo-stylus` is available on GitHub. Explore the code, contribute, or use it as a reference.

- **[cargo-stylus repository](https://github.com/OffchainLabs/stylus):** Visit the GitHub repository for more information.

## Additional resources

For more advanced usage and detailed guides, refer to the following resources:

- **[Optimize WASM binaries](/stylus/how-tos/optimizing-binaries.mdx)**
- **[Troubleshooting](/stylus/troubleshooting-building-stylus.md)**
- **[Run a Stylus dev node](/run-arbitrum-node/03-run-local-full-chain-simulation.mdx)**

This overview page serves as the starting point for mastering the CLI tools available for Stylus development. From optimizing your contracts to debugging and verifying them, the `cargo-stylus` toolset is integral to a smooth development experience.

---

## .mdx (concepts/gas-metering.mdx)
---
title: 'Gas metering'
description: 'A conceptual overview of gas and ink, the primitives that Stylus uses to measure the cost of WASM activation, compute, memory, and storage.'
author: rachel-bousfield
sme: rachel-bousfield
target_audience: 'Developers deploying smart contracts using Stylus.'
sidebar_position: 3
---

**Gas and ink** are the pricing primitives that are used to determine the cost of handling specific opcodes and host I/Os on Stylus. For an overview of specific opcode and host I/O costs, see [Gas and ink costs](/stylus/reference/opcode-hostio-pricing).

<!-- todo as a future optimization: pull the "Ink and gas" content up top; give devs what they need to know in order to build stuff - the "just in time" information - progressively disclose the "just in case" details. An example flow:
   1. "think of gas and ink as dollars and cents"
   2. "here's what you need to know about how it works, how it differs from traditional Solidity dApp cost modeling"
   3. "here's where costs are incurred along the lifecycle, simply said"
   4. "here's where costs are incurred along the lifecycle, with more technical precision

   If devs need more guidance, we could author a how-to titled "Measure and manage your gas costs"
-->

## Stylus gas costs

Stylus introduces new pricing models for WASM programs. Intended for high-compute applications, Stylus makes the following more affordable:

- Compute, which is generally **10-100x** cheaper depending on the program. This is primarily due to the efficiency of the WASM runtime relative to the EVM, and the quality of the code produced by Rust, C, and C++ compilers. Another factor that matters is the quality of the code itself. For example, highly optimized and audited C libraries that implement a particular cryptographic operation are usually deployable without modification and perform exceptionally well. The fee reduction may be smaller for highly optimized Solidity that makes heavy use of native precompiles vs. an unoptimized Stylus equivalent that doesn't do the same.
- Memory, which is **100-500x** cheaper due to Stylus's novel exponential pricing mechanism intended to address Vitalik's concerns with the EVM's per-call, [quadratic memory pricing policy](https://notes.ethereum.org/@vbuterin/proposals_to_adjust_memory_gas_costs). For the first time ever, high-memory applications are possible on an EVM-equivalent chain.
- Storage, for which the Rust SDK promotes better access patterns and type choices. Note that while the underlying <a href="https://www.evm.codes/#54"><code>SLOAD</code></a> and <a href="https://www.evm.codes/#55"><code>SSTORE</code></a> operations cost as they do in the EVM, the Rust SDK implements an optimal caching policy that minimizes their use. Exact savings depends on the program.
- VM affordances, including common operations like `keccak` and reentrancy detection. No longer is it expensive to make safety the default.

There are, however, minor overheads to using Stylus that may matter to your application:

- The first time a WASM is deployed, it must be _activated_. This is generally a few million gas, though to avoid testnet DoS, we've set it to a fixed 14 million. Note that you do not have to activate future copies of the same program. For example, the same NFT template can be deployed many times without paying this cost more than once. We will soon make the fees paid depend on the program, so that the gas used is based on the complexity of the WASM instead of this very conservative, worst-case estimate.
- Calling a Stylus contract costs 128-2048 gas. We're working with Wasmer to improve setup costs, but there will likely always be some amount of gas one pays to jump into WASM execution. This means that if a contract does next to nothing, it may be cheaper in Solidity. However if a contract starts doing interesting work, the dynamic fees will quickly make up for this fixed-cost overhead.

Though conservative bounds have been chosen for testnet, all of this is subject to change as pricing models mature and further optimizations are made. Since gas numbers will vary across updates, it may make more sense to clock the time it takes to perform an operation rather than going solely by the numbers reported in receipts.

## Ink and gas

Because WASM opcodes are orders of magnitude faster than their EVM counterparts, almost every operation that Stylus does costs less than `1 gas`. “Fractional gas” isn’t an EVM concept, so the Stylus VM introduces a new unit of payment known as ink that’s orders of magnitude smaller.

```jsx
1 gas = 10,000 ink
```

### Intuition

To build intuition for why this is the case, consider the `ADD` instruction.

#### In the EVM

1. Pay for gas, requiring multiple look-ups of an in-memory table
2. Consider tracing, even if disabled
3. Pop two items of the simulated stack
4. Add them together
5. Push the result

#### In the Stylus VM

1. Execute a single x86 or ARM `ADD` instruction

Note that unlike the EVM, which charges for gas before running each opcode, the Stylus VM strategically charges for many opcodes all at once. This cuts fees considerably, since the VM only rarely needs to execute gas charging logic. Additionally, gas charging happens _inside the program_, removing the need for an in-memory table.

### The ink price

The ink price, which measures the amount of ink a single EVM gas buys, is configurable by the chain owner. By default, the exchange rate is `1:10000`, but this may be adjusted as the EVM and Stylus VM improve over time.

For example, if the Stylus VM becomes 2x faster, instead of cutting the nominal cost of each operation, the ink price may instead be halved, allowing 1 EVM gas to buy twice as much ink. This provides an elegant mechanism for smoothly repricing resources between the two VMs as each makes independent progress.

### User experience

It is important to note that users never need to worry about this notion of ink. Receipts will always be measured in gas, with the exchange rate applied automatically under the hood as the VMs pass execution back and forth.

However, developers optimizing contracts may choose to measure performance in ink to pin down the exact cost of executing various routines. The <a href="https://docs.rs/stylus-sdk/0.3.0/stylus_sdk/evm/fn.ink_left.html"><code>ink_left</code></a> function exposes this value, and various methods throughout the Rust SDK optionally accept ink amounts too.

### See also

- [Gas and ink costs](/stylus/reference/opcode-hostio-pricing): Detailed costs per opcode and host I/O
- [Caching strategy](/stylus/how-tos/caching-contracts): Description of the Stylus caching strategy and the `CacheManager` contract

---

## .mdx (concepts/how-it-works.md)
---
id: how-it-works
title: 'Architecture overview'
sidebar_label: 'Architecture overview'
description: 'Learn what powers Stylus'
author: srinjoyc
SME: srinjoyc
user_story: As an Ethereum developer/project owner, I need to vet the Stylus.
content_type: concept
---

import { VanillaAdmonition } from '@site/src/components/VanillaAdmonition/';

There are four main steps for bringing a Stylus program to life: **coding, activation, execution, and proving**.

## Coding

Developers can now write smart contracts in any programming language that compiles to WASM.

:::note
Some high-level languages generate far more performant WASMs than others.
:::

Currently, there is support for Rust, C, and C++. However, the levels of support vary. Rust has rich language support from day one, with an open-source SDK that makes writing smart contracts in Rust as easy as possible. C and C++ are supported off the bat, enabling the deployment of existing contracts in those languages onchain with minimal modifications.

The Stylus SDK for Rust contains the smart contract development framework and language features most developers will need to use in Stylus. The SDK also makes it possible to perform all EVM-specific functionalities that smart contract developers use. Check out the [Rust SDK Guide](/stylus/reference/rust-sdk-guide.md) and the [Crate Docs](https://docs.rs/stylus-sdk/latest/stylus_sdk/index.html).

## Activation

Starting from a high-level language (such as Rust, C, or C++), the first compilation stage happens using the CLI provided in the Stylus SDK for Rust or any other compiler, such as Clang for C and C++. Once compiled, the WASM is posted onchain. Then, in an activation process, WASM gets lowered to a node's native machine code (such as ARM or x86).

Activating a Stylus program requires a new precompile, ArbWasm. This precompile produces efficient binary code tailored to a node's native assembly. During this step, a series of middlewares ensure that user programs execute safely and are deterministically fraud-proven. Instrumentation includes gas metering, depth-checking, memory charging, and more to guarantee all WASM programs are safe for the chain to execute. Stylus contracts can be called only after activation.

Gas metering is essential for certifying that computational resources are paid for. In Stylus, the unit for measuring cost is called **ink**, which is similar to Ethereum's gas but thousands of times smaller. There are two reasons why a new measurement is used: First, WASM execution is so much faster than the EVM that executing thousands of WASM opcodes could be done in the same amount of time it takes the EVM to execute one. Second, the conversion rate of ink to gas can change based on future hardware or VM improvements. For a conceptual introduction to Stylus gas and ink, see [gas and ink (Stylus)](/stylus/concepts/gas-metering.mdx).

<VanillaAdmonition type="note">
  Stylus smart contracts will need to be reactivated once per year (365 days) or whenever an upgrade
  to Stylus (which will always involve an ArbOS upgrade), even if they are in the cache. To complete
  this reactivation, you can use
  [`cargo-stylus`](https://docs.arbitrum.io/stylus/using-cli#cargo-stylus-commands-reference) or
  directly through the [ArbWasm
  precompile](https://docs.arbitrum.io/build-decentralized-apps/precompiles/reference#arbwasm). If
  contracts do not get reactivated, they will no longer be callable.
</VanillaAdmonition>

## Execution

Stylus programs execute in a fork of [Wasmer](https://wasmer.io/), the leading WebAssembly runtime, with minimal changes to optimize their codebase for blockchain-specific use cases. Wasmer executes native code much faster than <a data-quicklook-from="geth">Geth</a> executes EVM bytecode, contributing to the significant gas savings that Stylus provides.

EVM contracts continue to execute the same way they were before Stylus. When calling a contract, the difference between an EVM contract and a WASM program is visible via an [EOF](https://notes.ethereum.org/@ipsilon/evm-object-format-overview)-inspired contract header. From there, the contract executes using its corresponding runtime. Contracts written in Solidity and WASM languages can make cross-contract calls to each other, meaning a developer never has to consider which language the contract is in. Everything is interoperable.

## Proving

Nitro operates in two modes: a "happy case" where it compiles execution history to native code, and a "sad case" during validator disputes, where it compiles execution history to WASM for interactive fraud proofs on Ethereum. Stylus builds on Nitro's fraud-proving technology, allowing it to verify both execution history and WASM programs deployed by developers.

Stylus is made possible by Nitro’s ability to replay and verify disputes using WASM. Validators bisect disputes until an invalid step is identified and proven onchain through a [“one-step proof.”](/how-arbitrum-works/05-validation-and-proving/03-proving-and-challenges.mdx#simplified-bisection-protocol). This deterministic fraud-proving capability ensures the correctness of any arbitrary program compiled to WASM. The combination of WASM's and Nitro's properties enables this technological leap we call Stylus.

For more details on Nitro’s architecture, refer to the [documentation](/how-arbitrum-works/01-a-gentle-introduction.mdx) or the [Nitro whitepaper](https://github.com/OffchainLabs/nitro/blob/master/docs/Nitro-whitepaper.pdf).

## Why does this matter?

Stylus innovates on many levels, with the key ones described here:

### One chain, many languages

There are roughly 20k Solidity developers, compared to three million Rust developers or 12 million C developers [[1](https://slashdatahq.medium.com/state-of-the-developer-nation-23rd-edition-the-fall-of-web-frameworks-coding-languages-711525e3df3a)]. Developers can now use their preferred programming language, which is interoperable on any <a data-quicklook-from="arbitrum-chain">Arbitrum chain</a> with Stylus. By onboarding the next million developers, scaling to the next billion users becomes possible.

### A better EVM

Stylus' MultiVM brings the best of both worlds. Developers still get all of the benefits of the EVM, including the ecosystem and liquidity, while getting efficiency improvements and access to existing libraries in Rust, C, and C++, all without changing anything about how the EVM works. EVM equivalence is no longer the ceiling; it's the floor.

### Cheaper execution

Stylus is a more efficient execution environment than the EVM, leading directly to gas savings for complex smart contracts. Computation and memory can be significantly cheaper. Deploying cryptography libraries can be done permissionlessly as custom application layer precompiles. Use cases that are impractical in the EVM are now possible in Stylus.

### Opt-in reentrancy

Stylus doesn't just improve on cost and speed. WASM programs are also safer. Reentrancy is a common vulnerability developers can only attempt to mitigate in Solidity. Stylus provides cheap reentrancy detection, and using the Rust SDK, reentrancy is disabled by default unless intentionally overridden.

### Fully interoperable

Solidity programs and WASM programs are completely composable. If working in Solidity, a developer can call a Rust program or rely on another dependency in a different language. If working in Rust, all Solidity functionalities are accessible out of the box.

---

## .mdx (concepts/public-preview-expectations.md)
---
title: 'Public preview: What to expect'
description: 'Stylus is currently tagged as a `release-candidate` supported by *public preview* documentation. This concept document explains what this means, and what to expect.'
author: symbolpunk
sidebar_position: 10
---

Stylus is currently tagged as a `release-candidate` supported by _public preview_ documentation. This concept document explains what "public preview" means, what to expect from public preview capabilities, and how to engage with our team as you tinker.

### How products are developed at Offchain Labs

Offchain Labs builds products in a way that aligns loosely with the spirit of "building in public". We like to release things **early and often** so that we can capture feedback and iterate in service of your needs, as empirically as possible.

To do this, some of our product offerings are documented with **public preview** disclaimers that look like this:

This banner's purpose is to set expectations while inviting readers like you to express your needs so that we can incorporate them into the way that we iterate on product.

### What to expect when using public preview offerings

As you tinker and provide feedback, we'll be listening. Sometimes, we'll learn something non-obvious that will result in a significant change. More commonly, you'll experience incremental improvements to the developer experience as the offering grows out of its **public preview** status, towards **stable** status.

Public preview offerings are evolving rapidly, so don't expect the degree of release notes discipline that you'd expect from a stable offering. Keep your eyes open for notifications regarding patch, minor, and major changes, along with corresponding relnotes that highlight breaking changes and new capabilities.

### How to provide feedback

Our product team primarily uses three feedback channels while iterating on public preview capabilities:

1.  **Docs**: Click on the **Request an update** button located in the top-right corner of any document to provide feedback on the docs and/or developer experience. This will lead you to a prefilled Github issue that members of our product team periodically review.
2.  **Discord**: [Join the Arbitrum Discord](https://discord.gg/arbitrum) to engage with members of the Arbitrum community and product team.
3.  **Google form**: Complete [this form](http://bit.ly/3yy6EUK) to ask for support.

### What to expect when providing feedback

Our ability to respond to feedback is determined by our ever-evolving capacity and priorities. We can't guarantee responses to all feedback submissions, but our small-but-mighty team is listening, and we'll try our best to acknowledge and respond to your feedback. No guarantees though!

:::info
[Our small-but-mighty team is hiring](https://jobs.lever.co/offchainlabs).
:::

### Thank you!

Thanks for helping us build things that meet your needs! We're excited to engage with OGs and newcomers alike; please don't hesitate to reach out.

---

## .mdx (gentle-introduction.mdx)
---
id: gentle-introduction
title: 'A gentle introduction to Stylus'
description: 'An educational introduction that provides a high-level understanding of Stylus, a new way to write EVM-compatible smart contracts using your favorite programming languages.'
author: amarrazza
sme: amarrazza
target_audience: 'Developers who want to build on Arbitrum using popular programming languages, like Rust'
sidebar_position: 1
---

import ImageZoom from '@site/src/components/ImageZoom';

# A gentle introduction: Stylus

### In a nutshell:

- Stylus lets you write smart contracts in programming languages that compile to WASM, such as **Rust, C, C++, and many others**, allowing you to tap into their ecosystem of libraries and tools. Rich language and tooling support already exist for Rust. You can try the SDK and CLI with the [quickstart](/stylus/quickstart.mdx).
- Solidity contracts and Stylus contracts are **fully interoperable**. In Solidity, you can call a Rust program and vice versa, thanks to a second, coequal WASM virtual machine.
- Stylus contracts offer significantly **faster execution and lower gas fees** for memory and compute-intensive operations, thanks to the superior efficiency of <a data-quicklook-from="wasm">WASM</a> programs.

### What's Stylus?

Stylus is an upgrade to Arbitrum Nitro [(ArbOS 32)](/run-arbitrum-node/arbos-releases/arbos32.mdx), the tech stack powering Arbitrum One, Arbitrum Nova, and Arbitrum chains. This upgrade adds a second, coequal virtual machine to the EVM, where EVM contracts continue to behave exactly as they would in Ethereum. We call this paradigm **MultiVM** since **everything is entirely additive.**

<ImageZoom src="/img/stylus-multivm.jpg" alt="Stylus gives you MultiVM" className="img-600px" />

This second virtual machine executes WebAssembly (WASM) rather than EVM bytecode. WASM is a modern binary format popularized by its use in major web standards, browsers, and companies to speed up computation. WASM is built to be fast, portable, and human-readable. It has sandboxed execution environments for security and simplicity. Working with WASM is nothing new for Arbitrum chains. Ever since the [Nitro upgrade](https://medium.com/offchainlabs/arbitrum-nitro-one-small-step-for-l2-one-giant-leap-for-ethereum-bc9108047450), WASM has been a fundamental component of Arbitrum's fully functioning fraud proofs.

With a WASM VM, any programming language compilable to WASM is within Stylus's scope. While many popular programming languages can compile into WASM, some compilers are more suitable for smart contract development than others, like Rust, C, and C++. Other languages like Go, Sway, Move, and Cairo are also supported. Languages that include their own runtimes, like Python and Javascript, are more complex for Stylus to support, although not impossible. Compared to Solidity, WASM programs are much more efficient for memory-intensive applications. There are many reasons for this, including the decades of compiler development for Rust and C. WASM also has a faster runtime than the EVM, resulting in faster execution. Third-party contributions in the form of libraries for new and existing languages are welcomed!

### Use Cases

While many developers will be drawn to new use cases, rebuilding existing applications in Stylus will also open the door to innovation and optimization. dApps have never been faster, cheaper, or safer. Stylus can integrate easily into existing Solidity projects by calling a Stylus contract to optimize specific parts of your dApp or building the entire dApp with Stylus. It's impossible to list all of the use cases Stylus enables; think about the properties of all WASM-compatible languages! That said, here are some particularly exciting ideas:

- **Efficient Onchain Verification with ZK-Proofs**: Enable cost-effective onchain verification
  using zero-knowledge proving systems for privacy, interoperability, and more (see [case
  study](https://blog.arbitrum.io/renegade-stylus-case-study/)).
- **Advanced DeFi Instruments**: Power complex financial instruments and processes like custom
  pricing curves for AMMs, synthetic assets, options, and futures with onchain computation via
  extending current protocols (i.e., Uniswap V4 hooks) or building your own.
- **High-Performance Onchain Logic**: Support memory and compute-intensive applications like
  onchain games and generative art either by writing all of the application in Stylus or enhance
  performance of existing Solidity contracts by optimizing specific parts.
- **Endless Possibilities**: Enable innovative use cases such as generative art, compute-heavy
  AI models, onchain games, and projects utilizing advanced cryptography, unlocking the full potential
  of resource-intensive applications onchain.

### Getting Started

1. Utilize our [quickstart](/stylus/quickstart.mdx), [Rust SDK](/stylus/reference/overview.md), to help you start building.
2. Join our Stylus Developer [Telegram](https://t.me/arbitrum_stylus) group and [Arbitrum Discord](https://discord.gg/arbitrum) for support as well as the official Arbitrum ([@Arbitrum](https://twitter.com/arbitrum)) and Arbitrum Developers ([@ArbitrumDevs](https://twitter.com/ArbitrumDevs)) X accounts for announcements.
3. Check out the [Awesome Stylus](https://github.com/OffchainLabs/awesome-stylus) repository for various community contributed Stylus projects and tools. If you build something useful, we'd be happy to add it there.
4. Stay updated with the latest from the Stylus community through tutorials, builder interviews, technical deep dives, and more with the [Stylus Saturdays](https://stylus-saturdays.com/) newsletter.

---

## .mdx (how-tos/adding-support-for-new-languages.mdx)
---
title: 'How to add a new programming language to Stylus'
description: 'Stylus shipped with native support for Rust and C, but you can add support for any programming language that compiles to WebAssembly. This how-to walks you through the process of extending Stylus with Zig support, allowing you to write EVM-compatible Zig smart contracts.'
author: rauljordan
sme: rauljordan
target_audience: 'Developers deploying smart contracts using Stylus'
content_type: how-to
sidebar_position: 1
---

[Arbitrum Stylus](../gentle-introduction.mdx) is a new technology developed for Arbitrum chains which gives smart contract developers superpowers. With Stylus, developers can write EVM-compatible smart contracts in many different programming languages, and reap massive performance gains. Stylus slashes fees, with performance gains ranging from 10-70x, and memory efficiency gains as high as 100-500x.

This is possible thanks to [WebAssembly](https://www.infoworld.com/article/3291780/what-is-webassembly-the-next-generation-web-platform-explained.html) technology, which all Stylus contracts compile to. Stylus smart contracts live under the **same Ethereum state trie** in Arbitrum nodes, and can fully interoperate with Solidity or Vyper EVM smart contracts. With Stylus, developers can write smart contracts in Rust that talk to Solidity and vice versa without any limitations.

Today, the Stylus testnet also comes with two officially supported [SDKs](/stylus/overview.mdx) for developers to write contracts in the [Rust](../reference/rust-sdk-guide.md) or [C](https://github.com/OffchainLabs/stylus-sdk-c) programming languages.

However, _anyone_ can add support for new languages in Stylus. **As long as a programming language can compile to WebAssembly**, Stylus will let you use it to write EVM-compatible smart contracts. Note that in order to be deployed onchain, your compiled program must fit under the 24Kb brotli-compressed limit, and should meet Stylus gas metering requirements.

In this document, we go over how we added support for the up-and-coming [Zig](https://ziglang.org/) programming language, which is meant to be a spiritual successor to C that comes with great performance and memory safety **within 20 lines of code**.

Why Zig?

1. Zig contains **memory safety guardrails**, requiring developers to think hard about manual memory allocation in a prudent manner
2. Zig is a **C equivalent** language, and its tooling is also a C compiler. This means C projects can incrementally adopt Zig when refactoring
3. Zig is **lightning fast** and produces **small binaries**, making it suitable for blockchain applications

Programs written in Zig and deployed to Stylus have a tiny footprint and will have gas costs comparable, if not equal to, C programs.

## Requirements

- Download and install [Zig 0.11.0](https://ziglang.org/downloads)
- Install [Rust](https://www.rust-lang.org/tools/install), which we'll need for the [Stylus CLI tool](https://github.com/OffchainLabs/cargo-stylus) to deploy our program to the Stylus testnet

We'll also be using Rust to run an example script that can call our Zig contract on the Stylus testnet using the popular [ethers-rs](https://github.com/gakonst/ethers-rs) library.

Once Rust is installed, also install the Stylus CLI tool with

```bash
RUSTFLAGS="-C link-args=-rdynamic" cargo install --force cargo-stylus
```

## Using Zig with Stylus

First, let's clone the repository:

```bash
git clone https://github.com/offchainlabs/zig-on-stylus && cd zig-on-stylus
```

then delete everything inside of `main.zig`. We'll be filling it out ourselves in this tutorial.

To support Stylus, your Zig programs need to define a special entrypoint function, which takes in the length of its input args, `len`, and returns a status code `i32`, which is either 0 or 1. We won't need the Zig standard library for this.

One more thing it needs is to use a special function, called `memory_grow` which can allocate memory for your program. This function is _injected_ into all Stylus contracts as an external import. Internally, we call these `vm_hooks`, and also refer to them as `host-io's`, because they give you access to the host, EVM environment.

Go ahead and replace everything in your `main.zig` function with:

```c
pub extern "vm_hooks" fn memory_grow(len: u32) void;

export fn mark_unused() void {
    memory_grow(0);
    @panic("");
}

// The main entrypoint to use for execution of the Stylus WASM program.
export fn user_entrypoint(len: usize) i32 {
    _ = len;
    return 0;
}
```

At the top, we declare the `memory_grow` external function for use.

Next, we can build our Zig library to a freestanding WASM file for our onchain deployment:

```bash
zig build-lib ./src/main.zig -target wasm32-freestanding -dynamic --export=user_entrypoint -OReleaseSmall --export=mark_unused
```

This is enough for us to deploy on the Stylus testnet! We'll use the [Stylus CLI tool](https://github.com/OffchainLabs/cargo-stylus), which you installed earlier using `cargo install`:

```
cargo stylus deploy --private-key=<YOUR_TESTNET_PRIVKEY> --wasm-file=main.wasm
```

The tool will send two transactions: one to deploy your Zig contract's code onchain, and the other to activate it for usage.

```
Uncompressed WASM size: 112 B
Compressed WASM size to be deployed onchain: 103 B
```

You can see that our Zig program is _tiny_ when compiled to WASM. Next, we can call our contract to make sure it works using any of your favorite Ethereum tooling. In this example below, we use the `cast` CLI tool provided by [foundry](https://github.com/foundry-rs/foundry). The contract above has been deployed to the Stylus testnet at address `0xe0CD04EA8c148C9a5A58Fee1C895bc2cf6896799`.

```
export ADDR=0xe0CD04EA8c148C9a5A58Fee1C895bc2cf6896799
cast call --rpc-url 'https://stylus-testnet.arbitrum.io/rpc' $ADDR '0x'
```

Calling the contract via RPC should simply return the value `0` as we programmed it to.

```
0x
```

### Reading input and writing output data

Smart contracts on Ethereum, at the bare minimum, can take in data and output data as bytes. Stylus contracts are no different, and to do anything useful, we need to be able to read from user input also write our output to the caller. To do this, the Stylus runtime provides all Stylus contracts with two additional, useful host-ios:

```c
pub extern "vm_hooks" fn read_args(dest: *u8) void;
pub extern "vm_hooks" fn write_result(data: *const u8, len: usize) void;
```

Add these near the top of your `main.zig` file.

The first, `read_args` takes in a pointer to a byte slice where the input arguments will be written to. The length of this byte slice must equal the length of the program args received in the `user_entrypoint`. We can write a helper function that uses this vm hook and gives us a byte slice in Zig we can then operate on.

```c
// Allocates a Zig byte slice of length=`len` reads a Stylus contract's calldata
// using the read_args hostio function.
pub fn input(len: usize) ![]u8 {
    var input = try allocator.alloc(u8, len);
    read_args(@ptrCast(*u8, input));
    return input;
}
```

Next, we implement a helper function that outputs the data bytes to the Stylus contract's caller:

```c
// Outputs data as bytes via the write_result hostio to the Stylus contract's caller.
pub fn output(data: []u8) void {
    write_result(@ptrCast(*u8, data), data.len);
}
```

Let's put these together:

```c
// The main entrypoint to use for execution of the Stylus WASM program.
// It echoes the input arguments to the caller.
export fn user_entrypoint(len: usize) i32 {
    var in = input(len) catch return 1;
    output(in);
    return 0;
}
```

We're almost good to go, let's try to compile to WASM and deploy to the Stylus testnet. Let's run our build command again:

```
src/main.zig:21:20: error: use of undeclared identifier 'allocator'
    var data = try allocator.alloc(u8, len);
                   ^~~~~~~~~
```

Oops! Looks like we need an allocator to do our job here. Zig, as a language, requires programmers to think carefully about memory allocation and it's a typical pattern to require them to manually provide an allocator. There are many to choose from, but the Zig standard library already has one built specifically for WASM programs. Memory in WASM programs grows in increments of 64Kb, and the allocator from the stdlib has us covered here.

Let's try to use it by adding the following to the top of our `main.zig`

```c
const std = @import("std");
const allocator = std.heap.WasmAllocator;
```

Our code compiles, but will it deploy onchain? Run `cargo stylus check --wasm-file=main.wasm` and see:

```
Caused by:
    missing import memory_grow
```

What's wrong? This means that the `WasmAllocator` from the Zig standard library should actually be using our special `memory_grow` hostio function underneath the hood. We can fix this by copying over the WasmAllocator.zig file from the standard library, and modifying a single line to use `memory_grow`.

You can find this file under `WasmAllocator.zig` in the OffchainLabs/zig-on-stylus repository. We can now use it:

```c
const std = @import("std");
const WasmAllocator = @import("WasmAllocator.zig");

// Uses our custom WasmAllocator which is a simple modification over the wasm allocator
// from the Zig standard library as of Zig 0.11.0.
pub const allocator = std.mem.Allocator{
    .ptr = undefined,
    .vtable = &WasmAllocator.vtable,
};
```

Building again and running `cargo stylus check` should now succeed:

```bash
Uncompressed WASM size: 514 B
Compressed WASM size to be deployed onchain: 341 B
Connecting to Stylus RPC endpoint: https://stylus-testnet.arbitrum.io/rpc
Stylus program with same WASM code is already activated onchain
```

Let's deploy it:

```
cargo stylus deploy --private-key=<YOUR_TESTNET_PRIVKEY> --wasm-file=main.wasm
```

Now if we try to call it, it will output whatever input we send it, like an echo. Let's send it the input 0x123456:

```
export ADDR=0x20Aa65a9D3F077293993150C0345f62B50CCb549
cast call --rpc-url 'https://stylus-testnet.arbitrum.io/rpc' $ADDR '0x123456'

0x123456
```

Works!

## Prime number checker implementation

Let's build something a little bit fancier: this time we'll implement a primality checker in Zig using an ancient algorithm called the [sieve of erathosthenes](https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes). Given a number, our contract will output 1 if it is prime, or 0 otherwise. We'll implement in a pretty naive way, but leverage one of Zig's awesome features: [comptime](https://kristoff.it/blog/what-is-zig-comptime/).

The `comptime` keyword tells the Zig compiler to evaluate the code involved at compile time, allowing you to define computation that would normally make runtime more expensive and do it while your binary is being compiled! Comptime in Zig is extremely flexible. In this example, we use it to define a slice of booleans up to a certain limit at compile time, which we'll use to mark which numbers are prime or not.

```c
fn sieve_of_erathosthenes(comptime limit: usize, nth: u16) bool {
    var prime = [_]bool{true} ** limit;
    prime[0] = false;
    prime[1] = false;
    var i: usize = 2;
    while (i * i < limit) : (i += 1) {
        if (prime[i]) {
            var j = i * i;
            while (j < limit) : (j += i)
                prime[j] = false;
        }
    }
    return prime[nth];
}
```

Checking if a number `N` is prime would involve just checking if the value at index `N` in this `prime` boolean slice is `true`. We can then integrate this function into our `user_entrypoint`:

```c
// The main entrypoint to use for execution of the Stylus WASM program.
export fn user_entrypoint(len: usize) i32 {
    // Expects the input is a u16 encoded as little endian bytes.
    var input = args(len) catch return 1;
    var check_nth_prime = std.mem.readIntSliceLittle(u16, input);
    const limit: u16 = 10_000;
    if (check_nth_prime > limit) {
        @panic("input is greater than limit of 10,000 primes");
    }
    // Checks if the number is prime and returns a boolean using the output function.
    var is_prime = sieve_of_erathosthenes(limit, check_nth_prime);
    var out = input[0..1];
    if (is_prime) {
        out[0] = 1;
    } else {
        out[0] = 0;
    }
    output(out);
    return 0;
}
```

Let's check and deploy it:

```
Uncompressed WASM size: 10.8 KB
Compressed WASM size to be deployed onchain: 525 B
```

Our uncompressed size is big because of that giant array of booleans, but the program is highly compressible because all of them are zeros!

An instance of this program has been deployed to the Stylus testnet at address `0x0c503Bb757b1CaaD0140e8a2700333C0C9962FE4`

## Interacting With Stylus contracts Using Ethers-rs

An example is included in this repo under `rust-example` which uses the popular [ethers-rs](https://github.com/gakonst/ethers-rs) library to interact with our prime sieve contract on the Stylus testnet. To run it, do:

```
export STYLUS_PROGRAM_ADDRESS=0x0c503Bb757b1CaaD0140e8a2700333C0C9962FE4
cargo run
```

...and see:

```
Checking if 2 is_prime = true, took: 404.146917ms
Checking if 3 is_prime = true, took: 154.802083ms
Checking if 4 is_prime = false, took: 123.239583ms
Checking if 5 is_prime = true, took: 109.248709ms
Checking if 6 is_prime = false, took: 113.086625ms
Checking if 32 is_prime = false, took: 280.19975ms
Checking if 53 is_prime = true, took: 123.667958ms
```

## Next steps

The hostios defined in this walkthrough are not the only ones! Check out our [stylus-sdk-c](https://github.com/OffchainLabs/stylus-sdk-c) to see all the hostios you can use under `hostio.h`. These include affordances for the EVM, utilities to access storage, and utilities to call other Arbitrum smart contracts.

---

## .mdx (how-tos/caching-contracts.mdx)
---
id: 'caching-contracts'
title: 'Caching contracts with Stylus'
description: 'A conceptual overview of the Stylus caching strategy and CacheManager contract, and a practical guide for using its functionality.'
sme: mahsa-moosavi
target_audience: 'Developers deploying smart contracts using Stylus.'
sidebar_position: 3
---

<a data-quicklook-from="stylus">Stylus</a> is designed for fast computation and efficiency. However,
the initialization process when entering a contract can be resource-intensive and time-consuming.

This initialization process, if repeated frequently, may lead to inefficiencies. To address this, we have implemented a caching strategy. By storing frequently accessed contracts in memory, we can avoid repeated initializations. This approach saves resources and time, significantly enhancing the speed and efficiency of contract execution.

## CacheManager contract

The core component of our caching strategy is the [`CacheManager` contract](https://github.com/OffchainLabs/nitro-contracts/blob/main/src/chain/CacheManager.sol). This smart contract manages the cache, interacts with precompiles, and determines which contracts should be cached. The `CacheManager` can hold approximately 4,000 contracts in memory.

The `CacheManager` defines how contracts remain in the cache and how they compete with other contracts for cache space. Its primary purpose is to reduce high initialization costs, ensuring efficient contract activation and usage. The contract includes methods for adding and removing cache entries, querying the status of cached contracts, and managing the lifecycle of cached data.

### Key features

The `CacheManager` plays a crucial role in our caching strategy by keeping a specific set of contracts in memory rather than retrieving them from disk. This significantly reduces the activation time for frequently accessed contracts. The `CacheManager` contract is an onchain contract that accepts bids for inserting contract code into the cache. It then calls a precompile that loads or unloads the contracts in the `ArbOS` cache, which follows the onchain cache but operates locally in the client and marks the contract as in or out of the cache in the `ArbOS` state.

The cache operates through an auction system where dApp developers submit bids to insert their contracts into the cache. If the cache is at capacity, lower bids are evicted to make space for higher bids. The cache maintains a minimum heap of bids for `codeHashes`, with bids encoded as `bid << 64 + index`, where `index` represents the position in the list of all bids. When an insertion exceeds the cache's maximum size, items are popped off the minimum heap and deleted until there is enough space to insert the new item. Contracts with equal bids will be popped in a random order, while the smallest bid is evicted first.

To ensure that developers periodically pay to maintain their position in the cache, we use a global decay parameter computed by `decay = block.timestamp * _decay`. This inflates the value of bids over time, making newer bids more valuable.

### Cache access and costs

During activation, we compute the contract's initialization costs for both non-cached and cached initialization. These costs take into account factors such as the number of functions, types, code size, data length, and memory usage. It's important to note that accessing an uncached contract does not automatically add it to the `CacheManager`'s cache. Only explicit calls to the `CacheManager` contract will add a contract to the cache. If a contract is removed from the cache, calling the contract becomes more expensive unless it is re-added.

To see how much gas contract initialization would cost, you need to call `programInitGas(address)` from the [ArbWasm precompile](https://github.com/OffchainLabs/nitro/blob/d906798140e562500beb9005d2503b0272852298/precompiles/ArbWasm.go). This function returns both the initialization cost when the contract is cached and when it is not.

### How to use the CacheManager API

This section provides a practical guide for interacting with the `CacheManager` contract API, either directly or through the `cargo stylus` command-line tool.

## Step 1: Determine the minimum bid

Before placing a bid, it's important to know the minimum bid required to cache the Stylys contract. This can be done using the `getMinBid` function, or using the `cargo stylus cache suggest-bid` command.

### Method 1: Direct smart contract call

```solidity
uint192 minBid = cacheManager.getMinBid(contractAddress);
```

### Method 2: Cargo stylus command

Here, the [`contractAddress`] is the address of the Stylus contract you want to cache.

```bash
cargo stylus cache suggest-bid [contractAddress]
```

## Step 2: Place a bid

You can place a bid using either of the following methods:

### Method 1: Direct smart contract call

Here, `bidAmount` is the amount you want to bid, and `contractAddress` is the address of the Stylus contract you're bidding for.

```solidity
cacheManager.placeBid{value: bidAmount}(contractAddress);
```

### Method 2: Cargo stylus command

You can place a bid using the `cargo stylus cache bid` command:

```bash
cargo stylus cache bid <--private-key-path <PRIVATE_KEY_PATH>|--private-key <PRIVATE_KEY>|--keystore-path <KEYSTORE_PATH>> [contractAddress] [bidAmount]
```

- `[contractAddress]`: The address of the Stylus contract you want to cache.
- `[bidAmount]`: The amount you want to bid. If not specified, the default bid is 0.

If you specify a bid amount using `cargo stylus`, it will automatically validate that the bid is greater than or equal to the result of the `getMinBid` function. If the bid is insufficient, the command will fail, ensuring that only valid bids are placed.

## Step 3: Check cache status

To check if a specific address is cached, you can use the `cargo stylus status` command:

```bash
cargo stylus cache status --address=[contractAddress]
```

### Additional information

- **Pausing Bids**: The `CacheManager` contract has an `isPaused` state that can be toggled by the owner to prevent or allow new bids.
- **Checking Cache Size**: You can monitor the current cache size and decay rate using the `getCacheSize` and `getDecayRate` functions respectively.

By following these steps, you can effectively interact with the `CacheManager` contract, either directly through smart contract calls or using the `cargo stylus` command-line tool. This ensures that your bids meet the necessary requirements for caching programs on the network, optimizing your contracts for faster and more efficient execution.

---

## .mdx (how-tos/debugging-tx.mdx)
---
id: debugging-tx
title: 'How to debug Stylus transactions using Cargo Stylus Replay'
description: 'This how-to explains how to perform trace calls and use GDB to replay and debug Stylus transactions, providing detailed analysis and troubleshooting.'
author: mahsamoosavi
sme: mahsamoosavi
target_audience: 'Developers deploying smart contracts using Stylus'
content_type: how-to
sidebar_position: 2
---

Debugging smart contracts can be challenging, especially when dealing with complex transactions. The `cargo-stylus` crate simplifies the debugging process by allowing developers to replay Stylus transactions. This tool leverages GDB to provide an interactive debugging experience, enabling developers to set breakpoints, inspect state changes, and trace the execution flow step-by-step. This capability is crucial for identifying and resolving issues, ensuring that smart contracts function correctly and efficiently.

### Overview

Cargo Stylus is a tool designed to simplify the development and debugging process for smart contracts written in Rust for the Stylus execution environment. One of its powerful features is the `cargo stylus` subcommand, which provides essential functionalities for developers:

1. **Trace transactions**: Perform trace calls against Stylus transactions using Ethereum nodes' `debug_traceTransaction` RPC. This feature enables developers to analyze the execution flow and state changes of their transactions in a detailed manner.
2. **Debugging with GDB or LLDB**: Replay and debug the execution of a Stylus transaction using a debugger. This allows developers to set breakpoints, inspect variables, and step through the transaction execution line by line, providing an in-depth understanding of the transaction's behavior.

### Replaying transactions

#### Requirements

- **Rust** (version 1.77 or higher)
- **Crate**: `cargo-stylus`
- **GNU Debugger (GDB)** (Linux) or **LLDB** (MacOS)
- **[Cast](https://book.getfoundry.sh/cast/)** (an Ethereum CLI tool)
- **[Arbitrum RPC Provider](#rpc-endpoint-compatibility)** with tracing endpoints enabled or a [local Stylus dev node](/run-arbitrum-node/run-nitro-dev-node)

`cargo stylus replay` allows users to debug the execution of a Stylus transaction using [GDB](https://sourceware.org/gdb/) or [LLDB](https://lldb.llvm.org/) against the Rust source code.

### Installation and setup

1. **Install the required crates and debugger**: First, let's ensure that the following crates are installed:

```sh
cargo install cargo-stylus
```

If on Linux, install GDB if it's not already installed:

```sh
sudo apt-get install gdb
```

If on MacOS, install LLDB if it's not already installed:

```sh
xcode-select --install
```

2. **Deploy your Stylus contract**: For this guide, we demonstrate how to debug the execution of the `increment()` method in the [stylus-hello-world](https://github.com/OffchainLabs/stylus-hello-world) smart contract. In Rust, it looks something like this, within `src/lib.rs`:

```sh
#[external]
impl Counter {
    ...
    /// Increments number and updates its value in storage.
    pub fn increment(&mut self) {
        let number = self.number.get();
        self.set_number(number + U256::from(1));
    }
    ...
}
```

Set your RPC endpoint to a node with **tracing enabled** and your private key:

```sh
export RPC_URL=...
export PRIV_KEY=...
```

and deploy your contract:

```sh
cargo stylus deploy --private-key=$PRIV_KEY --endpoint=$RPC_URL
```

You should see an output similar to:

```sh
contract size: 4.0 KB
wasm size: 12.1 KB
contract size: 4.0 KB
deployed code at address: 0x2c8d8a1229252b07e73b35774ad91c0b973ecf71
wasm already activated!
```

3. **Send a transaction**: First, set the address of the deployed contract as an environment variable:

```sh
export ADDR=0x2c8d8a1229252b07e73b35774ad91c0b973ecf71
```

And send a transaction using `Cast`:

```sh
cast send --rpc-url=$RPC_URL --private-key=$PRIV_KEY $ADDR "increment()"
```

4. **Replay the transaction with the debugger**: Now, we can replay the transaction with cargo stylus and the debugger to inspect each step of it against our source code. Make sure GDB is installed and that you are on a Linux, x86 system.
   Also, you should set the transaction hash as an environment variable:

```sh
export TX_HASH=0x18b241841fa0a59e02d3c6d693750ff0080ad792204aac7e5d4ce9e20c466835
```

And replay the transaction:

```sh
cargo stylus replay --tx=$TX_HASH --endpoint=$RPC_URL --use-native-tracer
```

Options:

```sh
--tx: Specifies the transaction hash to replay.
--endpoint: Specifies the RPC endpoint for fetching transaction data.
--use-native-tracer: Uses the native Stylus tracer instead of the default JS tracer. The native tracer has broader support from RPC providers.
```

:::note
The `--use-native-tracer` flag uses `stylusTracer` instead of `jsTracer`, which is required for tracing Stylus transactions on most RPC providers. See more details [below](#rpc-endpoint-compatibility).
:::

The debugger will load and set a breakpoint automatically at the `user_entrypoint` internal Stylus function. While the examples below showcase GDB commands, you can find the LLDB equivalents [here](https://lldb.llvm.org/use/map.html).

```sh
[Detaching after vfork from child process 370003]

Thread 1 "cargo-stylus" hit Breakpoint 1, stylus_hello_world::user_entrypoint (len=4) at src/lib.rs:38
38	    #[entrypoint]
(gdb)
```

5. **Debugging**: Now, set a breakpoint at the `increment()` method:

```sh
(gdb) b stylus_hello_world::Counter::increment
Breakpoint 2 at 0x7ffff7e4ee33: file src/lib.rs, line 69.
```

Then, type `c` to continue the execution and you will reach that line where `increment` is called:

```sh
(gdb) c
```

Once you reach the `increment` method, inspect the state:

```sh
Thread 1 "cargo-stylus" hit Breakpoint 2, stylus_hello_world::Counter::increment (self=0x7fffffff9ae8) at src/lib.rs:69
69	        let number = self.number.get();
(gdb) p number
```

### Trace a transaction

For traditional tracing, `cargo stylus` supports calls to `debug_traceTransaction`. To trace a transaction, you can use the following command:

```sh
cargo stylus trace [OPTIONS] --tx <TX> --use-native-tracer
```

Options:

```sh
  -e, --endpoint <ENDPOINT>  RPC endpoint [default: http://localhost:8547]
  -t, --tx <TX>              Tx (transaction) to replay
  -p, --project <PROJECT>    Project path [default: .]
  -h, --help                 Print help
  -V, --version              Print version
      --use-native-tracer    Uses the native Stylus tracer instead of the default JS tracer. The native tracer has broader support from RPC providers.
```

Run the following command to obtain a trace output:

```sh
cargo stylus trace --tx=$TX_HASH --endpoint=$RPC_URL --use-native-tracer
```

This will produce a trace of the functions called and [ink](/stylus/concepts/gas-metering#ink-and-gas) left along each method:

```sh
[{"args":[0,0,0,4],"endInk":846200000,"name":"user_entrypoint","outs":[],"startInk":846200000},{"args":[],"endInk":846167558,"name":"msg_reentrant","outs":[0,0,0,0],"startInk":846175958},{"args":[],"endInk":846047922,"name":"read_args","outs":[208,157,224,138],"startInk":846061362},{"args":[],"endInk":845914924,"name":"msg_value","outs":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"startInk":845928364},{"args":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"endInk":227196069,"name":"storage_load_bytes32","outs":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"startInk":844944549},{"args":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],"endInk":226716083,"name":"storage_cache_bytes32","outs":[],"startInk":226734563},{"args":[0],"endInk":226418732,"name":"storage_flush_cache","outs":[],"startInk":226486805},{"args":[],"endInk":226362319,"name":"write_result","outs":[],"startInk":226403481},{"args":[],"endInk":846200000,"name":"user_returned","outs":[0,0,0,0],"startInk":846200000}]
```

### RPC endpoint compatibility

Both `cargo stylus trace` and `cargo stylus replay` require an RPC endpoint that supports `debug_traceTransaction`.
By default, the `jsTracer` type is used, which is not supported by most RPC providers. If the `--use-native-tracer` flag is used, the `stylusTracer` type is used, which is supported by many RPC providers.
Both `jsTracer` and `stylusTracer` are available on local nodes, but `stylusTracer` is more efficient.
See this [list of RPC providers](/for-devs/dev-tools-and-resources/chain-info#third-party-rpc-providers) for tracing support.

---

## .mdx (how-tos/optimizing-binaries.mdx)
---
title: 'How to optimize Stylus WASM binaries'
description: 'A guide on optimizing Stylus WASM program sizes'
author: rauljordan
sme: rauljordan
target_audience: 'Developers deploying smart contracts using Stylus'
content_type: how-to
sidebar_position: 1
---

To be deployed onchain, the size of your **uncompressed WebAssembly (WASM) file** must not exceed 128Kb, while the **compressed binary** must not exceed 24KB. Stylus conforms with the same contract size limit as the EVM to remain fully interoperable with all smart contracts on Arbitrum chains.

[cargo-stylus](https://github.com/OffchainLabs/cargo-stylus), the Stylus CLI tool, automatically compresses your WASM programs, but there are additional steps that you can take to further reduce the size of your binaries.

Your options fall into two categories: Rust compiler flags, and third-party optimization tools.

## Rust compiler flags

The Rust compiler supports various config options for shrinking binary sizes.

### `Cargo.toml`

```toml
[profile.release]
codegen-units = 1        # prefer efficiency to compile time
panic = "abort"          # use simple panics
opt-level = "z"          # optimize for size ("s" may also work)
strip = true             # remove debug info
lto = true               # link time optimization
debug = false            # no debug data
rpath = false            # no run-time search path
debug-assertions = false # prune debug assertions
incremental = false      # no incremental builds
```

## Third-party optimization tooling

Additional WASM-specific tooling exists to shrink binaries. Due to being third party, users should use these at their own risk.

### `wasm-opt`

[wasm-opt](https://docs.rs/wasm-opt/0.113.0/wasm_opt/) applies techniques to further reduce binary size, usually netting around 10%.

### `twiggy`

[twiggy](https://github.com/rustwasm/twiggy) is a code size profiler for WASM, it can help you estimate the impact of each added component on your binaries' size.

Our team has also curated a [list of recommended libraries](/stylus/recommended-libraries) that are helpful to Stylus development and optimally sized.

### Frequently asked questions

#### Will future releases of Stylus introduce additional optimizations?

Yes! We're actively working on improving WASM sizes generated by Rust code with the Stylus SDK.

#### Why don't I have to worry about this type of optimization when I use `cargo` without using Stylus?

On modern platforms, tools like `cargo` don’t have to worry about the size of the binaries they produce. This is because there’s many orders of magnitude more storage available than even the largest of binaries, and for most applications it’s media like images and videos that constitutes the majority of the footprint.

Resource constraints when building on blockchains are extremely strict. Hence, while not the default option, tooling often provides mechanisms for reducing binary bloat, such as the options outlined in this document.

---

## .mdx (how-tos/testing-contracts.mdx)
---
id: 'testing-contracts'
title: 'Testing Smart Contracts with Stylus'
description: 'A comprehensive guide to writing and running tests for Stylus smart contracts.'
sme: anegg0
target_audience: 'Developers writing smart contracts using Stylus.'
sidebar_position: 3
---

import CustomDetails from '@site/src/components/CustomDetails';
import { VanillaAdmonition } from '@site/src/components/VanillaAdmonition/';

## Introduction

The Stylus SDK provides a robust testing framework that allows developers to write and run tests for their contracts directly in Rust without deploying to a blockchain. This guide will walk you through the process of writing and running tests for Stylus contracts using the built-in testing framework.

The Stylus testing framework allows you to:

- Simulate a complete Ethereum environment for your tests without the need for running a test node
- Test contract storage operations and state transitions
- Mock transaction context and block information
- Test contract-to-contract interactions with mocked calls
- Verify contract logic without deployment costs or delays
- Simulate various user scenarios and edge cases

### Prerequisites

Before you begin, make sure you have:

- Basic familiarity with Rust and smart contract development
- Understanding of unit testing concepts
- Rust toolchain: follow the instructions on [Rust Lang's installation page](https://www.rust-lang.org/tools/install) to install a complete Rust toolchain (v1.81 or newer) on your system. After installation, ensure you can access the programs `rustup`, `rustc`, and `cargo` from your preferred terminal application.

## The Stylus Testing Framework

The Stylus SDK includes `testing`, a module that provides all the tools you need to test your contracts. This module includes:

- **TestVM**: A mock implementation of the Stylus VM that can simulate all host functions
- **TestVMBuilder**: A builder pattern to conveniently configure the test VM
- Built-in utilities for mocking calls, storage, and other EVM operations

### Key Components

Here are the components you'll use when testing your Stylus contracts:

- **TestVM**: The core component that simulates the Stylus execution environment
- **Storage accessors**: For testing contract state changes
- **Call mocking**: For simulating interactions with other contracts
- **Block context**: For testing time-dependent logic

## Example Smart Contract: Cupcake Vending Machine

Let's look at a Rust-based cupcake vending machine smart contract. This contract follows two simple rules:

1. The vending machine will distribute a cupcake to anyone who hasn't received one in the last 5 seconds
2. The vending machine tracks each user's cupcake balance

<VanillaAdmonition type="note">

You can find all the code in this tutorial as a Rust workspace in the [Quickstart repo](https://github.com/OffchainLabs/stylus-quickstart-vending-machine)

</VanillaAdmonition>

<CustomDetails summary="Cupcake Vending Machine Contract">

```rust
#![cfg_attr(not(any(test, feature = "export-abi")), no_main)]
extern crate alloc;

/// Import items from the SDK. The prelude contains common traits and macros.
use stylus_sdk::alloy_primitives::{Address, U256};
use stylus_sdk::console;
use stylus_sdk::prelude::\*;
use alloy_sol_types::sol;

// Define the event using the sol! macro
sol! {
    event CupcakeDistributed(address indexed recipient, uint256 indexed timestamp, uint256 new_balance);
}

sol_storage! { #[entrypoint]
pub struct VendingMachine {
mapping(address => uint256) cupcake_balances;
mapping(address => uint256) cupcake_distribution_times;
}
}

#[public]
impl VendingMachine {
pub fn give_cupcake_to(&mut self, user_address: Address) -> Result<bool, Vec<u8>> {
// Get the last distribution time for the user.
let last_distribution = self.cupcake_distribution_times.get(user_address);
// Calculate the earliest next time the user can receive a cupcake.
let five_seconds_from_last_distribution = last_distribution + U256::from(5);

        // Get the current block timestamp using the VM pattern
        let current_time = self.vm().block_timestamp();
        // Check if the user can receive a cupcake.
        let user_can_receive_cupcake =
            five_seconds_from_last_distribution <= U256::from(current_time);

        if user_can_receive_cupcake {
            // Increment the user's cupcake balance.
            let mut balance_accessor = self.cupcake_balances.setter(user_address);
            let balance = balance_accessor.get() + U256::from(1);
            balance_accessor.set(balance);

            // Get current timestamp using the VM pattern BEFORE creating the mutable borrow
            let new_distribution_time = self.vm().block_timestamp();

            // Update the distribution time to the current time.
            let mut time_accessor = self.cupcake_distribution_times.setter(user_address);
            time_accessor.set(U256::from(new_distribution_time));

            // Emit the CupcakeDistributed event
            log(self.vm(), CupcakeDistributed {
                recipient: user_address,
                timestamp: U256::from(new_distribution_time),
                new_balance: balance,
            });

            return Ok(true);
        } else {
            // User must wait before receiving another cupcake.
            console!(
                "HTTP 429: Too Many Cupcakes (you must wait at least 5 seconds between cupcakes)"
            );
            return Ok(false);
        }
    }
    pub fn get_cupcake_balance_for(&self, user_address: Address) -> Result<U256, Vec<u8>> {
        Ok(self.cupcake_balances.get(user_address))
    }

}
```

</CustomDetails>

## Writing Tests for the Vending Machine

Now, let's write tests for our vending machine contract using the Stylus testing framework. We'll create tests that verify:

1. Users can get an initial cupcake
2. Users must wait 5 seconds between cupcakes
3. Cupcake balances are tracked correctly
4. The contract state updates properly

### Test Structure

Create a test file using standard Rust test patterns. Here's the basic structure:

```rust
// Import necessary dependencies
#[cfg(test)]
mod test {
    use super::*;
    use alloy_primitives::address;
    use stylus_sdk::testing::*;

    #[test]
    fn test_give_cupcake_to() {
    // Set up test environment
    let vm = TestVM::default();
    // Initialize your contract
    let mut contract = VendingMachine::from(&vm);

    // Test logic goes here...
}
```

### Using the TestVM

The `TestVM` simulates the execution environment for your contract, removing the need to run your tests against a test node.
The `TestVM` allows you to control aspects like:

- Block timestamp and number
- Account balances
- Transaction value and sender
- Storage state

Let's create a test suite that covers all aspects of our contract, we'll go over the code features one by one:

<CustomDetails summary="Test Vending Machine Contract">

```rust
#[cfg(test)]
mod test {
    use super::*;
    use alloy_primitives::address;
    use stylus_sdk::testing::*;

    #[test]
    fn test_give_cupcake_to() {
        let vm = TestVM::default();

        let mut contract = VendingMachine::from(&vm);
        let user = address!("0xCDC41bff86a62716f050622325CC17a317f99404");
        assert_eq!(contract.get_cupcake_balance_for(user).unwrap(), U256::ZERO);

        vm.set_block_timestamp(vm.block_timestamp() + 6);

        // Give a cupcake and verify it succeeds
        assert!(contract.give_cupcake_to(user).unwrap());

        // Check balance is now 1
        assert_eq!(
            contract.get_cupcake_balance_for(user).unwrap(),
            U256::from(1)
        );

        // Try to give another cupcake immediately - should fail due to time restriction
        assert!(!contract.give_cupcake_to(user).unwrap());

        // Balance should still be 1
        assert_eq!(
            contract.get_cupcake_balance_for(user).unwrap(),
            U256::from(1)
        );

        // Advance block timestamp by 6 seconds
        vm.set_block_timestamp(vm.block_timestamp() + 6);

        // Now giving a cupcake should succeed
        assert!(contract.give_cupcake_to(user).unwrap());

        // Balance should now be 2
        assert_eq!(
            contract.get_cupcake_balance_for(user).unwrap(),
            U256::from(2)
        );
    }
}
```

</CustomDetails>

### TestVM: advanced use

This test shows how you can use advanced configuration and usage of the TestVM by creating and configuring a TestVM with custom parameters:

- Setting blockchain state (timestamps, block numbers)
- Interacting with contract methods
- Taking and inspecting VM state snapshots
- Mocking external contract calls
- Testing time-dependent contract behavior
- Testing logs

<CustomDetails summary="Advanced TestVM Configuration">

#### 1: TestVM Setup and Configuration

```rust
#[test]
fn test_advanced_testvm_configuration() {

    // Create a TestVM with custom configuration using the builder pattern
    // This approach allows for fluent, readable test setup
    let vm: TestVM = TestVMBuilder::new()
        // Set the transaction sender address (msg.sender in Solidity)
        .sender(address!("0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266"))
        // Set the address where our contract is deployed
        .contract_address(address!("0x5FbDB2315678afecb367f032d93F642f64180aa3"))
        // Set the ETH value sent with the transaction (msg.value in Solidity)
        .value(U256::from(1))
        .build();

    // Configure additional blockchain state parameters directly on the VM instance
    // This demonstrates how to set parameters after VM creation
    vm.set_block_number(12345678);

    // Note: The chain ID is set to 42161 (Arbitrum One) by default in the TestVM
    // We don't need to set it explicitly as it's already configured in the VM state
```

#### 2: Contract Initialization and User Setup

```rust
    // Initialize our VendingMachine contract with the configured VM
    // The `from` method connects our contract to the test environment
    let mut contract = VendingMachine::from(&vm);

    // Define a user address that will interact with our contract
    // This represents an external user's Ethereum address
    let user = address!("0xCDC41bff86a62716f050622325CC17a317f99404");

    // 3: Initial State Verification
    // ------------------------------------

    // Verify the user starts with zero cupcakes
    // This confirms our contract's initial state is as expected
    assert_eq!(contract.get_cupcake_balance_for(user).unwrap(), U256::ZERO);

    // Set the initial block timestamp by advancing it by 10 seconds
    // This ensures we're past any time-based restrictions
    vm.set_block_timestamp(vm.block_timestamp() + 10);
```

#### 4: Contract Interaction

```rust
    // Give a cupcake to the user and verify the operation succeeds
    // The contract should return true when a cupcake is successfully given
    assert!(contract.give_cupcake_to(user).unwrap());

    // Verify the user now has exactly one cupcake
    // This confirms our contract correctly updated its storage
    assert_eq!(
        contract.get_cupcake_balance_for(user).unwrap(),
        U256::from(1)
    );
```

#### 5: VM State Inspection

```rust
    // Take a snapshot of the current VM state for inspection
    // This captures all storage, balances, and blockchain parameters
    let snapshot = vm.snapshot();

    // Inspect various aspects of the VM state to verify configuration
    // Chain ID should be Arbitrum One (42161) which is the default
    assert_eq!(snapshot.chain_id, 42161);
    // Message value should match what we configured (1 wei)
    assert_eq!(snapshot.msg_value, U256::from(1));
```

#### 6: Mocking External Contract Calls

```rust
    // Define an external contract we might want to interact with
    let external_contract = address!("0x8626f6940E2eb28930eFb4CeF49B2d1F2C9C1199");
    // Define example call data we would send to that contract
    let call_data = vec![0xab, 0xcd, 0xef];
    // Define the expected response from that contract
    let expected_response = vec![0x12, 0x34, 0x56];

    // Mock the external call so it returns our expected response
    // This allows testing contract interactions without deploying external contracts
    vm.mock_call(external_contract, call_data, Ok(expected_response));
```

#### 7: Time-Dependent Behavior Testing

```rust
    // Set a specific block timestamp
    // This simulates the passage of time on the blockchain
    vm.set_block_timestamp(1006);

    // Try giving another cupcake after the time restriction has passed
    // The contract should allow this since enough time has elapsed
    assert!(contract.give_cupcake_to(user).unwrap());

    // Verify the user now has two cupcakes
    // This confirms our contract correctly handles time-based restrictions
    assert_eq!(
        contract.get_cupcake_balance_for(user).unwrap(),
        U256::from(2)
    );
```

#### 8: Testing Event Logs

```rust
    // Test that events are emitted when cupcakes are distributed
    let vm_logs = TestVM::new();
    let mut contract_logs = VendingMachine::from(&vm_logs);
    let user_logs = address!("0xCDC41bff86a62716f050622325CC17a317f99404");

    // Set initial timestamp to ensure we can give a cupcake
    vm_logs.set_block_timestamp(100);

    // Give a cupcake to the user - this should emit an event
    let result = contract_logs.give_cupcake_to(user_logs).unwrap();
    assert!(result, "Should successfully give first cupcake");

    // Get all emitted logs from the VM
    let logs = vm_logs.get_emitted_logs();

    // Verify that exactly one event was emitted
    assert_eq!(logs.len(), 1, "Should emit exactly one CupcakeDistributed event");

    // Calculate the expected event signature for CupcakeDistributed
    // Event signature: CupcakeDistributed(address indexed recipient, uint256 indexed timestamp, uint256 new_balance)
    // The signature is calculated as: keccak256("CupcakeDistributed(address,uint256,uint256)")
    use alloy_primitives::hex;
    use stylus_sdk::alloy_primitives::B256;
    let event_signature: B256 =
        hex!("c12a96437276bfca30ffd7a90b5e9d233c71c97f759c3b76b886f29e87989bb2").into();

    // Check the first topic (event signature)
    assert_eq!(
        logs[0].0[0],
        event_signature,
        "First topic should be the event signature"
    );

    // Extract the indexed recipient address from the second topic
    let recipient_topic = logs[0].0[1];
    let recipient_bytes: [u8; 32] = recipient_topic.into();

    // Indexed addresses are padded to 32 bytes - extract the last 20 bytes
    let mut recipient_address = [0u8; 20];
    recipient_address.copy_from_slice(&recipient_bytes[12..32]);

    // Verify the recipient address matches our user
    assert_eq!(
        Address::from(recipient_address),
        user_logs,
        "Event recipient should match the user who received the cupcake"
    );

    // The new_balance is not indexed, so it's in the data field
    let log_data = &logs[0].1;

    // For a single uint256, it should be 32 bytes
    assert_eq!(log_data.len(), 32, "Event data should contain one uint256 (32 bytes)");

    // Convert the data bytes to U256
    let mut balance_bytes = [0u8; 32];
    balance_bytes.copy_from_slice(&log_data[0..32]);
    let new_balance = U256::from_be_bytes(balance_bytes);

    // Verify the balance is 1 (first cupcake)
    assert_eq!(
        new_balance,
        U256::from(1),
        "Event should show new balance of 1 cupcake"
    );
}
```

</CustomDetails>

Here is a `cargo.toml` file to add the required dependencies:

<CustomDetails summary="cargo.toml">

```rust
[package]
name = "stylus-cupcake-example"
version = "0.1.7"
edition = "2021"
license = "MIT OR Apache-2.0"
keywords = ["arbitrum", "ethereum", "stylus", "alloy"]
[dependencies]
alloy-primitives = "=0.8.20"
alloy-sol-types = "=0.8.20"
mini-alloc = "0.8.4"
stylus-sdk = "0.8.4"
hex = "0.4.3"
dotenv = "0.15.0"

[dev-dependencies]
tokio = { version = "1.12.0", features = ["full"] }
ethers = "2.0"
eyre = "0.6.8"
stylus-sdk = { version = "0.8.4", features = ["stylus-test"] }
alloy-primitives = { version = "=0.8.20", features = ["sha3-keccak"] }

[features]
export-abi = ["stylus-sdk/export-abi"]
debug = ["stylus-sdk/debug"]

[[bin]]
name = "stylus-cupcake-example"
path = "src/main.rs"

[lib]
crate-type = ["lib", "cdylib"]

[profile.release]
codegen-units = 1
strip = true
lto = true
panic = "abort"

# If you need to reduce the binary size, it is advisable to try other
# optimization levels, such as "s" and "z"
opt-level = 3
```

</CustomDetails>

You can find the example above in the [stylus-quickstart-vending-machine git repository](https://github.com/OffchainLabs/stylus-quickstart-vending-machine).

## Running Tests

To run your tests, you can use the standard Rust test command:

```shell
cargo test
```

Or with the `cargo-stylus` CLI tool:

To run a specific test:

```shell
cargo test test_give_cupcake
```

## Testing Best Practices

1. **Test Isolation**

   - Create a new `TestVM` instance for each test
   - Avoid relying on state from previous tests

2. **Comprehensive Coverage**

   - Test both success and error conditions
   - Test edge cases and boundary conditions
   - Verify all public functions and important state transitions

3. **Clear Assertions**

   - Use descriptive error messages in assertions
   - Make assertions that verify the actual behavior you care about

4. **Realistic Scenarios**

   - Test real-world usage patterns
   - Include tests for authorization and access control

5. **Gas and Resource Efficiency**
   - For complex contracts, consider testing gas usage patterns
   - Look for storage optimization opportunities

## Migrating from Global Accessors to VM Accessors

As of Stylus SDK 0.8.0, there's a shift away from global host function invocations to using the `.vm()` method. This is a safer approach that makes testing easier. For example:

```rust
// Old style (deprecated)
let timestamp = block::timestamp();

// New style (preferred)
let timestamp = self.vm().block_timestamp();
```

To make your contracts more testable, make sure they access host methods through the `HostAccess` trait with the `.vm()` method.

---

## .mdx (how-tos/using-constructors.mdx)
---
title: 'Using constructors with Stylus'
description: 'A comprehensive guide to implementing and deploying smart contracts with constructors in Stylus'
sidebar_label: 'Using constructors'
author: 'anegg0'
sme: 'anegg0'
user_story: 'As a Rust developer, I want to understand how to implement and use constructors in Stylus smart contracts'
content_type: 'how-to'
sidebar_position: 3
---

import CustomDetails from '@site/src/components/CustomDetails';
import { VanillaAdmonition } from '@site/src/components/VanillaAdmonition/';

Constructors allow you to initialize your Stylus smart contracts with specific parameters when deploying them. This guide will show you how to implement constructors in Rust, understand their behavior, and deploy contracts using them.

## What you'll accomplish

By the end of this guide, you'll be able to:

- Implement constructor functions in Stylus contracts
- Understand the constructor rules and limitations
- Deploy contracts with constructor parameters
- Test the constructor functionality
- Handle constructor errors and validation

## Prerequisites

Before implementing constructors, ensure you have:

<CustomDetails summary="Rust toolchain">

Follow the instructions on [Rust Lang's installation page](https://www.rust-lang.org/tools/install) to install a complete Rust toolchain (v1.88 or newer) on your system. After installation, ensure you can access the programs `rustup`, `rustc`, and `cargo` from your preferred terminal application.

</CustomDetails>

<CustomDetails summary="cargo stylus">

In your terminal, run:

```shell
cargo install --force cargo-stylus
```

Add WASM ([WebAssembly](https://webassembly.org/)) as a build target for the specific Rust toolchain you are using. The below example sets your default Rust toolchain to 1.88 as well as adding the WASM build target:

```shell
rustup default 1.88
rustup target add wasm32-unknown-unknown --toolchain 1.88
```

You can verify that cargo stylus is installed by running `cargo stylus --help` in your terminal, which will return a list of helpful commands.

</CustomDetails>

<CustomDetails summary="A local Arbitrum test node">

Instructions on how to set up a local Arbitrum test node can be found [in the Nitro-devnode repository](https://github.com/OffchainLabs/nitro-devnode).

</CustomDetails>

## Understanding Stylus constructors

Stylus constructors provide an atomic way to deploy, activate, and initialize a contract in a single transaction. If your contract lacks a constructor, it may allow access to the contract's storage before the initialization logic runs, leading to unexpected behavior.

<VanillaAdmonition type="info" title="Constructors and composition">
  Stylus uses trait-based composition instead of traditional inheritance. When building contracts
  that compose multiple traits, constructors help initialize all components properly. See the
  [Constructor with trait-based composition](#constructor-with-trait-based-composition) section for
  examples.
</VanillaAdmonition>

### Constructor rules and guarantees

Stylus constructors follow these important rules:

| Rule                                        | Why it exists                                                                           |
| ------------------------------------------- | --------------------------------------------------------------------------------------- |
| **Exactly 0 or 1 constructor** per contract | Mimics Solidity behavior and avoids ambiguity                                           |
| **Must be annotated with `#[constructor]`** | Guarantees the deployer calls the correct initialization method                         |
| **Must take `&mut self`**                   | Allows writing to contract storage during deployment                                    |
| **Returns `()` or `Result<(), Error>`**     | Enables error handling; reverting aborts deployment                                     |
| **Use `tx_origin()` for deployer address**  | Factory contracts are used in deployment, so `msg_sender()` returns the factory address |
| **Constructor runs exactly once**           | The SDK uses a sentinel system to prevent re-execution                                  |

<VanillaAdmonition type="info" title="Factory pattern in deployment">
  Stylus uses a factory pattern for deployment, which means `msg_sender()` in a constructor returns
  the factory contract address, not the deployer. Always use `tx_origin()` to get the actual
  deployer address.
</VanillaAdmonition>

## Basic constructor implementation

Here's a simple example of a constructor in a Stylus contract:

```rust
#![cfg_attr(not(any(test, feature = "export-abi")), no_main)]

extern crate alloc;

use alloy_primitives::{Address, U256};
use alloy_sol_types::sol;
use stylus_sdk::prelude::*;

sol! {
    #[derive(Debug)]
    error InvalidAmount();
}

sol_storage! {
    #[entrypoint]
    pub struct SimpleToken {
        address owner;
        uint256 total_supply;
        string name;
        string symbol;
        mapping(address => uint256) balances;
    }
}

#[derive(SolidityError, Debug)]
pub enum SimpleTokenError {
    InvalidAmount(InvalidAmount),
}

#[public]
impl SimpleToken {
    /// Constructor initializes the token with a name, symbol, and initial supply
    #[constructor]
    #[payable]
    pub fn constructor(
        &mut self,
        name: String,
        symbol: String,
        initial_supply: U256,
    ) -> Result<(), SimpleTokenError> {
        // Validate input parameters
        if initial_supply == U256::ZERO {
            return Err(SimpleTokenError::InvalidAmount(InvalidAmount {}));
        }

        // Get the deployer address using tx_origin()
        let deployer = self.vm().tx_origin();

        // Initialize contract state
        self.owner.set(deployer);
        self.name.set_str(&name);
        self.symbol.set_str(&symbol);
        self.total_supply.set(initial_supply);

        // Mint initial supply to deployer
        self.balances.setter(deployer).set(initial_supply);

        Ok(())
    }

    // Additional contract methods...
    pub fn balance_of(&self, account: Address) -> U256 {
        self.balances.get(account)
    }

    pub fn total_supply(&self) -> U256 {
        self.total_supply.get()
    }
}
```

### Key implementation details

1. **Parameter validation**: Always validate constructor parameters before proceeding with initialization
2. **Error handling**: Use `Result<(), Error>` to handle initialization failures gracefully
3. **Payable constructors**: Add `#[payable]` to receive ETH during deployment
4. **State initialization**: Set all necessary contract state in the constructor

## Advanced constructor patterns

### Constructor with complex validation

```rust
#[constructor]
#[payable]
pub fn constructor(
    &mut self,
    name: String,
    symbol: String,
    initial_supply: U256,
    max_supply: U256,
) -> Result<(), TokenContractError> {
    // Multiple validation checks
    if initial_supply == U256::ZERO {
        return Err(TokenContractError::InvalidAmount(InvalidAmount {}));
    }

    if initial_supply > max_supply {
        return Err(TokenContractError::TooManyTokens(TooManyTokens {}));
    }

    if name.is_empty() || symbol.is_empty() {
        return Err(TokenContractError::InvalidAmount(InvalidAmount {}));
    }

    let deployer = self.vm().tx_origin();

    // Initialize with timestamp tracking
    self.owner.set(deployer);
    self.name.set_str(&name);
    self.symbol.set_str(&symbol);
    self.total_supply.set(initial_supply);
    self.max_supply.set(max_supply);
    self.created_at.set(U256::from(self.vm().block_timestamp()));

    // Mint tokens to deployer
    self.balances.setter(deployer).set(initial_supply);

    // Emit initialization event
    log(self.vm(), TokenCreated {
        creator: deployer,
        name: name.clone(),
        symbol: symbol.clone(),
        initial_supply,
    });

    Ok(())
}
```

### Constructor with trait-based composition

Stylus uses trait-based composition instead of traditional inheritance. When implementing constructors with traits, each component typically has its own initialization logic:

```rust
// Define traits for different functionality
trait IErc20 {
    fn balance_of(&self, account: Address) -> U256;
    fn transfer(&mut self, to: Address, value: U256) -> bool;
}

trait IOwnable {
    fn owner(&self) -> Address;
    fn transfer_ownership(&mut self, new_owner: Address) -> bool;
}

// Define storage for each component
#[storage]
struct Erc20Component {
    balances: StorageMap<Address, StorageU256>,
    total_supply: StorageU256,
}

#[storage]
struct OwnableComponent {
    owner: StorageAddress,
}

// Main contract that composes functionality
#[storage]
#[entrypoint]
struct MyToken {
    erc20: Erc20Component,
    ownable: OwnableComponent,
    name: StorageString,
    symbol: StorageString,
}

#[public]
#[implements(IErc20, IOwnable)]
impl MyToken {
    #[constructor]
    pub fn constructor(
        &mut self,
        name: String,
        symbol: String,
        initial_supply: U256,
    ) -> Result<(), TokenError> {
        // Initialize each component
        self.initialize_ownable()?;
        self.initialize_erc20(initial_supply)?;

        // Initialize contract-specific state
        self.name.set_str(&name);
        self.symbol.set_str(&symbol);

        Ok(())
    }

    fn initialize_ownable(&mut self) -> Result<(), TokenError> {
        let deployer = self.vm().tx_origin();
        self.ownable.owner.set(deployer);
        Ok(())
    }

    fn initialize_erc20(&mut self, initial_supply: U256) -> Result<(), TokenError> {
        if initial_supply == U256::ZERO {
            return Err(TokenError::InvalidSupply);
        }

        let deployer = self.vm().tx_origin();
        self.erc20.total_supply.set(initial_supply);
        self.erc20.balances.setter(deployer).set(initial_supply);
        Ok(())
    }
}
```

<VanillaAdmonition type="info" title="Trait-based composition in Stylus">
  Unlike Solidity's inheritance, Stylus uses Rust's trait system for composition. Each component is
  initialized explicitly in the constructor.
</VanillaAdmonition>

## Testing constructors

The Stylus SDK provides comprehensive testing tools for constructor functionality:

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use stylus_sdk::testing::*;

    #[test]
    fn test_constructor_success() {
        let vm = TestVMBuilder::new()
            .sender(Address::from([0x01; 20]))
            .build();

        let mut contract = SimpleToken::from(&vm);

        let result = contract.constructor(
            "Test Token".to_string(),
            "TEST".to_string(),
            U256::from(1000000),
        );

        assert!(result.is_ok());
        assert_eq!(contract.name.get_string(), "Test Token");
        assert_eq!(contract.symbol.get_string(), "TEST");
        assert_eq!(contract.total_supply.get(), U256::from(1000000));
        assert_eq!(
            contract.balance_of(Address::from([0x01; 20])),
            U256::from(1000000)
        );
    }

    #[test]
    fn test_constructor_validation() {
        let vm = TestVMBuilder::new()
            .sender(Address::from([0x01; 20]))
            .build();

        let mut contract = SimpleToken::from(&vm);

        // Test zero supply rejection
        let result = contract.constructor(
            "Test Token".to_string(),
            "TEST".to_string(),
            U256::ZERO,
        );

        assert!(result.is_err());
        assert!(matches!(
            result.unwrap_err(),
            SimpleTokenError::InvalidAmount(_)
        ));
    }
}
```

## Deploying contracts with constructors

### Using cargo stylus

Deploy your contract with constructor arguments using `cargo stylus deploy`:

```bash
# Deploy with constructor parameters
cargo stylus deploy \
  --private-key-path ~/.arbitrum/key \
  --endpoint https://sepolia-rollup.arbitrum.io/rpc \
  --constructor-args "MyToken" "MTK" 1000000
```

### Constructor argument encoding

`cargo stylus` automatically encodes the constructor arguments. The arguments should be provided in the same order as defined in your constructor function.

For complex types:

- **Strings**: Provide as quoted strings
- **Numbers**: Provide as decimal or hex (0x prefix)
- **Addresses**: Provide as hex strings with 0x prefix
- **Arrays**: Use JSON array syntax

```bash
# Example with multiple argument types
cargo stylus deploy \
  --constructor-args "TokenName" "TKN" 1000000 "0x742d35Cc6635C0532925a3b8D95B5C1b0ea3C28F"
```

## Best practices

### Constructor parameter validation

Always validate constructor parameters to prevent deployment of misconfigured contracts:

```rust
#[constructor]
pub fn constructor(&mut self, params: ConstructorParams) -> Result<(), Error> {
    // Validate all parameters before any state changes
    self.validate_parameters(&params)?;

    // Initialize state only after validation passes
    self.initialize_state(params)?;

    Ok(())
}

fn validate_parameters(&self, params: &ConstructorParams) -> Result<(), Error> {
    if params.name.is_empty() {
        return Err(Error::InvalidName);
    }
    // Additional validation...
    Ok(())
}
```

### Error handling patterns

Use descriptive error types and provide meaningful error messages:

```rust
sol! {
    #[derive(Debug)]
    error InvalidName(string reason);
    #[derive(Debug)]
    error InvalidSupply(uint256 provided, uint256 max_allowed);
    #[derive(Debug)]
    error Unauthorized(address caller);
}

#[derive(SolidityError, Debug)]
pub enum ConstructorError {
    InvalidName(InvalidName),
    InvalidSupply(InvalidSupply),
    Unauthorized(Unauthorized),
}
```

### State initialization order

Initialize contract state in a logical order to avoid dependency issues:

```rust
#[constructor]
pub fn constructor(&mut self, params: ConstructorParams) -> Result<(), Error> {
    // 1. Validate parameters first
    self.validate_parameters(&params)?;

    // 2. Set basic contract metadata
    self.name.set_str(&params.name);
    self.symbol.set_str(&params.symbol);

    // 3. Set ownership and permissions
    let deployer = self.vm().tx_origin();
    self.owner.set(deployer);

    // 4. Initialize token economics
    self.total_supply.set(params.initial_supply);
    self.max_supply.set(params.max_supply);

    // 5. Set up initial balances
    self.balances.setter(deployer).set(params.initial_supply);

    // 6. Emit events last
    log(self.vm(), ContractInitialized { /* ... */ });

    Ok(())
}
```

## Common pitfalls and solutions

### Using msg_sender() instead of tx_origin()

**Problem**: Using `msg_sender()` in constructors returns the factory contract address, not the deployer.

```rust
// ❌ Wrong - returns factory address
let deployer = self.vm().msg_sender();

// ✅ Correct - returns actual deployer
let deployer = self.vm().tx_origin();
```

### Missing parameter validation

**Problem**: Not validating constructor parameters can lead to unusable contracts.

```rust
// ❌ Wrong - no validation
#[constructor]
pub fn constructor(&mut self, supply: U256) {
    self.total_supply.set(supply); // Could be zero!
}

// ✅ Correct - validate first
#[constructor]
pub fn constructor(&mut self, supply: U256) -> Result<(), Error> {
    if supply == U256::ZERO {
        return Err(Error::InvalidSupply);
    }
    self.total_supply.set(supply);
    Ok(())
}
```

### Forgetting the #[constructor] annotation

**Problem**: Functions named "constructor" without the annotation won't be recognized.

```rust
// ❌ Wrong - missing annotation
pub fn constructor(&mut self, value: U256) {
    // This won't be called during deployment
}

// ✅ Correct - properly annotated
#[constructor]
pub fn constructor(&mut self, value: U256) {
    // This will be called during deployment
}
```

## Summary

Constructors in Stylus provide a powerful way to initialize your smart contracts during deployment. Key takeaways:

- Use `#[constructor]` annotation and `&mut self` parameter
- Always use `tx_origin()` to get the deployer address
- Validate all parameters before initializing state
- Handle errors gracefully with `Result<(), Error>` return type
- Test the constructor behavior thoroughly
- Deploy with `cargo stylus deploy --constructor-args`

---

## .mdx (how-tos/using-inheritance.mdx)
---
title: 'Composition and trait-based routing model'
description: 'Learn how to implement trait-based composition in your Stylus smart contracts'
author: anegg0
sme: mahsamoosavi
content_type: how-to
sidebar_position: 1
---

import CustomDetails from '@site/src/components/CustomDetails';
import { VanillaAdmonition } from '@site/src/components/VanillaAdmonition/';

Inheritance allows you to build upon existing smart contract functionality without duplicating code. In Stylus, the Rust SDK provides tools to implement inheritance patterns similar to Solidity, but with some important differences. This guide walks you through implementing trait-based composition in your Stylus smart contracts.

<VanillaAdmonition type="caution" title="For Solidity developers">
  There's no direct equivalent of inheritance in Rust, but the following will show you the Rust-way
  of achieving similar results.
</VanillaAdmonition>

## Overview

The Stylus SDK offers trait-based composition using traits and the `#[implements]` annotation. This approach follows Rust's composition patterns and provides stronger type safety.

## Getting started

Before implementing trait-based composition, ensure you have:

<CustomDetails summary="Rust toolchain">

Follow the instructions on [Rust Lang's installation page](https://www.rust-lang.org/tools/install) to install a complete Rust toolchain (v1.81 or newer) on your system. After installation, ensure you can access the programs `rustup`, `rustc`, and `cargo` from your preferred terminal application.

</CustomDetails>

<CustomDetails summary="cargo stylus">

In your terminal, run:

```shell
cargo install --force cargo-stylus
```

Add WASM ([WebAssembly](https://webassembly.org/)) as a build target for the specific Rust toolchain you are using. The below example sets your default Rust toolchain to 1.81 as well as adding the WASM build target:

```shell
rustup default 1.88
rustup target add wasm32-unknown-unknown --toolchain 1.88
```

You can verify that cargo stylus is installed by running `cargo stylus --help` in your terminal, which will return a list of helpful commands.

</CustomDetails>

## Trait-based composition model

The recommended approach to inheritance in Stylus uses traits and the `#[implements]` annotation, which follows Rust's standard composition patterns:

### Basic example of trait-based composition

<CustomDetails summary="Trait-Based Inheritance Example: ERC-20">

```rust
use stylus_sdk::{
    alloy_primitives::{Address, U256},
    prelude::*,
    storage::{StorageAddress, StorageMap, StorageU256},
};

// Define traits for different functionality
trait IErc20 {
    fn name(&self) -> String;
    fn symbol(&self) -> String;
    fn decimals(&self) -> U256;
    fn total_supply(&self) -> U256;
    fn balance_of(&self, account: Address) -> U256;
    fn transfer(&mut self, to: Address, value: U256) -> bool;
}

trait IOwnable {
    fn owner(&self) -> Address;
    fn transfer_ownership(&mut self, new_owner: Address) -> bool;
    fn renounce_ownership(&mut self) -> bool;
}

// Define storage for each component
#[storage]
struct Erc20 {
    balances: StorageMap<Address, StorageU256>,
    total_supply: StorageU256,
}

#[storage]
struct Ownable {
    owner: StorageAddress,
}

// Define the main contract that composes different functionality
#[storage]
#[entrypoint]
struct Contract {
    erc20: Erc20,
    ownable: Ownable,
}

// The #[implements] attribute connects the contract to the traits it implements
#[public]
#[implements(IErc20, IOwnable)]
impl Contract {}

// Implement the ERC20 interface for the contract
#[public]
impl IErc20 for Contract {
    fn name(&self) -> String {
        "MyToken".to_string()
    }

    fn symbol(&self) -> String {
        "MTK".to_string()
    }

    fn decimals(&self) -> U256 {
        U256::from(18)
    }

    fn total_supply(&self) -> U256 {
        self.erc20.total_supply.get()
    }

    fn balance_of(&self, account: Address) -> U256 {
        self.erc20.balances.get(account)
    }

    fn transfer(&mut self, to: Address, value: U256) -> bool {
        // Implementation here
        true
    }
}

// Implement the Ownable interface for the contract
#[public]
impl IOwnable for Contract {
    fn owner(&self) -> Address {
        self.ownable.owner.get()
    }

    fn transfer_ownership(&mut self, new_owner: Address) -> bool {
        // Implementation here
        true
    }

    fn renounce_ownership(&mut self) -> bool {
        // Implementation here
        true
    }
}
```

</CustomDetails>

### How trait-based composition works

The trait-based composition model follows these principles:

1. Define traits that represent interfaces (similar to Solidity interfaces)
2. Implement these traits for your contract
3. Use the `#[implements(...)]` attribute to tell the Stylus SDK which traits your contract implements
4. The router will connect incoming calls to the appropriate implementation

This approach is aligned with Rust's composition patterns and offers better type safety.

### Method overriding

If both parent and child implement the same method, the one in the child will override the one in the parent. This allows for customizing inherited functionality.

<VanillaAdmonition type="caution" title="No explicit override keywords">
  Stylus does not currently contain explicit override or virtual keywords for marking override
  functions. It is important to carefully ensure that contracts are only overriding the functions
  you intend to override.
</VanillaAdmonition>

### ABI export considerations with trait-based composition

When using trait-based composition, you need to be careful about function selectors to ensure correct ABI generation. Due to how Rust handles traits, you may need to explicitly set selectors for methods to match Solidity's expected function signatures.

<VanillaAdmonition type="caution" title="Selector precision">
  When implementing traits with methods that have matching names, you must manually use the
  `#[selector(name = "ActualName")]` attribute to avoid method selector collisions. This is
  particularly important when implementing standard interfaces like `ERC-20` or `ERC-721`.
</VanillaAdmonition>

<CustomDetails summary="Selector issue example: ERC-721">
```rust
// In Solidity, both these functions would have different selectors:
// function safeTransferFrom(address from, address to, uint256 tokenId)
// function safeTransferFrom(address from, address to, uint256 tokenId, bytes data)

// In Rust, we need to use different method names, but want the same selectors: #[public]
impl<T: Erc721Params> Erc721<T> {
// Use the #[selector] attribute to specify the correct Solidity-compatible name #[selector(name = "safeTransferFrom")]
pub fn safe_transfer_from_with_data<S: TopLevelStorage + BorrowMut<Self>>(
storage: &mut S,
from: Address,
to: Address,
token_id: U256,
data: Bytes,
) -> Result<(), Erc721Error> {
// Implementation
}

    // This method also needs the same selector name
    #[selector(name = "safeTransferFrom")]
    pub fn safe_transfer_from<S: TopLevelStorage + BorrowMut<Self>>(
        storage: &mut S,
        from: Address,
        to: Address,
        token_id: U256,
    ) -> Result<(), Erc721Error> {
        // Implementation
    }

}

```
</CustomDetails>

<VanillaAdmonition type="info" title="ABI generation and inheritance">
  The Stylus SDK generates ABIs based on the methods that are available at the entrypoint contract.
  When using trait-based composition, make sure that all methods you want exposed in the ABI are
  properly included through the #[implements] attribute.
</VanillaAdmonition>

## Methods search order

When using trait-based composition, it's important to understand the order in which methods are searched:

1. The search starts in the type that uses the `#[entrypoint]` macro
2. If the method is not found, the search continues in the implemented traits, in the order specified in the `#[implements]` annotation
3. If the method is not found in any implemented trait, the call reverts

In a typical composition chain:

- Calling a method first searches in the contract itself
- If not found there, it looks in the first trait specified in the inheritance list
- If still not found, it searches in the next trait in the list
- This continues until the method is found or all possibilities are exhausted
```

---

## .mdx (how-tos/verifying-contracts-arbiscan.mdx)
---
title: 'How to verify Stylus contracts on Arbiscan'
description: 'This page provides a step-by-step guide to verifying Stylus contracts on Arbiscan, including contract details, source code submission, and handling previously verified contracts.'
author: mahsamoosavi
sme: mahsamoosavi
target_audience: 'Developers deploying smart contracts using Stylus'
content_type: how-to
sidebar_position: 1
---

import ImageZoom from '@site/src/components/ImageZoom';

This how-to will show you how to verify deployed contracts using Arbiscan, Arbitrum's block explorer.

Here's an example of a verified contract: the [English Auction Stylus contract](https://github.com/OffchainLabs/stylus-english-auction), which has been verified on Arbitrum Sepolia. You can view the verified contract [here](https://sepolia.arbiscan.io/address/0xe85a046fd3ea22ceeb3caef3a0d38123eecbe3ca).

You can also see a list of all Stylus contracts verified on Arbiscan by visiting:

- [Verified Stylus Contracts on Arbitrum One](https://arbiscan.io/contractsVerified?filter=stylus).
- [Verified Stylus Contracts on Arbitrum Sepolia](https://sepolia.arbiscan.io/contractsVerified?filter=stylus).

Here are the steps to take to verify a contract on Arbiscan:

## Step 1: Navigate to the verification page

You have two options to access the contract verification page on Arbiscan:

1. **Direct link:** Visit [Arbiscan Verify Contract](https://arbiscan.io/verifyContract) to go directly to the verification form. This option is ideal if you already have the contract address and details ready.
2. **From the contract page:** If you're viewing the contract's page on Arbiscan:
   - Go to the **Contract** tab.
   - Click on **Verify and Publish**.

<ImageZoom
  src="/img/stylus-arbiscan-verification-1.png"
  alt="Verify through the contract page"
  className="img-600px"
/>

Both methods will take you to the contract verification form, where you can proceed to the next step.

## Step 2: Enter the contract's details

You will need to fill in the following fields on the contract verification page:

- **Contract address**: Enter the contract address you want to verify.
- **Compiler type**: Select **Stylus** for Stylus contracts.
- **Compiler version**: Choose the `cargo stylus` version that was used to deploy the contract.
- **Open source license type**: Select the appropriate license for your contract.

<ImageZoom
  src="/img/stylus-arbiscan-verification-2.png"
  alt="Enter contract details"
  className="img-600px"
/>

## Step 3: Submit source code

After entering the contract details, you’ll need to provide the contract's source code:

- **Manual submission**: Copy and paste the source code into the provided text box.
- **Fetch from GitHub (Recommended)**: It's recommended to use the **Fetch from Git** option, as it's easier and helps automate the process. However, note that contracts located in subdirectories of the repository cannot be verified. Ensure that the contract's code is placed directly in the repository's root for verification to succeed.

<ImageZoom
  src="/img/stylus-arbiscan-verification-3.png"
  alt="Fetch source code"
  className="img-600px"
/>

## Step 4: Set EVM version

The **EVM Version to Target** can be left as default unless specific requirements dictate otherwise.

<ImageZoom
  src="/img/stylus-arbiscan-verification-4.png"
  alt="Verify and publish"
  className="img-600px"
/>

## Step 5: Verify and publish

Click **Verify and Publish**. The verification process will take a few seconds. Refresh the contract page, and if successful, the contract will be marked as verified.

<ImageZoom src="/img/stylus-arbiscan-verification-5.png" alt="Verified" className="img-600px" />

## Behavior when deploying a verified contract

When deploying another instance of a previously verified contract, if the bytecode matches, Arbiscan will automatically link the new instance to the verified source code, displaying a message like:

> "This contract matches the deployed Bytecode of the Source Code for Contract [verified contract address]."

However, the new contract will still appear as "Not Verified" until you explicitly verify it.

---

## .mdx (how-tos/verifying-contracts.mdx)
---
title: 'How to verify contracts for Stylus contracts'
sidebar_label: 'Verify contracts'
description: 'Learn how to verify stylus contracts'
author: rauljordan
sme: rauljordan
target_audience: 'Developers learning how to use Stylus'
sidebar_position: 2
user_story: 'As a current or prospective Stylus user, I want to learn how to make sure my Stylus contracts deployment are reproducible by anyone running the same environment.'
content_type: how-to
---

:::caution
This page will walk you through how to verify your Stylus contracts locally. Stylus contract verification is also available on [Arbiscan](https://arbiscan.io/). Please note, however, that Stylus contract verification on Arbiscan is only supported for Stylus contracts deployed using `cargo-stylus` 0.5.0 or higher.
:::

## Background

Stylus contracts written in Rust and deployed onchain can be verified against a local codebase by using the `cargo stylus` tool.

## Goals

- To ensure Stylus contract deployments are reproducible by anyone who is running the same architecture as the deployed item
- To sandbox the reproducible environment and standardize it as much as possible to prevent foot guns
- To guarantee that programs reproducibly deployed with a cargo stylus version >= 0.4.2 are verifiable

## Opting out

By default, `cargo stylus deploy` is reproducible as it runs in a Docker container. Users can opt-out by specifying `--no-verify` as a flag.

## Reproducible deployments

Required knowledge and setup:

- System architecture of your host computer (x86 / ARM)
- The git commit of your project used for deployment
- A **Rust** stylus project, such as [OffchainLabs/stylus-hello-world](https://github.com/OffchainLabs/stylus-hello-world) which contains a `rust-toolchain.toml` file
- Your cargo stylus version (run `cargo stylus --version` to obtain this value)
- [Docker](https://www.docker.com/) installed and running on your machine

Your project's [toolchain](https://rust-lang.github.io/rustup/overrides.html#the-toolchain-file) file must contain the Rust version you wish to use for your deployment, such as `major.minor.patch`

```
[toolchain]
channel = "1.79.0"
```

It **cannot** be `stable`, `nightly`, or `beta` by itself, as a specific version must be added. For instance, you can specify `nightly-YYYY-MM-DD` or `major.minor.patch` for your channel. This is so that deployments have a very specific version to prevent potential mismatches from being more generic.

```
# Replace {PRIV_KEY} with your actual private key or set it as a local variable
cargo stylus deploy --private-key={PRIV_KEY} --verbose
```

Upon completion, you will obtain the deployment transaction hash:

```
deployment tx hash: 0x1d8ae97e245e1db21dd188e5b64ad9025c1fb4e5f82a8d38bc8ae2b7a387600b
```

Save this transaction hash, as verifiers will need it.

## Reproducible verification

To verify a program, the verifier will need Docker installed and also know:

- System architecture the deployer used (x86 / ARM). Note: ARM devices that can emulate x86, such as Mac M series via Rosetta, can verify x86 Stylus deployments
- The git commit of the project the deployer used
- Your cargo stylus version the deployer used
- The deployment transaction hash

Navigate to the project's directory and check out the git commit that was used at deployment. Ensure your `cargo stylus --version` matches what the deployer used.

```
# Replace {DEPLOYMENT_TX_HASH} with the actual DEPLOYMENT_TX_HASH or set it as a local variable
cargo stylus verify --deployment-tx={DEPLOYMENT_TX_HASH}
```

This command will run the verification pipeline through a Docker environment, recreate the project metadata hash, and verify that the deployed program matches what the command reconstructed locally.

## How it works

On deployment, a `keccak256` hash is created from the contents of all Rust source files in the project, sorted by file path, along with a rust-toolchain.toml, Cargo.toml and Cargo.lock files by default. This hash is injected in as a custom section of the user WASM's code. This means all data in the source files will be used for reproducible verification of a Stylus contract, including code comments.

This means the `codehash` onchain of the program will change due to this deployment metadata hash.

The verification routine fetches the deployment transaction by hash via RPC, then attempts to build the local project to reconstruct the deployment init code and WASM using cargo build. It then checks that the deployment transaction data matches the created init code.

## Important details

**Docker image**
The docker container used for reproducibility standardizes all builds to x86, and it looks like this:

```shell
FROM --platform=linux/amd64 rust:1.79 as builder
RUN rustup toolchain install $VERSION-x86_64-unknown-linux-gnu
RUN rustup default $VERSION-x86_64-unknown-linux-gnu
RUN rustup target add wasm32-unknown-unknown
RUN rustup target add wasm32-wasi
RUN rustup target add x86_64-unknown-linux-gnu
RUN cargo install cargo-stylus
```

The docker container uses the `rust:1.79` version as a base for all projects. This will install cargo tooling and rust targets, but the toolchain actually used for compilation will be specified by the project being deployed in its `rust-toolchain.toml` file.

For instance, **a future toolchain can be used** despite the base image being 1.79, as when cargo stylus is installed, it will use that particular toolchain. Future cargo stylus updates could update this base image but may not impact the compiled WASM as the image will be using the specified toolchain. However, this is why knowing the specific cargo stylus version used for the reproducible verification from the deployer is important.

**The build toolchain**

All verifiable Stylus contracts in Rust _must_ have a standard [rust-toolchain.toml](https://rust-lang.github.io/rustup/overrides.html#the-toolchain-file) file which specifies the channel for their deployment. It **cannot** be `stable`, `nightly`, or `beta` by itself, as a specific version must be added. For instance, you can specify `nightly-YYYY-MM-DD` or `major.minor.patch` for your channel. This is so that deployments have a very specific version to prevent potential mismatches from being more generic.

---

## .mdx (overview.mdx)
---
id: stylus-overview
title: Write Stylus Contracts
sidebar_label: Write Stylus contracts
---

import Card from '@site/src/components/Cards/Card';

# Write Stylus Contracts

Let's learn how to write contracts with Stylus!

<div
  style={{
    display: 'grid',
    gridTemplateColumns: 'repeat(auto-fit, minmax(300px, 1fr))',
    gap: '20px',
  }}
>
  <Card
    title="A gentle introduction"
    description="Start with the basics of Stylus contracts."
    href="https://docs.arbitrum.io/stylus/gentle-introduction"
  />
  <Card
    title="Quickstart (Rust)"
    description="Get started quickly with Rust."
    href="https://docs.arbitrum.io/stylus/quickstart"
  />
  <Card
    title="Testnet"
    description="Explore the testnet environment."
    href="https://docs.arbitrum.io/stylus/reference/testnet-information"
  />
  <Card
    title="Stylus by example"
    description="Learn Stylus through examples."
    href="https://stylus-by-example.org"
  />
  <Card
    title="Stylus Rust SDK"
    description="Dive into the Stylus Rust SDK."
    href="https://docs.arbitrum.io/stylus/reference/rust-sdk-guide"
  />
  <Card
    title="Gas, ink and caching"
    description="Learn about gas, ink, and caching strategies."
    href="https://docs.arbitrum.io/stylus/concepts/gas-metering"
  />
  <Card
    title="CLI tools (cargo-stylus)"
    description="Master the CLI tools for Stylus."
    href="/stylus/cli-tools-overview"
  />
  <Card
    title="Run a Stylus dev node"
    description="Set up and run a development node."
    href="https://docs.arbitrum.io/run-arbitrum-node/run-local-dev-node"
  />
  <Card
    title="Other supported languages"
    description="Explore other languages supported by Stylus."
    href="https://docs.arbitrum.io/stylus/reference/stylus-sdk"
  />
  <Card
    title="Troubleshooting"
    description="Find solutions to common issues."
    href="https://docs.arbitrum.io/stylus/troubleshooting-building-stylus"
  />
  <Card
    title="Source code repository"
    description="Check out the source code."
    href="https://github.com/OffchainLabs/stylus"
  />
  <Card
    title="Public preview"
    description="View the public preview of Stylus."
    href="https://docs.arbitrum.io/stylus/concepts/public-preview-expectations"
  />
</div>

---

## .mdx (partials/_stylus-no-multi-inheritance-banner-partial.mdx)
:::info

Stylus doesn't support contract multi-inheritance yet.

:::

---

## .mdx (partials/_stylus-public-preview-banner-partial.md)
:::info ALPHA RELEASE, PUBLIC PREVIEW DOCS

Stylus is currently tagged as a `release-candidate`. The code has been audited, and testing in production environments is underway. Caution should be taken when used in production scenarios. This documentation is currently in [public preview](/stylus/concepts/public-preview-expectations).

To provide feedback, click the **Request an update** button at the top of this document, [join the Arbitrum Discord](https://discord.gg/arbitrum), or reach out to our team directly by completing [this form](http://bit.ly/3yy6EUK).

:::

---

## .mdx (quickstart.mdx)
---
id: quickstart
title: 'Quickstart: write a smart contract in Rust using Stylus'
description: 'Leads a developer from 0 to 1 writing and deploying a smart contract in Rust using Stylus'
author: chrisco512, anegg0
sme: chrisco512, anegg0
sidebar_position: 2
target_audience: Developers writing Stylus contracts in Rust using Stylus
---

import ImageZoom from '@site/src/components/ImageZoom';

<ImageZoom src="/img/stylus-primary.svg" alt="" className="img-100px" />

This guide will get you started with <a data-quicklook-from="stylus">Stylus</a>' basics. We'll cover the following steps:

1. [Setting up your development environment](./quickstart#setting-up-your-development-environment)
2. [Creating a Stylus project with cargo stylus](./quickstart#creating-a-stylus-project-with-cargo-stylus)
3. [Checking the validity of your contract](./quickstart#checking-if-your-stylus-project-is-valid)
4. [Deploying your contract](./quickstart#deploying-your-contract)
5. [Exporting your contract's ABIs](#exporting-the-solidity-abi-interface)
6. [Calling your contract](./quickstart#calling-your-contract)
7. [Sending a transaction to your contract](./quickstart#sending-a-transaction-to-your-contract)

## Setting up your development environment

### Prerequisites

<details>
<summary>Rust toolchain</summary>

Follow the instructions on [Rust Lang's installation page](https://www.rust-lang.org/tools/install) to install a complete Rust toolchain (v1.81 or newer) on your system. After installation, ensure you can access the programs `rustup`, `rustc`, and `cargo` from your preferred terminal application.

</details>

<details>
<summary>VS Code</summary>

We recommend [VSCode](https://code.visualstudio.com/) as the IDE of choice for its excellent Rust support, but feel free to use another text editor or IDE if you're comfortable with those.

Some helpful VS Code extensions for Rust development:

- [rust-analyzer](https://marketplace.visualstudio.com/items?itemName=rust-lang.rust-analyzer): Provides advanced features like smart code completion and on-the-fly error checks
- [Error Lens](https://marketplace.visualstudio.com/items?itemName=usernamehw.errorlens): Immediately highlights errors and warnings in your code
- [Even Better TOML](https://marketplace.visualstudio.com/items?itemName=tamasfe.even-better-toml): Improves syntax highlighting and other features for TOML files, often used in Rust projects
- [Dependi](https://marketplace.visualstudio.com/items?itemName=fill-labs.dependi): Helps manage Rust crate versions directly from the editor

 </details>

<details>
<summary>Docker</summary>

The testnode we will use as well as some `cargo stylus` commands require Docker to operate.

You can download Docker from [Docker's website](https://www.docker.com/products/docker-desktop).

</details>

<details>
<summary>Foundry's Cast</summary>

Foundry's Cast is a command-line tool that allows you to interact with your EVM contracts. You need to [install the Foundry CLI](https://getfoundry.sh) to use Cast.

</details>

<details>
<summary>Nitro devnode</summary>

Stylus is available on Arbitrum Sepolia, but we'll use nitro devnode which has a pre-funded wallet saving us the effort of wallet provisioning or running out of tokens to send transactions.

```shell title="Install your devnode"
git clone https://github.com/OffchainLabs/nitro-devnode.git
cd nitro-devnode
```

```shell title="Launch your devnode"
./run-dev-node.sh
```

</details>

## Creating a Stylus project with cargo stylus

[cargo stylus](https://github.com/OffchainLabs/cargo-stylus/blob/main/main/VALID_WASM.md) is a CLI toolkit built to facilitate the development of Stylus contracts.

It is available as a plugin to the standard cargo tool used for developing Rust programs.

### Installing cargo stylus

In your terminal, run:

```shell
cargo install --force cargo-stylus
```

Add WASM ([WebAssembly](https://webassembly.org/)) as a build target for the specific Rust toolchain you are using. The below example sets your default Rust toolchain to 1.80 as well as adding the WASM build target:

```shell
rustup default 1.80
rustup target add wasm32-unknown-unknown --toolchain 1.80
```

You can verify that cargo stylus is installed by running `cargo stylus --help` in your terminal, which will return a list of helpful commands, we will use some of them in this guide:

```shell title="cargo stylus --help returns:"
Cargo command for developing Stylus projects

Usage: cargo stylus <COMMAND>

Commands:
  new         Create a new Stylus project
  init        Initializes a Stylus project in the current directory
  export-abi  Export a Solidity ABI
  activate    Activate an already deployed contract [aliases: a]
  cache       Cache a contract using the Stylus CacheManager for Arbitrum chains
  check       Check a contract [aliases: c]
  deploy      Deploy a contract [aliases: d]
  verify      Verify the deployment of a Stylus contract [aliases: v]
  cgen        Generate c code bindings for a Stylus contract
  replay      Replay a transaction in gdb [aliases: r]
  trace       Trace a transaction [aliases: t]
  help        Print this message or the help of the given command(s)

Options:
  -h, --help     Print help
  -V, --version  Print version
```

### Creating a project

Let's create our first Stylus project by running:

```shell
cargo stylus new <YOUR_PROJECT_NAME>
```

`cargo stylus new` generates a starter template that implements a Rust version of the Solidity `Counter` smart contract example:

```solidity
// SPDX-License-Identifier: MIT
pragma solidity >=0.4.22 <0.9.0;

contract Counter {

   uint count;

  function setCount() public {
    count = count + 1;
  }

  function getCount() view public returns(uint) {
      return count;
  }
}
```

At this point, you can move on to the next step of this guide or develop your first Rust smart contract. Feel free to use the [Stylus Rust SDK reference section](./reference/overview) as a starting point; it offers many examples to help you quickly familiarize yourself with Stylus.

## Checking if your Stylus project is valid

By running `cargo stylus check` against your first contract, you can check if your program can be successfully **deployed and activated** onchain.

:::warning Important
Ensure your Docker service runs so this command works correctly.
:::

```shell
cargo stylus check
```

`cargo stylus check` executes a dry run on your project by compiling your contract to WASM and verifying if it can be deployed and activated onchain.

If the command above fails, you'll see detailed information about why your contract would be rejected:

```shell
Reading WASM file at bad-export.wat
Compressed WASM size: 55 B
Stylus checks failed: program pre-deployment check failed when checking against
ARB_WASM_ADDRESS 0x0000…0071: (code: -32000, message: program activation failed: failed to parse program)

Caused by:
    binary exports reserved symbol stylus_ink_left

Location:
    prover/src/binary.rs:493:9, data: None
```

The contract can fail the check for various reasons (on compile, deployment, etc...). Reading the [Invalid Stylus WASM Contracts explainer](https://github.com/OffchainLabs/cargo-stylus/blob/main/main/VALID_WASM.md) can help you understand what makes a WASM contract valid or not.

If your contract succeeds, you'll see something like this:

```shell
Finished release [optimized] target(s) in 1.88s
Reading WASM file at hello-stylus/target/wasm32-unknown-unknown/release/hello-stylus.wasm
Compressed WASM size: 3 KB
Program succeeded Stylus onchain activation checks with Stylus version: 1
```

Note that running `cargo stylus check` may take a few minutes, especially if you're verifying a contract for the first time.

See `cargo stylus check --help` for more options.

## Deploying your contract

Once you're ready to deploy your contract onchain, `cargo stylus deploy` will help you with the deployment and its gas estimation.

### Estimating gas

Note: For every transaction, we'll use the testnode pre-funded wallet, you can use `0xb6b15c8cb491557369f3c7d2c287b053eb229daa9c22138887752191c9520659` as your private key.

You can estimate the gas required to deploy your contract by running:

```shell
cargo stylus deploy \
  --endpoint='http://localhost:8547' \
  --private-key="0xb6b15c8cb491557369f3c7d2c287b053eb229daa9c22138887752191c9520659" \
  --estimate-gas
```

The command should return something like this:

```shell
deployment tx gas: 7123737
gas price: "0.100000000" gwei
deployment tx total cost: "0.000712373700000000" ETH
```

### Deployment

Let's move on to the contract's actual deployment. Two transactions will be sent onchain: the contract deployment and its activation.

```shell
cargo stylus deploy \
  --endpoint='http://localhost:8547' \
  --private-key="0xb6b15c8cb491557369f3c7d2c287b053eb229daa9c22138887752191c9520659"
```

Once the deployment and activations are successful, you'll see an output similar to this:

```shell
deployed code at address: 0x33f54de59419570a9442e788f5dd5cf635b3c7ac
deployment tx hash: 0xa55efc05c45efc63647dff5cc37ad328a47ba5555009d92ad4e297bf4864de36
wasm already activated!
```

Make sure to save the contract's deployment address for future interactions!

More options are available for sending and outputting your transaction data. See `cargo stylus deploy --help` for more details.

## Exporting the Solidity ABI interface

The cargo stylus tool makes it easy to export your contract's ABI using `cargo stylus export-abi`.

This command returns the Solidity ABI interface of your smart contract. If you have been running `cargo stylus new` without modifying the output, `cargo stylus export-abi` will return:

```shell
/**
 * This file was automatically generated by Stylus and represents a Rust program.
 * For more information, please see [The Stylus SDK](https://github.com/OffchainLabs/stylus-sdk-rs).
 */

// SPDX-License-Identifier: MIT-OR-APACHE-2.0
pragma solidity ^0.8.23;

interface ICounter {
    function number() external view returns (uint256);

    function setNumber(uint256 new_number) external;

    function mulNumber(uint256 new_number) external;

    function addNumber(uint256 new_number) external;

    function increment() external;
}
```

Ensure you save the console output to a file that you'll be able to use with your <a data-quicklook-from="dapp"><a data-quicklook-from="dapp">dApp</a></a>.

## Interacting with your Stylus contract

Stylus contracts are EVM-compatible, you can interact with them with your tool of choice, such as [Hardhat](https://hardhat.org/), [Foundry's Cast](https://book.getfoundry.sh/cast/), or any other Ethereum-compatible tool.

In this example, we'll use Foundry's Cast to send a call and then a transaction to our contract.

### Calling your contract

Our contract is a counter; in its initial state, it should store a counter value of `0`.
You can call your contract so it returns its current counter value by sending it the following command:

```shell title="Call to the function: number()(uint256)"
cast call --rpc-url 'http://localhost:8547' --private-key 0xb6b15c8cb491557369f3c7d2c287b053eb229daa9c22138887752191c9520659 \
[deployed-contract-address] "number()(uint256)"
```

Let's break down the command:

- `cast call` command sends a call to your contract
- The `--rpc-url` option is the `RPC URL` endpoint of our testnode: http://localhost:8547
- The `--private-key` option is the private key of our pre-funded development account. It corresponds to the address `0x3f1eae7d46d88f08fc2f8ed27fcb2ab183eb2d0e`
- The [deployed-contract-address] is the address we want to interact with, it's the address that was returned by `cargo stylus deploy`
- `number()(uint256)` is the function we want to call in Solidity-style signature. The function returns the counter's current value

```shell title="Calling 'number()(uint256)' returns:"
0
```

The `number()(uint256)` function returns a value of `0`, the contract's initial state.

### Sending a transaction to your contract

Let's increment the counter by sending a transaction to your contract's `increment()` function.
We'll use Cast's `send` command to send our transaction.

```shell title="Sending a transaction to the function: increment()"
cast send --rpc-url 'http://localhost:8547' --private-key 0xb6b15c8cb491557369f3c7d2c287b053eb229daa9c22138887752191c9520659 \
[deployed-contract-address] "increment()"
```

```shell title="Transaction returns:"
blockHash               0xfaa2cce3b9995f3f2e2a2f192dc50829784da9ca4b7a1ad21665a25b3b161f7c
blockNumber             20
contractAddress
cumulativeGasUsed       97334
effectiveGasPrice       100000000
from                    0x3f1Eae7D46d88F08fc2F8ed27FCb2AB183EB2d0E
gasUsed                 97334
logs                    []
logsBloom               0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
root
status                  1 (success)
transactionHash         0x28c6ba8a0b9915ed3acc449cf6c645ecc406a4b19278ec1eb67f5a7091d18f6b
transactionIndex        1
type                    2
blobGasPrice
blobGasUsed
authorizationList
to                      0x11B57FE348584f042E436c6Bf7c3c3deF171de49
gasUsedForL1             "0x0"
l1BlockNumber             "0x1223"
```

Our transactions returned a status of `1`, indicating success, and the counter has been incremented (you can verify this by calling your contract's `number()(uint256)` function again).

## Testing your contract

The Stylus testing framework includes `TestVM`, a simulation of the Stylus execution environment that enables you to test your contracts without deploying them. Here's a simple example of how
to test the counter contract:

```rust
#[cfg(test)]
mod test {
    use super::*;
    use alloy_primitives::address;
    use stylus_sdk::testing::*;

    #[test]
    fn test_counter_operations() {
        // Set up test environment
        let vm = TestVM::default();
        // Initialize your contract
        let mut contract = Counter::from(&vm);

        // Test initial state
        assert_eq!(contract.number().unwrap(), U256::ZERO);

        // Test increment
        contract.increment().unwrap();
        assert_eq!(contract.number().unwrap(), U256::from(1));

        // Test set number
        contract.set_number(U256::from(5)).unwrap();
        assert_eq!(contract.number().unwrap(), U256::from(5));
    }
}
```

To enable testing, you'll need to add the following to your `Cargo.toml`:

```toml
[dev-dependencies]
stylus-sdk = { version = "0.8.4", features = ["stylus-test"] }
```

### Running your tests

You can run your tests using the standard Rust test command:

```shell
cargo test
```

The testing framework allows you to:

- Simulate transaction context and block information
- Test contract storage operations
- Verify state transitions
- Mock contract-to-contract interactions
- Test various scenarios without deployment costs

For more advanced testing techniques and best practices, see the [Testing contracts with Stylus guide](./how-tos/testing-contracts).

## Conclusion

Congratulations! You've successfully initialized, deployed, and interacted with your first contract using Stylus and Rust.

Feel free to explore the [Stylus Rust SDK reference](./reference/overview) for more information on using Stylus in your Arbitrum projects.

---

## .mdx (recommended-libraries.md)
---
id: recommended-libraries
title: Recommended Libraries
sidebar_label: Use Rust Crates
---

# Recommended libraries

## Using public Rust crates

Rust provides a package registry at [crates.io](https://crates.io/), which lets developers conveniently access a plethora of open source libraries to utilize as dependencies in their code. Stylus Rust contracts can take advantage of these crates to simplify their development workflow.

While **crates.io** is a fantastic resource, many of these libraries were not designed with the constraints of a blockchain environment in mind. Some produce large binaries that exceed the 24KB compressed size limit of WASM smart contracts on Arbitrum. Many also take advantage of unsupported features such as:

- Random numbers
- Multi threading
- Floating point numbers and operations

Using the standard Rust library often bloats contract sizes beyond the maximum size. For this reason, libraries designated as `no_std` are typically much stronger candidates for usage as a smart contract dependency. **crates.io** has a special tag for marking crates as `no_std`; however, it's not universally used. Still, it can be a good starting point for locating supported libraries. See ["No standard library"](https://crates.io/categories/no-std) crates for more details.

## Curated crates

To save developers time on smart contract development for common dependencies, we've curated a list of crates and utilities that we found helpful. Keep in mind that we have not audited this code, and you should always be mindful about pulling dependencies into your codebase, whether they've been audited or not. We provide this list for you to use at your discretion and risk.

- [`rust_decimal`](https://crates.io/crates/rust_decimal): Decimal number implementation written in pure Rust. Suitable for financial and fixed-precision calculations
- [`special`](https://crates.io/crates/special): The package provides special functions, which are mathematical functions with special names due to their common usage, such as `sin`, `ln`, `tan`, etc.
- [`hashbrown`](https://crates.io/crates/hashbrown): Rust port of Google's SwissTable hash map
- [`time`](https://crates.io/crates/time): Date and time library
- [`hex`](https://crates.io/crates/hex): Encoding and decoding data into/from hexadecimal representation

We'll be adding more libraries to this list as we find them. Feel free to suggest an edit if you know of any great crates that would be generally useful here.

---

## .mdx (reference/opcode-hostio-pricing.md)
---
title: 'Gas and ink costs'
description: 'A reference of how much opcodes and host I/Os cost in Stylus, with measurements in ink and gas.'
author: rachel-bousfield
sme: rachel-bousfield
target_audience: 'Developers deploying smart contracts using Stylus.'
sidebar_position: 3
---

This reference provides the latest gas and ink costs for specific WASM opcodes and host I/Os when using Stylus. For a conceptual introduction to Stylus gas and ink, see [Gas and ink (Stylus)](/stylus/concepts/gas-metering).

## Opcode costs

The Stylus VM charges for WASM opcodes according to the following table, which was determined via a conservative statistical analysis and is expected to change as Stylus matures. Prices may fluctuate across upgrades as our analysis evolves and optimizations are made.

| Hex    | Opcode        | Ink          | Gas            | Notes                         |
| ------ | ------------- | ------------ | -------------- | ----------------------------- |
| 0x00   | Unreachable   | 1            | 0.0001         |                               |
| 0x01   | Nop           | 1            | 0.0001         |                               |
| 0x02   | Block         | 1            | 0.0001         |                               |
| 0x03   | Loop          | 1            | 0.0001         |                               |
| 0x04   | If            | 765          | 0.0765         |                               |
| 0x05   | Else          | 1            | 0.0001         |                               |
| 0x0b   | End           | 1            | 0.0001         |                               |
| 0x0c   | Br            | 765          | 0.0765         |                               |
| 0x0d   | BrIf          | 765          | 0.0765         |                               |
| 0x0e   | BrTable       | 2400 + 325x  | 0.24 + 0.0325x | Cost varies with table size   |
| 0x0f   | Return        | 1            | 0.0001         |                               |
| 0x10   | Call          | 3800         | 0.38           |                               |
| 0x11   | CallIndirect  | 13610 + 650x | 1.361 + 0.065x | Cost varies with no. of args  |
| 0x1a   | Drop          | 9            | 0.0009         |                               |
| 0x1b   | Select        | 1250         | 0.125          |                               |
| 0x20   | LocalGet      | 75           | 0.0075         |                               |
| 0x21   | LocalSet      | 210          | 0.0210         |                               |
| 0x22   | LocalTee      | 75           | 0.0075         |                               |
| 0x23   | GlobalGet     | 225          | 0.0225         |                               |
| 0x24   | GlobalSet     | 575          | 0.0575         |                               |
| 0x28   | I32Load       | 670          | 0.067          |                               |
| 0x29   | I64Load       | 680          | 0.068          |                               |
| 0x2c   | I32Load8S     | 670          | 0.067          |                               |
| 0x2d   | I32Load8U     | 670          | 0.067          |                               |
| 0x2e   | I32Load16S    | 670          | 0.067          |                               |
| 0x2f   | I32Load16U    | 670          | 0.067          |                               |
| 0x30   | I64Load8S     | 680          | 0.068          |                               |
| 0x31   | I64Load8U     | 680          | 0.068          |                               |
| 0x32   | I64Load16S    | 680          | 0.068          |                               |
| 0x33   | I64Load16U    | 680          | 0.068          |                               |
| 0x34   | I64Load32S    | 680          | 0.068          |                               |
| 0x35   | I64Load32U    | 680          | 0.068          |                               |
| 0x36   | I32Store      | 825          | 0.0825         |                               |
| 0x37   | I64Store      | 950          | 0.095          |                               |
| 0x3a   | I32Store8     | 825          | 0.0825         |                               |
| 0x3b   | I32Store16    | 825          | 0.0825         |                               |
| 0x3c   | I64Store8     | 950          | 0.095          |                               |
| 0x3d   | I64Store16    | 950          | 0.095          |                               |
| 0x3e   | I64Store32    | 950          | 0.095          |                               |
| 0x3f   | MemorySize    | 3000         | 0.3            |                               |
| 0x40   | MemoryGrow    | 8050         | 0.805          |                               |
| 0x41   | I32Const      | 1            | 0.0001         |                               |
| 0x42   | I64Const      | 1            | 0.0001         |                               |
| 0x45   | I32Eqz        | 170          | 0.017          |                               |
| 0x46   | I32Eq         | 170          | 0.017          |                               |
| 0x47   | I32Ne         | 170          | 0.017          |                               |
| 0x48   | I32LtS        | 170          | 0.017          |                               |
| 0x49   | I32LtU        | 170          | 0.017          |                               |
| 0x4a   | I32GtS        | 170          | 0.017          |                               |
| 0x4b   | I32GtU        | 170          | 0.017          |                               |
| 0x4c   | I32LeS        | 170          | 0.017          |                               |
| 0x4d   | I32LeU        | 170          | 0.017          |                               |
| 0x4e   | I32GeS        | 170          | 0.017          |                               |
| 0x4f   | I32GeU        | 170          | 0.017          |                               |
| 0x50   | I64Eqz        | 225          | 0.0225         |                               |
| 0x51   | I64Eq         | 225          | 0.0225         |                               |
| 0x52   | I64Ne         | 225          | 0.0225         |                               |
| 0x53   | I64LtS        | 225          | 0.0225         |                               |
| 0x54   | I64LtU        | 225          | 0.0225         |                               |
| 0x55   | I64GtS        | 225          | 0.0225         |                               |
| 0x56   | I64GtU        | 225          | 0.0225         |                               |
| 0x57   | I64LeS        | 225          | 0.0225         |                               |
| 0x58   | I64LeU        | 225          | 0.0225         |                               |
| 0x59   | I64GeS        | 225          | 0.0225         |                               |
| 0x5a   | I64GeU        | 225          | 0.0225         |                               |
| 0x67   | I32Clz        | 210          | 0.021          |                               |
| 0x68   | I32Ctz        | 210          | 0.021          |                               |
| 0x69   | I32Popcnt     | 2650         | 0.265          |                               |
| 0x6a   | I32Add        | 70           | 0.007          |                               |
| 0x6b   | I32Sub        | 70           | 0.007          |                               |
| 0x6c   | I32Mul        | 160          | 0.016          |                               |
| 0x6d   | I32DivS       | 1120         | 0.112          |                               |
| 0x6e   | I32DivU       | 1120         | 0.112          |                               |
| 0x6f   | I32RemS       | 1120         | 0.112          |                               |
| 0x70   | I32RemU       | 1120         | 0.112          |                               |
| 0x71   | I32And        | 70           | 0.007          |                               |
| 0x72   | I32Or         | 70           | 0.007          |                               |
| 0x73   | I32Xor        | 70           | 0.007          |                               |
| 0x74   | I32Shl        | 70           | 0.007          |                               |
| 0x75   | I32ShrS       | 70           | 0.007          |                               |
| 0x76   | I32ShrU       | 70           | 0.007          |                               |
| 0x77   | I32Rotl       | 70           | 0.007          |                               |
| 0x78   | I32Rotr       | 70           | 0.007          |                               |
| 0x79   | I64Clz        | 210          | 0.021          |                               |
| 0x7a   | I64Ctz        | 210          | 0.012          |                               |
| 0x7b   | I64Popcnt     | 6000         | 0.6            |                               |
| 0x7c   | I64Add        | 100          | 0.01           |                               |
| 0x7d   | I64Sub        | 100          | 0.01           |                               |
| 0x7e   | I64Mul        | 160          | 0.016          |                               |
| 0x7f   | I64DivS       | 1270         | 0.127          |                               |
| 0x80   | I64DivU       | 1270         | 0.127          |                               |
| 0x81   | I64RemS       | 1270         | 0.127          |                               |
| 0x82   | I64RemU       | 1270         | 0.127          |                               |
| 0x83   | I64And        | 100          | 0.01           |                               |
| 0x84   | I64Or         | 100          | 0.01           |                               |
| 0x85   | I64Xor        | 100          | 0.01           |                               |
| 0x86   | I64Shl        | 100          | 0.01           |                               |
| 0x87   | I64ShrS       | 100          | 0.01           |                               |
| 0x88   | I64ShrU       | 100          | 0.01           |                               |
| 0x89   | I64Rotl       | 100          | 0.01           |                               |
| 0x8a   | I64Rotr       | 100          | 0.01           |                               |
| 0xa7   | I32WrapI64    | 100          | 0.01           |                               |
| 0xac   | I64ExtendI32S | 100          | 0.01           |                               |
| 0xad   | I64ExtendI32U | 100          | 0.01           |                               |
| 0xc0   | I32Extend8S   | 100          | 0.01           |                               |
| 0xc1   | I32Extend16S  | 100          | 0.01           |                               |
| 0xc2   | I64Extend8S   | 100          | 0.01           |                               |
| 0xc3   | I64Extend16S  | 100          | 0.01           |                               |
| 0xc4   | I64Extend32S  | 100          | 0.01           |                               |
| 0xfc0a | MemoryCopy    | 950 + 100x   | 0.095 + 0.01x  | Cost varies with no. of bytes |
| 0xfc0b | MemoryFill    | 950 + 100x   | 0.095 + 0.01x  | Cost varies with no. of bytes |

## Host I/O costs

Certain operations require suspending WASM execution so that the Stylus VM can perform tasks natively in the host. This costs about `0.84 gas` to do. Though we’ll publish a full specification later, the following table details the costs of simple operations that run in the host.

Note that the values in this table were determined via a conservative statistical analysis and are expected to change as Stylus matures. Prices may fluctuate across upgrades as our analysis evolves and optimizations are made.

| Host I/O         | Ink             | Gas            | Notes                      |
| ---------------- | --------------- | -------------- | -------------------------- |
| read_args        | 8400 + 5040b    | 0.84 + 0.504b  | `b` = bytes after first 32 |
| write_result     | 8400 + 16381b   | 0.84 + 1.6381b | `b` = bytes after first 32 |
| keccak           | 121800 + 21000w | 12.18 + 2.1w   | `w` = EVM words            |
| block_basefee    | 13440           | 1.344          |                            |
| block_coinbase   | 13440           | 1.344          |                            |
| block_gas_limit  | 8400            | 0.84           |                            |
| block_number     | 8400            | 0.84           |                            |
| block_timestmap  | 8400            | 0.84           |                            |
| chain_id         | 8400            | 0.84           |                            |
| contract_address | 13440           | 1.344          |                            |
| evm_gas_left     | 8400            | 0.84           |                            |
| evm_ink_left     | 8400            | 0.84           |                            |
| msg_reentrant    | 8400            | 0.84           |                            |
| msg_sender       | 13440           | 1.344          |                            |
| msg_value        | 13440           | 1.344          |                            |
| return_data_size | 8400            | 0.84           |                            |
| tx_ink_price     | 8400            | 0.84           |                            |
| tx_gas_price     | 13440           | 1.344          |                            |
| tx_origin        | 13440           | 1.344          |                            |
| console_log_text | 0               | 0              | debug-only                 |
| console_log      | 0               | 0              | debug-only                 |
| console_tee      | 0               | 0              | debug-only                 |
| null_host        | 0               | 0              | debug-only                 |

### See also

- [Gas and ink (Stylus)](/stylus/concepts/gas-metering): A conceptual introduction to the "gas" and "ink" primitives

---

## .mdx (reference/overview.md)
---
title: 'Stylus Rust SDK overview'
description: 'An overview of the features provided by the Stylus Rust SDK'
author: jose-franco
sme: jose-franco
sidebar_position: 1
target_audience: Developers using the Stylus Rust SDK to write and deploy smart contracts.
---

This section provides an in-depth overview of the features provided by the [Stylus Rust SDK](https://github.com/OffchainLabs/stylus-sdk-rs). For information about deploying Rust smart contracts, see the `cargo stylus` [CLI Tool](https://github.com/OffchainLabs/cargo-stylus). For a conceptual introduction to Stylus, see [Stylus: A Gentle Introduction](../gentle-introduction.mdx). To deploy your first Stylus smart contract using Rust, refer to the [Quickstart](../quickstart.mdx).

The Stylus Rust SDK is built on top of [Alloy](https://www.paradigm.xyz/2023/06/alloy), a collection of crates empowering the Rust Ethereum ecosystem. Because the SDK uses the same [Rust primitives for Ethereum types](https://docs.rs/alloy-primitives/latest/alloy_primitives/), Stylus is compatible with existing Rust libraries.

The Stylus Rust SDK has been audited in August 2024 at [commit #62bd831](https://github.com/OffchainLabs/stylus-sdk-rs/tree/62bd8318c7f3ab5be954cbc264f85bf2ba3f4b06) by Open Zeppelin which can be viewed [on our audits page](audit-reports.mdx).

This section contains a set of pages that describe a certain aspect of the Stylus Rust SDK, like how to work with [variables](https://stylus-by-example.org/basic_examples/variables), or what ways are there to [send ether](https://stylus-by-example.org/basic_examples/sending_ether). Additionally, there's also a page that compiles a set of [advanced features](/stylus/reference/rust-sdk-guide.md) that the Stylus Rust SDK provides.

Finally, there's also a [Stylus by example](https://stylus-by-example.org) portal available that provides most of the information included in this section, as well as many different example contracts.

---

## .mdx (reference/partials/_stylus-faucets.mdx)
## Faucets

Below you can find faucets for obtaining testnet `ETH`. If using a faucet on Ethereum Sepolia or Arbitrum Sepolia, your testnet `ETH` can be bridged to the Stylus testnet on the [Arbitrum Bridge](https://bridge.arbitrum.io/).

| Faucet Operator    | Faucet URL                                       | Chain            |
| ------------------ | ------------------------------------------------ | ---------------- |
| QuickNode          | https://faucet.quicknode.com/arbitrum/sepolia    | Arbitrum Sepolia |
| Alchemy            | https://www.alchemy.com/faucets/arbitrum-sepolia | Arbitrum Sepolia |
| Sepolia PoW Faucet | https://sepolia-faucet.pk910.de/                 | Ethereum Sepolia |

---

## .mdx (reference/project-structure.mdx)
---
title: 'Structure of a Rust Contract'
description: 'A quick overview of how contracts are structured with the Stylus Rust SDK'
author: chrisco
sme: chrisco
sidebar_position: 1
target_audience: Developers using the Stylus Rust SDK to write and deploy smart contracts.
---

Contracts in Rust are similar to contracts in Solidity. Each contract can contain declarations of State Variables, Functions, Function Modifiers, Events, Errors, Struct Types, and Enum Types. In addition, Rust contracts can import third-party packages from [crates.io](https://crates.io) as dependencies and use them for advanced functionality.

## Project layout

In the most basic example, this is how a Rust contract will be organized. The simplest way to get going with a new project is to follow the [Quickstart](https://docs.arbitrum.io/stylus/quickstart) guide, or if you've already installed all dependencies, just run `cargo stylus new <YOUR_PROJECT_NAME>` from your terminal to begin a new project. Once installed, your project will include the following required files:

```shell
- src
    - lib.rs
    - main.rs
- Cargo.toml
- rust-toolchain.toml
```

`src/lib.rs` is the root module of your contract's code. Here, you can import utilities or methods from internal or external modules, define the data layout of your contract's state variables, and define your contract's public API. This module must define a root data struct with the `#[entrypoint]` macro and provide an impl block annotated with `#[public]` to define public or external methods. See [First App](https://stylus-by-example.org/basic_examples/first_app) for an example of this. These macros are used to maintain [Solidity ABI](https://docs.soliditylang.org/en/v0.8.19/abi-spec.html#basic-design) compatibility to ensure that Rust contracts work with existing Solidity libraries and tooling.

`src/main.rs` is typically auto-generated by [cargo-stylus](https://github.com/OffchainLabs/cargo-stylus) and does not usually need to be modified. Its purpose is to assist with the generation of [JSON describing](https://docs.soliditylang.org/en/v0.8.19/abi-spec.html#json) your contract's public interface, for use with automated tooling and frontend frameworks.

`Cargo.toml` is a standard file that Rust projects use to define a package's name, repository location, etc, as well as import dependencies and define feature and build flags. From here, you can define required dependencies such as the [Stylus SDK](https://crates.io/crates/stylus-sdk) itself or import third-party packages from [crates.io](https://crates.io). See [First Steps with Cargo](https://doc.rust-lang.org/cargo/getting-started/first-steps.html) if you are new to Rust.

`rust-toolchain.toml` is used by public blockchain explorers, like [Arbiscan](https://arbiscan.io/), to assist with source code verification. To ensure that source code can be compiled deterministically, we use this file to include relevant metadata like what version of Rust was used. It can also be used to pin the project to a specific Rust version that it's compatible with.

Your contract may also include other dot files (such as `.gitignore`, `.env`, etc), markdown files for docs, or additional subfolders.

## State variables

Like Solidity, Rust contracts are able to define _state variables_. These are variables which are stored on the chain's _state trie_, which is essentially the chain's database. They differ from standard Rust variables in that they must implement the `Storage` trait from the Stylus SDK. This trait is used to layout the data in the trie in a Solidity-compatible fashion. The Stylus SDK provides Storage types for all Solidity primitives out-of-the-box, such as `StorageAddress`, `StorageU256`, etc. See [storage module](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/index.html#structs) for more information.

When working with state variables, you can either use Rust-style syntax or Solidity-style syntax to define your data schema. The `#[storage]` macro is used to define Rust-style state variables while `sol_storage!` macro is used for Solidity-style state variables. Both styles may have more than one struct but must annotate a single struct as the root struct with `#[entrypoint]` macro. Below are examples of each.

**Rust-style Schema**

```rust
use stylus_sdk::{prelude::*, storage::{StorageU256, StorageAddress}};

#[storage]
#[entrypoint]
pub struct MyContract {
    owner: StorageAddress,
    version: StorageU256,
}
```

**Solidity-style Schema**

```rust
use stylus_sdk::{prelude::*};

sol_storage! {
    #[entrypoint]
    pub struct MyContract {
        address owner;
        version: uint256,
    }
}
```

To read from state or write to it, getters and setters are used:

```rust
let new_count = self.count.get() + U256::from(1);
self.count.set(new_count);
```

See [Storage Data Types](https://stylus-by-example.org/basic_examples/storage_data_types) for more examples of this.

## Functions

Contract functions are defined by providing an `impl` block for your contract's `#[entrypoint]` struct and annotating that block with `#[public]` to make the functions part of the contract's public API. The first parameter of each function is `&self`, which references the struct annotated with `#[entrypoint]`, it's used for reading state variables. By default, methods are view-only and cannot mutate state. To make a function mutable and able to alter state, `&mut self` must be used. Internal methods can be defined on a separate impl block for the struct that is not annotated with `#[public]`. Internal methods can access state.

```rust
// Defines the public, external methods for your contract
// This impl block must be for the #[entrypoint] struct defined prior
#[public]
impl Counter {
    // By annotating first arg with &self, this indicates a view function
    pub fn get(&self) -> U256 {
        self.count.get()
    }

    // By annotating with &mut self, this is a mutable public function
    pub fn set_count(&mut self, count: U256) {
        self.count.set(count);
    }
}

// Internal methods (NOT part of public API)
impl Counter {
    fn add(a: U256, b: U256) -> U256 {
        a + b
    }
}
```

## Modules

Modules are a way to organize code into logical units. While your contract must have a `lib.rs` which defines your entrypoint struct, you can also define utility functions, structs, enums, etc., in modules and import them to use in your contract's methods.

For example, with this file structure:

```
- src
    - lib.rs
    - main.rs
- utils
    - mod.rs
- Cargo.toml
- rust-toolchain.toml
```

In `lib.rs`:

```rust
// import module
mod utils;

// ..other code
const score = utils::check_score();
```

See [Defining modules](https://doc.rust-lang.org/book/ch07-02-defining-modules-to-control-scope-and-privacy.html) in the Rust book for more info on modules and how to use them.

## Importing packages

Rust has a robust package manager for managing dependencies and importing third-party libraries to use in your smart contracts. These packages (called crates in Rust) are located at [crates.io](https://crates.io). To make use of a dependency in your code, you'll need to complete these steps:

Add the package name and version to your `Cargo.toml`:

```toml
# Cargo.toml
[package]
# ...package info here

[dependencies]
rust_decimal = "1.36.0"
```

Import the package into your contract:

```rust
// lib.rs
use rust_decimal_macros::dec;
```

Use imported types in your contract:

```rust
// create a fixed point Decimal value
let price = dec!(72.00);
```

Note, not all Rust crates are compatible with Stylus since they need to be compiled to WASM and used in a blockchain context, which is more limited than a desktop application. For instance, the `rand` crate is not usable, as there is no onchain randomness available to smart contracts. In addition, contracts cannot access functions that use networking or filesystem access features. There is also a need to be mindful of the size of the crates you import, since the default contract size limit is 24KB (compressed). Crates that do not use the standard library (`no_std` crates) tend to work best. See [Using public Rust crates](https://docs.arbitrum.io/stylus/recommended-libraries#using-public-rust-crates) for more important details on using public Rust crates as well as a curated list of crates that tend to work well for smart contract development.

## Events

Events are used to publicly log values to the EVM. They can be useful for users to understand what occurred during a transaction while inspecting a transaction on a public explorer, like [Arbiscan](https://arbiscan.io/).

```rust
sol! {
    event HighestBidIncreased(address bidder, uint256 amount);
}

#[public]
impl AuctionContract {
    pub fn bid() {
        // ...
        evm::log(HighestBidIncreased {
            bidder: Address::from([0x11; 20]),
            amount: U256::from(42),
        });
    }
}
```

## Errors

Errors allow you to define descriptive names for failure situations. These can be useful for debugging or providing users with helpful information for why a transaction may have failed.

```rust
sol! {
    error NotEnoughFunds(uint256 request, uint256 available);
}

#[derive(SolidityError)]
pub enum TokenErrors {
    NotEnoughFunds(NotEnoughFunds),
}

#[public]
impl Token {
    pub fn transfer(&mut self, to: Address, amount: U256) -> Result<(), TokenErrors> {
        const balance = self.balances.get(msg::sender());
        if (balance < amount) {
            return Err(TokenErrors::NotEnoughFunds(NotEnoughFunds {
                request: amount,
                available: balance,
            }));
        }
        // .. other code here
    }
}
```

---

## .mdx (reference/rust-sdk-guide.md)
---
title: 'Stylus Rust SDK advanced features'
description: 'Advanced features of the Stylus Rust SDK'
author: rachel-bousfield, jose-franco, mehdi-salehi
sme: rachel-bousfield, jose-franco, mehdi-salehi
sidebar_position: 1
target_audience: Developers using the Stylus Rust SDK to write and deploy smart contracts.
---

import StylusNoMultiInheritanceBannerPartial from '../partials/_stylus-no-multi-inheritance-banner-partial.mdx';

This document provides information about advanced features included in the [Stylus Rust SDK](https://github.com/OffchainLabs/stylus-sdk-rs), that are not described in the previous pages. For information about deploying Rust smart contracts, see the `cargo stylus` [CLI Tool](https://github.com/OffchainLabs/cargo-stylus). For a conceptual introduction to Stylus, see [Stylus: A Gentle Introduction](../gentle-introduction.mdx). To deploy your first Stylus smart contract using Rust, refer to the [Quickstart](../quickstart.mdx).

:::info

Many of the affordances use macros. Though this section details what each does, it may be helpful to use [`cargo expand`](https://crates.io/crates/cargo-expand) to see what they expand into if you’re doing advanced work in Rust.

:::

## Storage

This section provides extra information about how the Stylus Rust SDK handles storage. You can find more information and basic examples in [Variables](https://stylus-by-example.org/basic_examples/variables).

Rust smart contracts may use state that persists across transactions. There’s two primary ways to define storage, depending on if you want to use Rust or Solidity definitions. Both are equivalent, and are up to the developer depending on their needs.

### [`#[storage]`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/attr.storage.html)

The [`#[storage]`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/attr.storage.html) macro allows a Rust struct to be used in persistent storage.

```rust
#[storage]
pub struct Contract {
    owner: StorageAddress,
    active: StorageBool,
    sub_struct: SubStruct,
}

#[storage]
pub struct SubStruct {
    // types implementing the `StorageType` trait.
}
```

Any type implementing the [`StorageType`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.StorageType.html) trait may be used as a field, including other structs, which will implement the trait automatically when [`#[storage]`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/attr.storage.html) is applied. You can even implement [`StorageType`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.StorageType.html) yourself to define custom storage types. However, we’ve gone ahead and implemented the common ones.

| Type | Info |
| ---- | ---- |

| [`StorageBool`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageBool.html) | Stores a bool |
| [`StorageAddress`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageAddress.html) | Stores an Alloy [`Address`](https://docs.rs/alloy-primitives/latest/alloy_primitives/struct.Address.html) |
| [`StorageUint`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageUint.html) | Stores an Alloy [`Uint`](https://docs.rs/ruint/1.10.1/ruint/struct.Uint.html) |
| [`StorageSigned`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageSigned.html) | Stores an Alloy [`Signed`](https://docs.rs/alloy-primitives/latest/alloy_primitives/struct.Signed.html) |
| [`StorageFixedBytes`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageFixedBytes.html) | Stores an Alloy [`FixedBytes`](https://docs.rs/alloy-primitives/latest/alloy_primitives/struct.FixedBytes.html) |
| [`StorageBytes`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageBytes.html) | Stores a Solidity bytes |
| [`StorageString`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageString.html) | Stores a Solidity string |
| [`StorageVec`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageVec.html) | Stores a vector of [`StorageType`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.StorageType.html) |
| [`StorageMap`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageMap.html) | Stores a mapping of [`StorageKey`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.StorageKey.html) to [`StorageType`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.StorageType.html) |
| [`StorageArray`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageArray.html) | Stores a fixed-sized array of [`StorageType`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.StorageType.html) |

Every [Alloy primitive](https://docs.rs/alloy-primitives/latest/alloy_primitives/) has a corresponding [`StorageType`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.StorageType.html) implementation with the word `Storage` before it. This includes aliases, like [`StorageU256`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/type.StorageU256.html) and [`StorageB64`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/type.StorageB64.html).

### [`sol_storage!`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/macro.sol_storage.html)

The types in [`#[storage]`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/attr.storage.html) are laid out in the EVM state trie exactly as they are in [Solidity](https://docs.soliditylang.org/en/latest/internals/layout_in_storage.html). This means that the fields of a struct definition will map to the same storage slots as they would in EVM programming languages.

Because of this, it is often nice to define your types using Solidity syntax, which makes that guarantee easier to see. For example, the earlier Rust struct can re-written to:

```rust
sol_storage! {
    pub struct Contract {
        address owner;                      // becomes a StorageAddress
        bool active;                        // becomes a StorageBool
        SubStruct sub_struct,
    }

    pub struct SubStruct {
        // other solidity fields, such as
        mapping(address => uint) balances;  // becomes a StorageMap
        Delegate delegates[];               // becomes a StorageVec
    }
}
```

The above will expand to the equivalent definitions in Rust, each structure implementing the [`StorageType`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.StorageType.html) trait. Many contracts, like [our example `ERC-20`](https://github.com/OffchainLabs/stylus-sdk-rs/blob/stylus/examples/erc20/src/main.rs), do exactly this.

Because the layout is identical to [Solidity’s](https://docs.soliditylang.org/en/latest/internals/layout_in_storage.html), existing Solidity smart contracts can upgrade to Rust without fear of storage slots not lining up. You simply copy-paste your type definitions.

:::warning Storage layout in contracts using inheritance

One exception to this storage layout guarantee is contracts which utilize inheritance. The current solution in Stylus using `#[borrow]` and `#[inherits(...)]` packs nested (inherited) structs into their own slots. This is consistent with regular struct nesting in solidity, but not inherited structs. We plan to revisit this behavior in an upcoming release.

:::

:::tip
Existing Solidity smart contracts can upgrade to Rust if they use proxy patterns.
:::

Consequently, the order of fields will affect the JSON ABIs produced that explorers and tooling might use. Most developers won’t need to worry about this though and can freely order their types when working on a Rust contract from scratch.

### Reading and writing storage

You can access storage types via getters and setters. For example, the `Contract` struct from earlier might access its `owner` address as follows.

```rust
impl Contract {
    /// Gets the owner from storage.
    pub fn owner(&self) -> Address {
        self.owner.get()
    }

    /// Updates the owner in storage
    pub fn set_owner(&mut self, new_owner: Address) {
        if msg::sender() == self.owner.get() { // we'll discuss msg::sender later
            self.owner.set(new_owner);
        }
    }

    /// Unlike other storage type, stringStorage needs to
    /// use `.set_str()` and `.get_string()` to set and get.
    pub fn set_base_uri(&mut self, base_uri: String) {
        self.base_uri.set_str(base_uri);
    }

    pub fn get_base_uri(&self) -> String {
        self.base_uri.get_string()
    }
}
```

In Solidity, one has to be very careful about storage access patterns. Getting or setting the same value twice doubles costs, leading developers to avoid storage access at all costs. By contrast, the Stylus SDK employs an optimal storage-caching policy that avoids the underlying [`SLOAD`](https://www.evm.codes/#54) or [`SSTORE`](https://www.evm.codes/#55) operations.

:::tip

Stylus uses storage caching, so multiple accesses of the same variable is virtually free.

:::

However it must be said that storage is ultimately more expensive than memory. So if a value doesn’t need to be stored in state, you probably shouldn’t do it.

### Collections

Collections like [`StorageVec`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageVec.html) and [`StorageMap`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageMap.html) are dynamic and have methods like [`push`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageVec.html#method.push), [`insert`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageMap.html#method.insert), [`replace`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageMap.html#method.replace), and similar.

```rust
impl SubStruct {
   pub fn add_delegate(&mut self, delegate: Address) {
        self.delegates.push(delegate);
    }

    pub fn track_balance(&mut self, address: Address) {
        self.balances.insert(address, address.balance());
    }
}
```

You may notice that some methods return types like [`StorageGuard`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageGuard.html) and [`StorageGuardMut`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageGuardMut.html). This allows us to leverage the Rust borrow checker for storage mistakes, just like it does for memory. Here’s an example that will fail to compile.

```rust
fn mistake(vec: &mut StorageVec<StorageU64>) -> U64 {
    let value = vec.setter(0);
    let alias = vec.setter(0);
    value.set(32.into());
    alias.set(48.into());
    value.get() // uh, oh. what value should be returned?
}
```

Under the hood, [`vec.setter()`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageVec.html#method.setter) returns a [`StorageGuardMut`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageGuardMut.html) instead of a [`&mut StorageU64`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/type.StorageU64.html). Because the guard is bound to a [`&mut StorageVec`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageVec.html) lifetime, `value` and `alias` cannot be alive simultaneously. This causes the Rust compiler to reject the above code, saving you from entire classes of storage aliasing errors.

In this way the Stylus SDK safeguards storage access the same way Rust ensures memory safety. It should never be possible to alias Storage without `unsafe` Rust.

### [`SimpleStorageType`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.SimpleStorageType.html)

You may run into scenarios where a collection’s methods like [`push`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageVec.html#method.push) and [`insert`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageMap.html#method.insert) aren’t available. This is because only primitives, which implement a special trait called [`SimpleStorageType`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.SimpleStorageType.html), can be added to a collection by value. For nested collections, one instead uses the equivalent [`grow`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageVec.html#method.grow) and [`setter`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageVec.html#method.setter).

```rust
fn nested_vec(vec: &mut StorageVec<StorageVec<StorageU8>>) {
    let mut inner = vec.grow();  // adds a new element accessible via `inner`
    inner.push(0.into());        // inner is a guard to a StorageVec<StorageU8>
}

fn nested_map(map: &mut StorageMap<u32, StorageVec<U8>>) {
    let mut slot = map.setter(0);
    slot.push(0);
}
```

### [`Erase`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.Erase.html) and [`#[derive(Erase)]`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/derive.Erase.html)

Some [`StorageType`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.StorageType.html) values implement [`Erase`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.Erase.html), which provides an [`erase()`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.Erase.html#tymethod.erase) method for clearing state. We’ve implemented [`Erase`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.Erase.html) for all primitives, and for vectors of primitives, but not maps. This is because a solidity [`mapping`](https://docs.soliditylang.org/en/latest/types.html#mapping-types) does not provide iteration, and so it’s generally impossible to know which slots to set to zero.

Structs may also be [`Erase`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.Erase.html) if all of the fields are. [`#[derive(Erase)]`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/derive.Erase.html) lets you do this automatically.

```rust
sol_storage! {
    #[derive(Erase)]
    pub struct Contract {
        address owner;              // can erase primitive
        uint256[] hashes;           // can erase vector of primitive
    }

    pub struct NotErase {
        mapping(address => uint) balances; // can't erase a map
        mapping(uint => uint)[] roots;     // can't erase vector of maps
    }
}
```

You can also implement [`Erase`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.Erase.html) manually if desired. Note that the reason we care about [`Erase`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.Erase.html) at all is that you get storage refunds when clearing state, lowering fees. There’s also minor implications for patterns using `unsafe` Rust.

### The storage cache

The Stylus SDK employs an optimal storage-caching policy that avoids the underlying [`SLOAD`](https://www.evm.codes/#54) or [`SSTORE`](https://www.evm.codes/#55) operations needed to get and set state. For the vast majority of use cases, this happens in the background and requires no input from the user.

However, developers working with `unsafe` Rust implementing their own custom [`StorageType`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.StorageType.html) collections, the [`StorageCache`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageCache.html) type enables direct control over this data structure. Included are `unsafe` methods for manipulating the cache directly, as well as for bypassing it altogether.

### Immutables and [`PhantomData`](https://doc.rust-lang.org/core/marker/struct.PhantomData.html)

So that generics are possible in [`sol_interface!`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/macro.sol_interface.html), [`core::marker::PhantomData`](https://doc.rust-lang.org/core/marker/struct.PhantomData.html) implements [`StorageType`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.StorageType.html) and takes up zero space, ensuring that it won’t cause storage slots to change. This can be useful when writing libraries.

```rust
pub trait Erc20Params {
    const NAME: &'static str;
    const SYMBOL: &'static str;
    const DECIMALS: u8;
}

sol_storage! {
    pub struct Erc20<T> {
        mapping(address => uint256) balances;
        PhantomData<T> phantom;
    }
}
```

The above allows consumers of `Erc20` to choose immutable constants via specialization. See our [`WETH` sample contract](https://github.com/OffchainLabs/stylus-sdk-rs/blob/stylus/examples/erc20/src/main.rs) for a full example of this feature.

## Functions

This section provides extra information about how the Stylus Rust SDK handles functions. You can find more information and basic examples in [Functions](https://stylus-by-example.org/basic_examples/function), [Bytes in, bytes out programming](https://stylus-by-example.org/basic_examples/bytes_in_bytes_out), [Inheritance](https://stylus-by-example.org/basic_examples/inheritance) and [Sending ether](https://stylus-by-example.org/basic_examples/sending_ether).

### Pure, View, and Write functions

For non-payable methods the [`#[public]`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/attr.public.html) macro can figure state mutability out for you based on the types of the arguments. Functions with `&self` will be considered `view`, those with `&mut self` will be considered `write`, and those with neither will be considered `pure`. Please note that `pure` and `view` functions may change the state of other contracts by calling into them, or even this one if the `reentrant` feature is enabled.

### [`#[entrypoint]`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/attr.entrypoint.html)

This macro allows you to define the entrypoint, which is where Stylus execution begins. Without it, the contract will fail to pass `cargo stylus check`. Most commonly, the macro is used to annotate the top level storage struct.

```rust
sol_storage! {
    #[entrypoint]
    pub struct Contract {
        ...
    }

    // only one entrypoint is allowed
    pub struct SubStruct {
        ...
    }
}
```

The above will make the public methods of `Contract` the first to consider during invocation.

### Reentrancy

If a contract calls another that then calls the first, it is said to be reentrant. By default, all Stylus contracts revert when this happens. However, you can opt out of this behavior by enabling the `reentrant` feature flag.

```rust
stylus-sdk = { version = "0.6.0", features = ["reentrant"] }
```

This is dangerous, and should be done only after careful review––ideally by third-party auditors. Numerous exploits and hacks have in Web3 are attributable to developers misusing or not fully understanding reentrant patterns.

If enabled, the Stylus SDK will flush the storage cache in between reentrant calls, persisting values to state that might be used by inner calls. Note that preventing storage invalidation is only part of the battle in the fight against exploits. You can tell if a call is reentrant via `msg::reentrant`, and condition your business logic accordingly.

### [`TopLevelStorage`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.TopLevelStorage.html)

The [`#[entrypoint]`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/attr.entrypoint.html) macro will automatically implement the [`TopLevelStorage`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.TopLevelStorage.html) trait for the annotated `struct`. The single type implementing [`TopLevelStorage`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.TopLevelStorage.html) is special in that mutable access to it represents mutable access to the entire program’s state. This idea will become important when discussing calls to other programs in later sections.

### Inheritance, `#[inherit]`, and `#[borrow]`.

<StylusNoMultiInheritanceBannerPartial />

Composition in Rust follows that of Solidity. Types that implement [`Router`](https://docs.rs/stylus-sdk/latest/stylus_sdk/abi/trait.Router.html), the trait that [`#[public]`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/attr.public.html) provides, can be connected via inheritance.

```rust
#[public]
#[inherit(Erc20)]
impl Token {
    pub fn mint(&mut self, amount: U256) -> Result<(), Vec<u8>> {
        ...
    }
}

#[public]
impl Erc20 {
    pub fn balance_of() -> Result<U256> {
        ...
    }
}
```

Because `Token` inherits `Erc20` in the above, if `Token` has the [`#[entrypoint]`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/attr.entrypoint.html), calls to the contract will first check if the requested method exists within `Token`. If a matching function is not found, it will then try the `Erc20`. Only after trying everything `Token` inherits will the call revert.

Note that because methods are checked in that order, if both implement the same method, the one in `Token` will override the one in `Erc20`, which won’t be callable. This allows for patterns where the developer imports a crate implementing a standard, like the `ERC-20`, and then adds or overrides just the methods they want to without modifying the imported `Erc20` type.

::::warning

Stylus does not currently contain explicit `override` or `virtual` keywords for explicitly marking override functions. It is important, therefore, to carefully ensure that contracts are only overriding the functions.

::::

Inheritance can also be chained. `#[inherit(Erc20, Erc721)]` will inherit both `Erc20` and `Erc721`, checking for methods in that order. `Erc20` and `Erc721` may also inherit other types themselves. Method resolution finds the first matching method by [Depth First Search](https://en.wikipedia.org/wiki/Depth-first_search).

For the above to work, `Token` must implement [`Borrow<Erc20>`](https://doc.rust-lang.org/core/borrow/trait.Borrow.html). You can implement this yourself, but for simplicity, [`#[storage]`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/attr.storage.html) and [`sol_storage!`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/macro.sol_storage.html) provide a `#[borrow]` annotation.

```rust
sol_storage! {
    #[entrypoint]
    pub struct Token {
        #[borrow]
        Erc20 erc20;
        ...
    }

    pub struct Erc20 {
        ...
    }
}
```

## Calls

Just as with storage and functions, Stylus SDK calls are Solidity ABI equivalent. This means you never have to know the implementation details of other contracts to invoke them. You simply import the Solidity interface of the target contract, which can be auto-generated via the `cargo stylus` [CLI tool](https://github.com/OffchainLabs/cargo-stylus#exporting-solidity-abis).

:::tip

You can call contracts in any programming language with the Stylus SDK.

:::

### [`sol_interface!`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/macro.sol_interface.html)

This macro defines a `struct` for each of the Solidity interfaces provided.

```rust
sol_interface! {
    interface IService {
        function makePayment(address user) payable returns (string);
        function getConstant() pure returns (bytes32)
    }

    interface ITree {
        // other interface methods
    }
}
```

The above will define `IService` and `ITree` for calling the methods of the two contracts.

::::info

Currently only functions are supported, and any other items in the interface will cause an error.

::::

For example, `IService` will have a `make_payment` method that accepts an [`Address`](https://docs.rs/alloy-primitives/latest/alloy_primitives/struct.Address.html) and returns a [`B256`](https://docs.rs/alloy-primitives/latest/alloy_primitives/aliases/type.B256.html).

```rust
pub fn do_call(&mut self, account: IService, user: Address) -> Result<String, Error> {
    account.make_payment(self, user)  // note the snake case
}
```

Observe the casing change. [`sol_interface!`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/macro.sol_interface.html) computes the selector based on the exact name passed in, which should almost always be `CamelCase`. For aesthetics, the rust functions will instead use `snake_case`.

### Configuring gas and value with [`Call`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/struct.Call.html)

[`Call`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/struct.Call.html) lets you configure a call via optional configuration methods. This is similar to how one would configure opening a [`File`](https://doc.rust-lang.org/std/fs/struct.OpenOptions.html#examples) in Rust.

```rust
pub fn do_call(account: IService, user: Address) -> Result<String, Error> {
    let config = Call::new_in()
        .gas(evm::gas_left() / 2)       // limit to half the gas left
        .value(msg::value());           // set the callvalue

    account.make_payment(config, user)
}
```

By default [`Call`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/struct.Call.html) supplies all gas remaining and zero value, which often means [`Call::new_in()`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/struct.Call.html#method.new_in) may be passed to the method directly. Additional configuration options are available in cases of reentrancy.

### Reentrant calls

Contracts that opt into reentrancy via the `reentrant` feature flag require extra care. When the `storage-cache` feature is enabled, cross-contract calls must [`flush`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageCache.html#method.flush) or [`clear`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageCache.html#method.clear) the [`StorageCache`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageCache.html) to safeguard state. This happens automatically via the type system.

```rust
sol_interface! {
    interface IMethods {
        function pureFoo() external pure;
        function viewFoo() external view;
        function writeFoo() external;
        function payableFoo() external payable;
    }
}

#[public]
impl Contract {
    pub fn call_pure(&self, methods: IMethods) -> Result<(), Vec<u8>> {
        Ok(methods.pure_foo(self)?)    // `pure` methods might lie about not being `view`
    }

    pub fn call_view(&self, methods: IMethods) -> Result<(), Vec<u8>> {
        Ok(methods.view_foo(self)?)
    }

    pub fn call_write(&mut self, methods: IMethods) -> Result<(), Vec<u8>> {
        methods.view_foo(self)?;       // allows `pure` and `view` methods too
        Ok(methods.write_foo(self)?)
    }

    #[payable]
    pub fn call_payable(&mut self, methods: IMethods) -> Result<(), Vec<u8>> {
        methods.write_foo(Call::new_in(self))?;   // these are the same
        Ok(methods.payable_foo(self)?)            // ------------------
    }
}
```

In the above, we’re able to pass `&self` and `&mut self` because `Contract` implements [`TopLevelStorage`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.TopLevelStorage.html), which means that a reference to it entails access to the entirety of the contract’s state. This is the reason it is sound to make a call, since it ensures all cached values are invalidated and/or persisted to state at the right time.

When writing Stylus libraries, a type might not be [`TopLevelStorage`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/trait.TopLevelStorage.html) and therefore `&self` or `&mut self` won’t work. Building a [`Call`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/struct.Call.html) from a generic parameter via [`new_in`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/struct.Call.html#method.new_in) is the usual solution.

```rust
pub fn do_call(
    storage: &mut impl TopLevelStorage,  // can be generic, but often just &mut self
    account: IService,                   // serializes as an Address
    user: Address,
) -> Result<String, Error> {

    let config = Call::new_in(storage)   // take exclusive access to all contract storage
        .gas(evm::gas_left() / 2)        // limit to half the gas left
        .value(msg::value());            // set the callvalue

    account.make_payment(config, user)   // note the snake case
}
```

In the context of a [`#[public]`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/attr.public.html) call, the `&mut impl` argument will correctly distinguish the method as being `write` or [`payable`](https://docs.alchemy.com/docs/solidity-payable-functions). This means you can write library code that will work regardless of whether the reentrant feature flag is enabled.

Also, that code that previously compiled with reentrancy disabled may require modification in order to type-check. This is done to ensure storage changes are persisted and that the storage cache is properly managed before calls.

### [`call`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/fn.call.html), [`static_call`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/fn.static_call.html), and [`delegate_call`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/fn.delegate_call.html)

Though [`sol_interface!`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/macro.sol_interface.html) and [`Call`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/struct.Call.html) form the most common idiom to invoke other contracts, their underlying [`call`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/fn.call.html) and [`static_call`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/fn.static_call.html) are exposed for direct access.

```rust
let return_data = call(Call::new_in(self), contract, call_data)?;
```

In each case the calldata is supplied as a [`Vec<u8>`](https://doc.rust-lang.org/alloc/vec/struct.Vec.html). The return result is either the raw return data on success, or a call [`Error`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/enum.Error.html) on failure.

[`delegate_call`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/fn.delegate_call.html) is also available, though it's `unsafe` and doesn't have a richly-typed equivalent. This is because a delegate call must trust the other contract to uphold safety requirements. Though this function clears any cached values, the other contract may arbitrarily change storage, spend ether, and do other things one should never blindly allow other contracts to do.

### [`transfer_eth`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/fn.transfer_eth.html)

This method provides a convenient shorthand for transferring ether.

:::note
This method invokes the other contract, which may in turn call others. All gas is supplied, which the recipient may burn. If this is not desired, the [`call`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/fn.call.html) function may be used instead.
:::

```rust
transfer_eth(recipient, value)?;                 // these two are equivalent

call(Call::new_in().value(value), recipient, &[])?; // these two are equivalent
```

### [`RawCall`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/struct.RawCall.html) and `unsafe` calls

Occasionally, an untyped call to another contract is necessary. [`RawCall`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/struct.RawCall.html) lets you configure an `unsafe` call by calling optional configuration methods. This is similar to how one would configure opening a [`File`](https://doc.rust-lang.org/std/fs/struct.OpenOptions.html#examples) in Rust.

```rust
let data = RawCall::new_delegate()   // configure a delegate call
    .gas(2100)                       // supply 2100 gas
    .limit_return_data(0, 32)        // only read the first 32 bytes back
    .flush_storage_cache()           // flush the storage cache before the call
    .call(contract, calldata)?;      // do the call
```

:::note
The [`call`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/struct.RawCall.html#method.call) method is `unsafe` when reentrancy is enabled. See [`flush_storage_cache`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/struct.RawCall.html#method.flush_storage_cache) and [`clear_storage_cache`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/struct.RawCall.html#method.clear_storage_cache) for more information.
:::

## [`RawDeploy`](https://docs.rs/stylus-sdk/latest/stylus_sdk/deploy/struct.RawDeploy.html) and `unsafe` deployments

Right now the only way to deploy a contract from inside Rust is to use [`RawDeploy`](https://docs.rs/stylus-sdk/latest/stylus_sdk/deploy/struct.RawDeploy.html), similar to [`RawCall`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/struct.RawCall.html). As with [`RawCall`](https://docs.rs/stylus-sdk/latest/stylus_sdk/call/struct.RawCall.html), this mechanism is inherently unsafe due to reentrancy concerns, and requires manual management of the [`StorageCache`](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/struct.StorageCache.html).

:::note
That the EVM allows init code to make calls to other contracts, which provides a vector for reentrancy. This means that this technique may enable storage aliasing if used in the middle of a storage reference's lifetime and if reentrancy is allowed.
:::

When configured with a `salt`, [`RawDeploy`](https://docs.rs/stylus-sdk/latest/stylus_sdk/deploy/struct.RawDeploy.html) will use [`CREATE2`](https://www.evm.codes/#f5) instead of the default [`CREATE`](https://www.evm.codes/#f0), facilitating address determinism.

---

## .mdx (reference/testnet-information.md)
---
title: 'Stylus testnet information'
description: A reference providing details about the Stylus testnet and faucets for obtaining testnet ETH
author: amarrazza
sme: amarrazza
target_audience: Developers building on the Stylus testnet
sidebar_position: 9
---

import StylusFaucets from './partials/_stylus-faucets.mdx';

import ArbitrumContractAddresses from '../../partials/_reference-arbitrum-contract-addresses-partial.mdx';

## Arbitrum public RPC endpoints

:::caution

- Unlike the RPC Urls, the Sequencer endpoints only support `eth_sendRawTransaction` and `eth_sendRawTransactionConditional` calls.
- Arbitrum public RPCs do not provide Websocket support.
- Stylus testnets v1 and v2 have been spun down and are not accessible anymore.
- Visit [Quicknode's Arbitrum Sepolia faucet](https://faucet.quicknode.com/arbitrum/sepolia), [Alchemy's Arbitrum sepolia faucet](https://www.alchemy.com/faucets/arbitrum-sepolia), or [Getblock's Arbitrum Sepolia faucet](https://getblock.io/faucet/arb-sepolia) for testnet Sepolia tokens on L2.

:::

This section provides an overview of the available public RPC endpoints for different Arbitrum chains that have Stylus enabled, and the necessary details to interact with them.

| Name                       | RPC Url(s)                             | Chain ID | Block explorer              | Underlying chain | Tech stack     | Sequencer feed URL                    | Sequencer endpoint<sup>⚠️</sup>                  |
| -------------------------- | -------------------------------------- | -------- | --------------------------- | ---------------- | -------------- | ------------------------------------- | ------------------------------------------------ |
| Arbitrum Sepolia (Testnet) | https://sepolia-rollup.arbitrum.io/rpc | 421614   | https://sepolia.arbiscan.io | Sepolia          | Nitro (Rollup) | wss://sepolia-rollup.arbitrum.io/feed | https://sepolia-rollup-sequencer.arbitrum.io/rpc |

<StylusFaucets />

<ArbitrumContractAddresses />

---

## .mdx (stylus-content-map.mdx)
---
id: stylus-content-map
title: Write Stylus Contracts
sidebar_label: Write Stylus contracts
---

import Card from '@site/src/components/Cards/Card';

# Write Stylus Contracts

Let's learn how to write contracts with Stylus!

<div
  style={{
    display: 'grid',
    gridTemplateColumns: 'repeat(auto-fit, minmax(300px, 1fr))',
    gap: '20px',
  }}
>
  <Card
    title="A gentle introduction"
    description="Start with the basics of Stylus contracts"
    href="/stylus/stylus-gentle-introduction"
    target="_blank"
  />
  <Card
    title="Quickstart (Rust)"
    description="Get started quickly with Rust"
    href="/stylus/stylus-quickstart"
    target="_blank"
  />
  <Card
    title="Testnet"
    description="Explore the testnet environment"
    href="/stylus/reference/testnet-information"
    target="_blank"
  />
  <Card
    title="Stylus by example"
    description="Learn Stylus through examples"
    href="https://stylus-by-example.org"
    target="_blank"
  />
  <Card
    title="Stylus Rust SDK"
    description="Dive into the Stylus Rust SDK"
    href="/stylus/reference/rust-sdk-guide"
    target="_blank"
  />
  <Card
    title="Gas, ink and caching"
    description="Learn about gas, ink, and caching strategies"
    href="/stylus/concepts/stylus-gas"
    target="_blank"
  />
  <Card
    title="CLI tools (cargo-stylus)"
    description="Master the CLI tools for Stylus"
    href="/stylus/cli-tools-overview"
    target="_blank"
  />
  <Card
    title="Run a Stylus dev node"
    description="Set up and run a development node"
    href="/run-arbitrum-node/run-local-dev-node"
    target="_blank"
  />
  <Card
    title="Other supported languages"
    description="Explore other languages supported by Stylus"
    href="/stylus/reference/stylus-sdk"
    target="_blank"
  />
  <Card
    title="Troubleshooting"
    description="Find solutions to common issues"
    href="/stylus/troubleshooting-building-stylus"
    target="_blank"
  />
  <Card
    title="Source code repository"
    description="Check out the source code"
    href="https://github.com/OffchainLabs/stylus"
    target="_blank"
  />
  <Card
    title="Public preview"
    description="View the public preview of Stylus"
    href="/stylus/concepts/public-preview-expectations"
    target="_blank"
  />
</div>

---

## .mdx (troubleshooting-building-stylus.md)
---
title: 'Troubleshooting Stylus'
description: List of questions and answers frequently asked by developers building with Stylus
user_story: As a developer, I want to understand how to troubleshoot common issues when building with Stylus.
content_type: faq
---

import FAQStructuredDataJsonLd from '@site/src/components/FAQStructuredData';
import FAQQuestions from '../partials/_troubleshooting-stylus-partial.mdx';

<FAQStructuredDataJsonLd faqsId="building-stylus" />
<FAQQuestions />

---

## .mdx (using-cli.mdx)
---
id: using-cli
title: 'Using Stylus CLI'
description: 'Get started with Stylus CLI, a Rust toolkit for developing Stylus contracts'
author: 'anegg0'
sme: 'anegg0'
sidebar_position: 2
target_audience: Developers writing Stylus contracts in Rust using Stylus
---

This guide will get you started using [cargo stylus](https://github.com/OffchainLabs/cargo-stylus), a CLI toolkit to help developers manage, compile, deploy, and optimize their Stylus contracts efficiently.

This overview will help you discover and learn how to uses cargo stylus tools.

### Installing cargo stylus

Cargo stylus is a plugin to the standard cargo tool for developing Rust programs.

#### Prerequisites

<details>
<summary>Rust toolchain</summary>

Follow the instructions on [Rust Lang's installation page](https://www.rust-lang.org/tools/install) to install a complete Rust toolchain (v1.81 or newer) on your system. After installation, ensure you can access the programs `rustup`, `rustc`, and `cargo` from your preferred terminal application.

</details>

<details>
<summary>Docker</summary>

We will use the testnet, and some `cargo stylus` commands will require Docker to operate.

You can download Docker from [Docker's website](https://www.docker.com/products/docker-desktop).

</details>

<details>
<summary>Foundry's Cast</summary>

[Foundry's Cast](https://book.getfoundry.sh/cast/) is a command-line tool for interacting with your EVM contracts.

</details>

<details>
<summary>Nitro devnode</summary>

Stylus is available on Arbitrum Sepolia, but we'll use Nitro devnode, which has a pre-funded wallet, saving us the effort of wallet provisioning or running out of tokens to send transactions.

```shell title="Install your devnode"
git clone https://github.com/OffchainLabs/nitro-devnode.git
cd nitro-devnode
```

```shell title="Launch your devnode"
./run-dev-node.sh
```

</details>

#### Installation

In your terminal, run:

```shell
cargo install --force cargo-stylus
```

Add WASM ([WebAssembly](https://webassembly.org/)) as a build target for the specific Rust toolchain you are using. The below example sets your default Rust toolchain to 1.80 as well as adding the WASM build target:

```shell
rustup default 1.80
rustup target add wasm32-unknown-unknown --toolchain 1.80
```

You can verify the cargo stylus installation by running `cargo stylus -V` in your terminal, returning something like:`stylus 0.5.6`

### Using cargo stylus

#### Cargo Stylus Commands Reference

| Command      | Description                                      | Arguments                                                                                                             | Options                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Example Usage                                                                                         |
| ------------ | ------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------- |
| `new`        | Create a new Stylus project                      | • `name`: Project name (required)                                                                                     | • `--minimal`: Create a minimal contract                                                                                                                                                                                                                                                                                                                                                                                                                                                         | `cargo stylus new <YOUR_PROJECT_NAME>`                                                                |
| `init`       | Initialize a Stylus project in current directory |                                                                                                                       | • `--minimal`: Create a minimal contract                                                                                                                                                                                                                                                                                                                                                                                                                                                         | `cargo stylus init --minimal`                                                                         |
| `export-abi` | Export a Solidity ABI                            |                                                                                                                       | • `--output`: Output file (defaults to stdout)<br />• `--json`: Write JSON ABI using `solc`                                                                                                                                                                                                                                                                                                                                                                                                      | `cargo stylus export-abi --json`                                                                      |
| `activate`   | Activate an already deployed contract            | • `--address`: Contract address to activate                                                                           | • `--data-fee-bump-percent`: Percent to bump estimated fee (default 20%)<br />• `--estimate-gas`: Only estimate gas without sending transaction                                                                                                                                                                                                                                                                                                                                                  | `cargo stylus activate --address <CONTRACT_ADDRESS>`                                                  |
| `cache`      | Cache contract using Stylus CacheManager         | • `bid`: Place bid on contract<br />• `status`: Check contract status<br />• `suggest-bid`: Get suggested minimum bid |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | `cargo stylus cache bid --address <CONTRACT_ADDRESS>`                                                 |
| `check`      | Check a contract                                 |                                                                                                                       | • `--wasm-file`: WASM file to check<br />• `--contract-address`: Deployment address                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                       |
| `deploy`     | Deploy a contract                                | • `--contract-address <CONTRACT_ADDRESS>`: Where to deploy and activate the contract (defaults to a random address)   | • `--estimate-gas`: Only perform estimation<br />• `--no-verify`: Skip reproducible container<br />• `--cargo-stylus-version`: Version for Docker image<br />• `--source-files-for-project-hash <SOURCE_FILES_FOR_PROJECT_HASH>`: Path to source files to include in the project hash<br />• `--max-fee-per-gas-gwei <MAX_FEE_PER_GAS_GWEI>`: Optional max fee per gas in `gwei` units<br />• `--wasm-file <WASM_FILE>`: The WASM file to check (defaults to any found in the current directory) | `cargo stylus deploy --endpoint='http://localhost:8547' --private-key="<PRIVATE_KEY>" --estimate-gas` |
| `verify`     | Verify contract deployment                       | • `--deployment-tx`: Hash of deployment transaction                                                                   | • `--no-verify`: Skip reproducible container<br />• `--cargo-stylus-version`: Version for Docker image                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                       |
| `cgen`       | Generate C code bindings                         | • `--input`: Input file path<br />• `--out_dir`: Output directory path                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                       |
| `replay`     | Replay transaction in GDB                        | • `-t, --tx <TX>`: Transaction to replay                                                                              | • `-p, --project <PROJECT>`: Project path (default: `.`)<br />• `-u, --use-native-tracer`: Use the native tracer instead of the JavaScript one (may not be available in the node)<br />• `-s, --stable-rust`: Use stable Rust (note that nightly is needed to expand macros)                                                                                                                                                                                                                     | `cargo stylus replay --tx <TX>`                                                                       |
| `trace`      | Trace a transaction                              | • `--tx`: Transaction hash                                                                                            | • `--endpoint`: RPC endpoint<br />• `--project`: Project path<br />• `--use-native-tracer`: Use native tracer                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                       |

##### Common options

These options are available across multiple commands:

| Option                          | Description                                            |
| ------------------------------- | ------------------------------------------------------ |
| --endpoint                      | Arbitrum RPC endpoint (default: http://localhost:8547) |
| --verbose                       | Print debug info                                       |
| --source-files-for-project-hash | Paths to source files for project hash                 |
| --max-fee-per-gas-gwei          | Optional max fee per gas in `gwei`                     |

##### Authentication options

Available for commands involving transactions:

| Option                   | Description                                          |
| ------------------------ | ---------------------------------------------------- |
| --private-key-path       | Path to file containing hex-encoded private key      |
| --private-key            | Private key as hex string (exposes to shell history) |
| --keystore-path          | Path to Ethereum wallet keystore file                |
| --keystore-password-path | Keystore password file path                          |

#### How-tos

| Topic                                                                              | Description                                                                                                                                 |
| ---------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
| [Learn how to optimize WASM binaries](/stylus/how-tos/optimizing-binaries.mdx)     | The `cargo-stylus` tool allows you to optimize WebAssembly (WASM) binaries, ensuring that your contracts are as efficient as possible.      |
| [Debug Stylus transactions](/stylus/how-tos/debugging-tx.mdx)                      | A guide to debugging transactions, helping you identify and fix issues. Gain insights into your Stylus contracts by debugging transactions. |
| [Verify contracts](/stylus/how-tos/verifying-contracts.mdx)                        | Ensure that your Stylus contracts are correctly verified. Step-by-step instructions on how to verify your contracts using `cargo-stylus`.   |
| [Run a Stylus dev node](/run-arbitrum-node/03-run-local-full-chain-simulation.mdx) | Learn how to run a local Arbitrum dev node to test your Stylus contracts.                                                                   |

#### Additional resources

#### [Troubleshooting](/stylus/troubleshooting-building-stylus.md): solve the most common issues.

#### [cargo-stylus repository](https://github.com/OffchainLabs/stylus): consult cargo stylus' source code.

---

# stylus-by-example Folder

## .mdx (applications/erc20.mdx)
---
title: 'ERC-20 • Stylus by Example'
description: 'An example implementation of the ERC-20 token standard in Rust using Arbitrum Stylus.'
---

{/* Begin Content */}

# ERC-20

Any contract that follows the [ERC-20 standard](https://eips.ethereum.org/EIPS/eip-20) is an ERC-20 token.

ERC-20 tokens provide functionalities to

- transfer tokens
- allow others to transfer tokens on behalf of the token holder

Here is the interface for ERC-20.

```solidity
interface IERC20 {
    function totalSupply() external view returns (uint256);
    function balanceOf(address account) external view returns (uint256);
    function transfer(address recipient, uint256 amount)
        external
        returns (bool);
    function allowance(address owner, address spender)
        external
        view
        returns (uint256);
    function approve(address spender, uint256 amount) external returns (bool);
    function transferFrom(address sender, address recipient, uint256 amount)
        external
        returns (bool);
}
```

Example implementation of an ERC-20 token contract written in Rust.

### src/erc20.rs

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
//! Implementation of the ERC-20 standard
//!
//! The eponymous [`Erc20`] type provides all the standard methods,
//! and is intended to be inherited by other contract types.
//!
//! You can configure the behavior of [`Erc20`] via the [`Erc20Params`] trait,
//! which allows specifying the name, symbol, and decimals of the token.
//!
//! Note that this code is unaudited and not fit for production use.

// Imported packages
use alloc::string::String;
use alloy_primitives::{Address, U256};
use alloy_sol_types::sol;
use core::marker::PhantomData;
use stylus_sdk::{
    evm,
    msg,
    prelude::*,
};

pub trait Erc20Params {
    /// Immutable token name
    const NAME: &'static str;

    /// Immutable token symbol
    const SYMBOL: &'static str;

    /// Immutable token decimals
    const DECIMALS: u8;
}

sol_storage! {
    /// Erc20 implements all ERC-20 methods.
    pub struct Erc20<T> {
        /// Maps users to balances
        mapping(address => uint256) balances;
        /// Maps users to a mapping of each spender's allowance
        mapping(address => mapping(address => uint256)) allowances;
        /// The total supply of the token
        uint256 total_supply;
        /// Used to allow [`Erc20Params`]
        PhantomData<T> phantom;
    }
}

// Declare events and Solidity error types
sol! {
    event Transfer(address indexed from, address indexed to, uint256 value);
    event Approval(address indexed owner, address indexed spender, uint256 value);

    error InsufficientBalance(address from, uint256 have, uint256 want);
    error InsufficientAllowance(address owner, address spender, uint256 have, uint256 want);
}

/// Represents the ways methods may fail.
#[derive(SolidityError)]
pub enum Erc20Error {
    InsufficientBalance(InsufficientBalance),
    InsufficientAllowance(InsufficientAllowance),
}

// These methods aren't exposed to other contracts
// Methods marked as "pub" here are usable outside of the erc20 module (i.e. they're callable from lib.rs)
// Note: modifying storage will become much prettier soon
impl<T: Erc20Params> Erc20<T> {
    /// Movement of funds between 2 accounts
    /// (invoked by the external transfer() and transfer_from() functions )
    pub fn _transfer(
        &mut self,
        from: Address,
        to: Address,
        value: U256,
    ) -> Result<(), Erc20Error> {
        // Decreasing sender balance
        let mut sender_balance = self.balances.setter(from);
        let old_sender_balance = sender_balance.get();
        if old_sender_balance < value {
            return Err(Erc20Error::InsufficientBalance(InsufficientBalance {
                from,
                have: old_sender_balance,
                want: value,
            }));
        }
        sender_balance.set(old_sender_balance - value);

        // Increasing receiver balance
        let mut to_balance = self.balances.setter(to);
        let new_to_balance = to_balance.get() + value;
        to_balance.set(new_to_balance);

        // Emitting the transfer event
        evm::log(Transfer { from, to, value });
        Ok(())
    }

    /// Mints `value` tokens to `address`
    pub fn mint(&mut self, address: Address, value: U256) -> Result<(), Erc20Error> {
        // Increasing balance
        let mut balance = self.balances.setter(address);
        let new_balance = balance.get() + value;
        balance.set(new_balance);

        // Increasing total supply
        self.total_supply.set(self.total_supply.get() + value);

        // Emitting the transfer event
        evm::log(Transfer {
            from: Address::ZERO,
            to: address,
            value,
        });

        Ok(())
    }

    /// Burns `value` tokens from `address`
    pub fn burn(&mut self, address: Address, value: U256) -> Result<(), Erc20Error> {
        // Decreasing balance
        let mut balance = self.balances.setter(address);
        let old_balance = balance.get();
        if old_balance < value {
            return Err(Erc20Error::InsufficientBalance(InsufficientBalance {
                from: address,
                have: old_balance,
                want: value,
            }));
        }
        balance.set(old_balance - value);

        // Decreasing the total supply
        self.total_supply.set(self.total_supply.get() - value);

        // Emitting the transfer event
        evm::log(Transfer {
            from: address,
            to: Address::ZERO,
            value,
        });

        Ok(())
    }
}

// These methods are external to other contracts
// Note: modifying storage will become much prettier soon
#[public]
impl<T: Erc20Params> Erc20<T> {
    /// Immutable token name
    pub fn name() -> String {
        T::NAME.into()
    }

    /// Immutable token symbol
    pub fn symbol() -> String {
        T::SYMBOL.into()
    }

    /// Immutable token decimals
    pub fn decimals() -> u8 {
        T::DECIMALS
    }

    /// Total supply of tokens
    pub fn total_supply(&self) -> U256 {
        self.total_supply.get()
    }

    /// Balance of `address`
    pub fn balance_of(&self, owner: Address) -> U256 {
        self.balances.get(owner)
    }

    /// Transfers `value` tokens from msg::sender() to `to`
    pub fn transfer(&mut self, to: Address, value: U256) -> Result<bool, Erc20Error> {
        self._transfer(msg::sender(), to, value)?;
        Ok(true)
    }

    /// Transfers `value` tokens from `from` to `to`
    /// (msg::sender() must be able to spend at least `value` tokens from `from`)
    pub fn transfer_from(
        &mut self,
        from: Address,
        to: Address,
        value: U256,
    ) -> Result<bool, Erc20Error> {
        // Check msg::sender() allowance
        let mut sender_allowances = self.allowances.setter(from);
        let mut allowance = sender_allowances.setter(msg::sender());
        let old_allowance = allowance.get();
        if old_allowance < value {
            return Err(Erc20Error::InsufficientAllowance(InsufficientAllowance {
                owner: from,
                spender: msg::sender(),
                have: old_allowance,
                want: value,
            }));
        }

        // Decreases allowance
        allowance.set(old_allowance - value);

        // Calls the internal transfer function
        self._transfer(from, to, value)?;

        Ok(true)
    }

    /// Approves the spenditure of `value` tokens of msg::sender() to `spender`
    pub fn approve(&mut self, spender: Address, value: U256) -> bool {
        self.allowances.setter(msg::sender()).insert(spender, value);
        evm::log(Approval {
            owner: msg::sender(),
            spender,
            value,
        });
        true
    }

    /// Returns the allowance of `spender` on `owner`'s tokens
    pub fn allowance(&self, owner: Address, spender: Address) -> U256 {
        self.allowances.getter(owner).get(spender)
    }
}
```

### lib.rs

```rust
// Only run this as a WASM if the export-abi feature is not set.
#![cfg_attr(not(any(feature = "export-abi", test)), no_main)]
extern crate alloc;

// Modules and imports
mod erc20;

use alloy_primitives::{Address, U256};
use stylus_sdk::{
    msg,
    prelude::*
};
use crate::erc20::{Erc20, Erc20Params, Erc20Error};

/// Immutable definitions
struct StylusTokenParams;
impl Erc20Params for StylusTokenParams {
    const NAME: &'static str = "StylusToken";
    const SYMBOL: &'static str = "STK";
    const DECIMALS: u8 = 18;
}

// Define the entrypoint as a Solidity storage object. The sol_storage! macro
// will generate Rust-equivalent structs with all fields mapped to Solidity-equivalent
// storage slots and types.
sol_storage! {
    #[entrypoint]
    struct StylusToken {
        // Allows erc20 to access StylusToken's storage and make calls
        #[borrow]
        Erc20<StylusTokenParams> erc20;
    }
}

#[public]
#[inherit(Erc20<StylusTokenParams>)]
impl StylusToken {
    /// Mints tokens
    pub fn mint(&mut self, value: U256) -> Result<(), Erc20Error> {
        self.erc20.mint(msg::sender(), value)?;
        Ok(())
    }

    /// Mints tokens to another address
    pub fn mint_to(&mut self, to: Address, value: U256) -> Result<(), Erc20Error> {
        self.erc20.mint(to, value)?;
        Ok(())
    }

    /// Burns tokens
    pub fn burn(&mut self, value: U256) -> Result<(), Erc20Error> {
        self.erc20.burn(msg::sender(), value)?;
        Ok(())
    }
}

```

### Cargo.toml

```toml
[package]
name = "stylus_erc20_example"
version = "0.1.7"
edition = "2021"
license = "MIT OR Apache-2.0"
keywords = ["arbitrum", "ethereum", "stylus", "alloy"]

[dependencies]
alloy-primitives = "=0.7.6"
alloy-sol-types = "=0.7.6"
mini-alloc = "0.4.2"
stylus-sdk = "0.6.0"
hex = "0.4.3"

[dev-dependencies]
tokio = { version = "1.12.0", features = ["full"] }
ethers = "2.0"
eyre = "0.6.8"

[features]
export-abi = ["stylus-sdk/export-abi"]

[lib]
crate-type = ["lib", "cdylib"]

[profile.release]
codegen-units = 1
strip = true
lto = true
panic = "abort"
opt-level = "s"

```

---

## .mdx (applications/erc721.mdx)
---
title: 'ERC-721 • Stylus by Example'
description: 'An example implementation of the ERC-721 token standard in Rust using Arbitrum Stylus.'
---

{/* Begin Content */}

# ERC-721

Any contract that follows the [ERC-721 standard](https://eips.ethereum.org/EIPS/eip-721) is an ERC-721 token.

Here is the interface for ERC-721.

```solidity
interface ERC721 {
    event Transfer(address indexed _from, address indexed _to, uint256 indexed _tokenId);
    event Approval(address indexed _owner, address indexed _approved, uint256 indexed _tokenId);
    event ApprovalForAll(address indexed _owner, address indexed _operator, bool _approved);

    function balanceOf(address _owner) external view returns (uint256);
    function ownerOf(uint256 _tokenId) external view returns (address);
    function safeTransferFrom(address _from, address _to, uint256 _tokenId, bytes data) external payable;
    function safeTransferFrom(address _from, address _to, uint256 _tokenId) external payable;
    function transferFrom(address _from, address _to, uint256 _tokenId) external payable;
    function approve(address _approved, uint256 _tokenId) external payable;
    function setApprovalForAll(address _operator, bool _approved) external;
    function getApproved(uint256 _tokenId) external view returns (address);
    function isApprovedForAll(address _owner, address _operator) external view returns (bool);
}
```

Example implementation of an ERC-721 token contract written in Rust.

### src/erc721.rs

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
//! Implementation of the ERC-721 standard
//!
//! The eponymous [`Erc721`] type provides all the standard methods,
//! and is intended to be inherited by other contract types.
//!
//! You can configure the behavior of [`Erc721`] via the [`Erc721Params`] trait,
//! which allows specifying the name, symbol, and token uri.
//!
//! Note that this code is unaudited and not fit for production use.

use alloc::{string::String, vec, vec::Vec};
use alloy_primitives::{Address, U256, FixedBytes};
use alloy_sol_types::sol;
use core::{borrow::BorrowMut, marker::PhantomData};
use stylus_sdk::{
    abi::Bytes,
    evm,
    msg,
    prelude::*
};

pub trait Erc721Params {
    /// Immutable NFT name.
    const NAME: &'static str;

    /// Immutable NFT symbol.
    const SYMBOL: &'static str;

    /// The NFT's Uniform Resource Identifier.
    fn token_uri(token_id: U256) -> String;
}

sol_storage! {
    /// Erc721 implements all ERC-721 methods
    pub struct Erc721<T: Erc721Params> {
        /// Token id to owner map
        mapping(uint256 => address) owners;
        /// User to balance map
        mapping(address => uint256) balances;
        /// Token id to approved user map
        mapping(uint256 => address) token_approvals;
        /// User to operator map (the operator can manage all NFTs of the owner)
        mapping(address => mapping(address => bool)) operator_approvals;
        /// Total supply
        uint256 total_supply;
        /// Used to allow [`Erc721Params`]
        PhantomData<T> phantom;
    }
}

// Declare events and Solidity error types
sol! {
    event Transfer(address indexed from, address indexed to, uint256 indexed token_id);
    event Approval(address indexed owner, address indexed approved, uint256 indexed token_id);
    event ApprovalForAll(address indexed owner, address indexed operator, bool approved);

    // Token id has not been minted, or it has been burned
    error InvalidTokenId(uint256 token_id);
    // The specified address is not the owner of the specified token id
    error NotOwner(address from, uint256 token_id, address real_owner);
    // The specified address does not have allowance to spend the specified token id
    error NotApproved(address owner, address spender, uint256 token_id);
    // Attempt to transfer token id to the Zero address
    error TransferToZero(uint256 token_id);
    // The receiver address refused to receive the specified token id
    error ReceiverRefused(address receiver, uint256 token_id, bytes4 returned);
}

/// Represents the ways methods may fail.
#[derive(SolidityError)]
pub enum Erc721Error {
    InvalidTokenId(InvalidTokenId),
    NotOwner(NotOwner),
    NotApproved(NotApproved),
    TransferToZero(TransferToZero),
    ReceiverRefused(ReceiverRefused),
}

// External interfaces
sol_interface! {
    /// Allows calls to the `onERC721Received` method of other contracts implementing `IERC721TokenReceiver`.
    interface IERC721TokenReceiver {
        function onERC721Received(address operator, address from, uint256 token_id, bytes data) external returns(bytes4);
    }
}

/// Selector for `onERC721Received`, which is returned by contracts implementing `IERC721TokenReceiver`.
const ERC721_TOKEN_RECEIVER_ID: u32 = 0x150b7a02;

// These methods aren't external, but are helpers used by external methods.
// Methods marked as "pub" here are usable outside of the erc721 module (i.e. they're callable from lib.rs).
impl<T: Erc721Params> Erc721<T> {
    /// Requires that msg::sender() is authorized to spend a given token
    fn require_authorized_to_spend(&self, from: Address, token_id: U256) -> Result<(), Erc721Error> {
        // `from` must be the owner of the token_id
        let owner = self.owner_of(token_id)?;
        if from != owner {
            return Err(Erc721Error::NotOwner(NotOwner {
                from,
                token_id,
                real_owner: owner,
            }));
        }

        // caller is the owner
        if msg::sender() == owner {
            return Ok(());
        }

        // caller is an operator for the owner (can manage their tokens)
        if self.operator_approvals.getter(owner).get(msg::sender()) {
            return Ok(());
        }

        // caller is approved to manage this token_id
        if msg::sender() == self.token_approvals.get(token_id) {
            return Ok(());
        }

        // otherwise, caller is not allowed to manage this token_id
        Err(Erc721Error::NotApproved(NotApproved {
            owner,
            spender: msg::sender(),
            token_id,
        }))
    }

    /// Transfers `token_id` from `from` to `to`.
    /// This function does check that `from` is the owner of the token, but it does not check
    /// that `to` is not the zero address, as this function is usable for burning.
    pub fn transfer(&mut self, token_id: U256, from: Address, to: Address) -> Result<(), Erc721Error> {
        let mut owner = self.owners.setter(token_id);
        let previous_owner = owner.get();
        if previous_owner != from {
            return Err(Erc721Error::NotOwner(NotOwner {
                from,
                token_id,
                real_owner: previous_owner,
            }));
        }
        owner.set(to);

        // right now working with storage can be verbose, but this will change upcoming version of the Stylus SDK
        let mut from_balance = self.balances.setter(from);
        let balance = from_balance.get() - U256::from(1);
        from_balance.set(balance);

        let mut to_balance = self.balances.setter(to);
        let balance = to_balance.get() + U256::from(1);
        to_balance.set(balance);

        // cleaning app the approved mapping for this token
        self.token_approvals.delete(token_id);

        evm::log(Transfer { from, to, token_id });
        Ok(())
    }

    /// Calls `onERC721Received` on the `to` address if it is a contract.
    /// Otherwise it does nothing
    fn call_receiver<S: TopLevelStorage>(
        storage: &mut S,
        token_id: U256,
        from: Address,
        to: Address,
        data: Vec<u8>,
    ) -> Result<(), Erc721Error> {
        if to.has_code() {
            let receiver = IERC721TokenReceiver::new(to);
            let received = receiver
                .on_erc_721_received(&mut *storage, msg::sender(), from, token_id, data.into())
                .map_err(|_e| Erc721Error::ReceiverRefused(ReceiverRefused {
                    receiver: receiver.address,
                    token_id,
                    returned: alloy_primitives::FixedBytes(0_u32.to_be_bytes()),
                }))?
                .0;

            if u32::from_be_bytes(received) != ERC721_TOKEN_RECEIVER_ID {
                return Err(Erc721Error::ReceiverRefused(ReceiverRefused {
                    receiver: receiver.address,
                    token_id,
                    returned: alloy_primitives::FixedBytes(received),
                }));
            }
        }
        Ok(())
    }

    /// Transfers and calls `onERC721Received`
    pub fn safe_transfer<S: TopLevelStorage + BorrowMut<Self>>(
        storage: &mut S,
        token_id: U256,
        from: Address,
        to: Address,
        data: Vec<u8>,
    ) -> Result<(), Erc721Error> {
        storage.borrow_mut().transfer(token_id, from, to)?;
        Self::call_receiver(storage, token_id, from, to, data)
    }

    /// Mints a new token and transfers it to `to`
    pub fn mint(&mut self, to: Address) -> Result<(), Erc721Error> {
        let new_token_id = self.total_supply.get();
        self.total_supply.set(new_token_id + U256::from(1u8));
        self.transfer(new_token_id, Address::default(), to)?;
        Ok(())
    }

    /// Burns the token `token_id` from `from`
    /// Note that total_supply is not reduced since it's used to calculate the next token_id to mint
    pub fn burn(&mut self, from: Address, token_id: U256) -> Result<(), Erc721Error> {
        self.transfer(token_id, from, Address::default())?;
        Ok(())
    }
}

// these methods are external to other contracts
#[public]
impl<T: Erc721Params> Erc721<T> {
    /// Immutable NFT name.
    pub fn name() -> Result<String, Erc721Error> {
        Ok(T::NAME.into())
    }

    /// Immutable NFT symbol.
    pub fn symbol() -> Result<String, Erc721Error> {
        Ok(T::SYMBOL.into())
    }

    /// The NFT's Uniform Resource Identifier.
    #[selector(name = "tokenURI")]
    pub fn token_uri(&self, token_id: U256) -> Result<String, Erc721Error> {
        self.owner_of(token_id)?; // require NFT exist
        Ok(T::token_uri(token_id))
    }

    /// Gets the number of NFTs owned by an account.
    pub fn balance_of(&self, owner: Address) -> Result<U256, Erc721Error> {
        Ok(self.balances.get(owner))
    }

    /// Gets the owner of the NFT, if it exists.
    pub fn owner_of(&self, token_id: U256) -> Result<Address, Erc721Error> {
        let owner = self.owners.get(token_id);
        if owner.is_zero() {
            return Err(Erc721Error::InvalidTokenId(InvalidTokenId { token_id }));
        }
        Ok(owner)
    }

    /// Transfers an NFT, but only after checking the `to` address can receive the NFT.
    /// It includes additional data for the receiver.
    #[selector(name = "safeTransferFrom")]
    pub fn safe_transfer_from_with_data<S: TopLevelStorage + BorrowMut<Self>>(
        storage: &mut S,
        from: Address,
        to: Address,
        token_id: U256,
        data: Bytes,
    ) -> Result<(), Erc721Error> {
        if to.is_zero() {
            return Err(Erc721Error::TransferToZero(TransferToZero { token_id }));
        }
        storage
            .borrow_mut()
            .require_authorized_to_spend(from, token_id)?;

        Self::safe_transfer(storage, token_id, from, to, data.0)
    }

    /// Equivalent to [`safe_transfer_from_with_data`], but without the additional data.
    ///
    /// Note: because Rust doesn't allow multiple methods with the same name,
    /// we use the `#[selector]` macro attribute to simulate solidity overloading.
    #[selector(name = "safeTransferFrom")]
    pub fn safe_transfer_from<S: TopLevelStorage + BorrowMut<Self>>(
        storage: &mut S,
        from: Address,
        to: Address,
        token_id: U256,
    ) -> Result<(), Erc721Error> {
        Self::safe_transfer_from_with_data(storage, from, to, token_id, Bytes(vec![]))
    }

    /// Transfers the NFT.
    pub fn transfer_from(&mut self, from: Address, to: Address, token_id: U256) -> Result<(), Erc721Error> {
        if to.is_zero() {
            return Err(Erc721Error::TransferToZero(TransferToZero { token_id }));
        }
        self.require_authorized_to_spend(from, token_id)?;
        self.transfer(token_id, from, to)?;
        Ok(())
    }

    /// Grants an account the ability to manage the sender's NFT.
    pub fn approve(&mut self, approved: Address, token_id: U256) -> Result<(), Erc721Error> {
        let owner = self.owner_of(token_id)?;

        // require authorization
        if msg::sender() != owner && !self.operator_approvals.getter(owner).get(msg::sender()) {
            return Err(Erc721Error::NotApproved(NotApproved {
                owner,
                spender: msg::sender(),
                token_id,
            }));
        }
        self.token_approvals.insert(token_id, approved);

        evm::log(Approval {
            approved,
            owner,
            token_id,
        });
        Ok(())
    }

    /// Grants an account the ability to manage all of the sender's NFTs.
    pub fn set_approval_for_all(&mut self, operator: Address, approved: bool) -> Result<(), Erc721Error> {
        let owner = msg::sender();
        self.operator_approvals
            .setter(owner)
            .insert(operator, approved);

        evm::log(ApprovalForAll {
            owner,
            operator,
            approved,
        });
        Ok(())
    }

    /// Gets the account managing an NFT, or zero if unmanaged.
    pub fn get_approved(&mut self, token_id: U256) -> Result<Address, Erc721Error> {
        Ok(self.token_approvals.get(token_id))
    }

    /// Determines if an account has been authorized to managing all of a user's NFTs.
    pub fn is_approved_for_all(&mut self, owner: Address, operator: Address) -> Result<bool, Erc721Error> {
        Ok(self.operator_approvals.getter(owner).get(operator))
    }

    /// Whether the NFT supports a given standard.
    pub fn supports_interface(interface: FixedBytes<4>) -> Result<bool, Erc721Error> {
        let interface_slice_array: [u8; 4] = interface.as_slice().try_into().unwrap();

        if u32::from_be_bytes(interface_slice_array) == 0xffffffff {
            // special cased in the ERC165 standard
            return Ok(false);
        }

        const IERC165: u32 = 0x01ffc9a7;
        const IERC721: u32 = 0x80ac58cd;
        const IERC721_METADATA: u32 = 0x5b5e139f;

        Ok(matches!(u32::from_be_bytes(interface_slice_array), IERC165 | IERC721 | IERC721_METADATA))
    }
}
```

### lib.rs

```rust
// Only run this as a WASM if the export-abi feature is not set.
#![cfg_attr(not(any(feature = "export-abi", test)), no_main)]
extern crate alloc;

// Modules and imports
mod erc721;

use alloy_primitives::{U256, Address};
/// Import the Stylus SDK along with alloy primitive types for use in our program.
use stylus_sdk::{
    msg, prelude::*
};
use crate::erc721::{Erc721, Erc721Params, Erc721Error};

/// Immutable definitions
struct StylusNFTParams;
impl Erc721Params for StylusNFTParams {
    const NAME: &'static str = "StylusNFT";
    const SYMBOL: &'static str = "SNFT";

    fn token_uri(token_id: U256) -> String {
        format!("{}{}{}", "https://my-nft-metadata.com/", token_id, ".json")
    }
}

// Define the entrypoint as a Solidity storage object. The sol_storage! macro
// will generate Rust-equivalent structs with all fields mapped to Solidity-equivalent
// storage slots and types.
sol_storage! {
    #[entrypoint]
    struct StylusNFT {
        #[borrow] // Allows erc721 to access StylusNFT's storage and make calls
        Erc721<StylusNFTParams> erc721;
    }
}

#[public]
#[inherit(Erc721<StylusNFTParams>)]
impl StylusNFT {
    /// Mints an NFT
    pub fn mint(&mut self) -> Result<(), Erc721Error> {
        let minter = msg::sender();
        self.erc721.mint(minter)?;
        Ok(())
    }

    /// Mints an NFT to another address
    pub fn mint_to(&mut self, to: Address) -> Result<(), Erc721Error> {
        self.erc721.mint(to)?;
        Ok(())
    }

    /// Burns an NFT
    pub fn burn(&mut self, token_id: U256) -> Result<(), Erc721Error> {
        // This function checks that msg::sender() owns the specified token_id
        self.erc721.burn(msg::sender(), token_id)?;
        Ok(())
    }

    /// Total supply
    pub fn total_supply(&mut self) -> Result<U256, Erc721Error> {
        Ok(self.erc721.total_supply.get())
    }
}
```

### Cargo.toml

```toml
[package]
name = "stylus_erc721_example"
version = "0.1.7"
edition = "2021"
license = "MIT OR Apache-2.0"
keywords = ["arbitrum", "ethereum", "stylus", "alloy"]

[dependencies]
alloy-primitives = "=0.7.6"
alloy-sol-types = "=0.7.6"
mini-alloc = "0.4.2"
stylus-sdk = "0.6.0"
hex = "0.4.3"

[dev-dependencies]
tokio = { version = "1.12.0", features = ["full"] }
ethers = "2.0"
eyre = "0.6.8"

[features]
export-abi = ["stylus-sdk/export-abi"]

[lib]
crate-type = ["lib", "cdylib"]

[profile.release]
codegen-units = 1
strip = true
lto = true
panic = "abort"
opt-level = "s"
```

---

## .mdx (applications/multi_call.mdx)
---
title: 'Multi Call • Stylus by Example'
description: 'An example implementation of the Multi Call contract in Rust using Arbitrum Stylus.'
---

{/* Begin Content */}

# Multicall

An Arbitrum Stylus version implementation of [Solidity Multi Call contract](https://solidity-by-example.org/app/multi-call/) that aggregates multiple queries using a for loop and RawCall.

Example implementation of a Multi Call contract written in Rust:
Here is the interface for TimeLock.

```solidity
/**
 * This file was automatically generated by Stylus and represents a Rust program.
 * For more information, please see [The Stylus SDK](https://github.com/OffchainLabs/stylus-sdk-rs).
 */

// SPDX-License-Identifier: MIT-OR-APACHE-2.0
pragma solidity ^0.8.23;

interface IMultiCall {
    function multicall(address[] memory addresses, bytes[] memory data) external view returns (bytes[] memory);

    error ArraySizeNotMatch();

    error CallFailed(uint256);
}
```

### src/lib.rs

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
#![cfg_attr(not(feature = "export-abi"), no_main)]
extern crate alloc;

#[global_allocator]
static ALLOC: mini_alloc::MiniAlloc = mini_alloc::MiniAlloc::INIT;

use alloy_primitives::U256;
use alloy_sol_types::sol;
use stylus_sdk::{abi::Bytes, alloy_primitives::Address, call::RawCall, prelude::*};

#[solidity_storage]
#[entrypoint]
pub struct MultiCall;

// Declare events and Solidity error types
sol! {
    error ArraySizeNotMatch();
    error CallFailed(uint256 call_index);
}

#[derive(SolidityError)]
pub enum MultiCallErrors {
    ArraySizeNotMatch(ArraySizeNotMatch),
    CallFailed(CallFailed),
}

#[external]
impl MultiCall {
    pub fn multicall(
        &self,
        addresses: Vec<Address>,
        data: Vec<Bytes>,
    ) -> Result<Vec<Bytes>, MultiCallErrors> {
        let addr_len = addresses.len();
        let data_len = data.len();
        let mut results: Vec<Bytes> = Vec::new();
        if addr_len != data_len {
            return Err(MultiCallErrors::ArraySizeNotMatch(ArraySizeNotMatch {}));
        }
        for i in 0..addr_len {
            let result = RawCall::new().call(addresses[i], data[i].to_vec().as_slice())
                .map_err(|_| MultiCallErrors::CallFailed(CallFailed { call_index: U256::from(i) }))?;
            results.push(result.into());
}
        Ok(results)
    }
}

```

### Cargo.toml

```toml
[package]
name = "stylus-multi-call-contract"
version = "0.1.5"
edition = "2021"
license = "MIT OR Apache-2.0"
keywords = ["arbitrum", "ethereum", "stylus", "alloy"]
description = "Stylus multi call example"

[dependencies]
alloy-primitives = "0.3.1"
alloy-sol-types = "0.3.1"
mini-alloc = "0.4.2"
stylus-sdk = "0.5.0"
hex = "0.4.3"

[dev-dependencies]
tokio = { version = "1.12.0", features = ["full"] }
ethers = "2.0"
eyre = "0.6.8"

[features]
export-abi = ["stylus-sdk/export-abi"]

[[bin]]
name = "stylus-multi-call"
path = "src/main.rs"

[lib]
crate-type = ["lib", "cdylib"]

[profile.release]
codegen-units = 1
strip = true
lto = true
panic = "abort"
opt-level = "s"
```

---

## .mdx (applications/vending_machine.mdx)
---
title: 'Vending Machine • Stylus by Example'
description: 'An example implementation of the Vending Machine in Rust using Arbitrum Stylus.'
---

{/* Begin Content */}

# Vending Machine

An example project for writing Arbitrum Stylus programs in Rust using the [stylus-sdk](https://github.com/OffchainLabs/stylus-sdk-rs). It includes a Rust implementation of a vending machine Ethereum smart contract.

- distribute Cupcakes to any given address
- count Cupcakes balance of any given address

Here is the interface for Vending Machine.

```solidity
interface IVendingMachine {
    // Function to distribute a cupcake to a user
    function giveCupcakeTo(address userAddress) external returns (bool);

    // Getter function for the cupcake balance of a user
    function getCupcakeBalanceFor(address userAddress) external view returns (uint);
}
```

Example implementation of the Vending Machine contract written in Rust.

### src/lib.rs

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
//!
//! Stylus Cupcake Example
//!
//! The program is ABI-equivalent with Solidity, which means you can call it from both Solidity and Rust.
//! To do this, run `cargo stylus export-abi`.
//!
//! Note: this code is a template-only and has not been audited.
//!

// Allow `cargo stylus export-abi` to generate a main function if the "export-abi" feature is enabled.
#![cfg_attr(not(feature = "export-abi"), no_main)]
extern crate alloc;

use alloy_primitives::{Address, Uint};
// Import items from the SDK. The prelude contains common traits and macros.
use stylus_sdk::alloy_primitives::U256;
use stylus_sdk::prelude::*;
use stylus_sdk::{block, console};

// Define persistent storage using the Solidity ABI.
// `VendingMachine` will be the entrypoint for the contract.
sol_storage! {
    #[entrypoint]
    pub struct VendingMachine {
        // Mapping from user addresses to their cupcake balances.
        mapping(address => uint256) cupcake_balances;
        // Mapping from user addresses to the last time they received a cupcake.
        mapping(address => uint256) cupcake_distribution_times;
    }
}

// Declare that `VendingMachine` is a contract with the following external methods.
#[public]
impl VendingMachine {
    // Give a cupcake to the specified user if they are eligible (i.e., if at least 5 seconds have passed since their last cupcake).
    pub fn give_cupcake_to(&mut self, user_address: Address) -> bool {
        // Get the last distribution time for the user.
        let last_distribution = self.cupcake_distribution_times.get(user_address);
        // Calculate the earliest next time the user can receive a cupcake.
        let five_seconds_from_last_distribution = last_distribution + U256::from(5);

        // Get the current block timestamp.
        let current_time = block::timestamp();
        // Check if the user can receive a cupcake.
        let user_can_receive_cupcake =
            five_seconds_from_last_distribution <= Uint::<256, 4>::from(current_time);

        if user_can_receive_cupcake {
            // Increment the user's cupcake balance.
            let mut balance_accessor = self.cupcake_balances.setter(user_address);
            let balance = balance_accessor.get() + U256::from(1);
            balance_accessor.set(balance);

            // Update the distribution time to the current time.
            let mut time_accessor = self.cupcake_distribution_times.setter(user_address);
            let new_distribution_time = block::timestamp();
            time_accessor.set(Uint::<256, 4>::from(new_distribution_time));
            return true;
        } else {
            // User must wait before receiving another cupcake.
            console!(
                "HTTP 429: Too Many Cupcakes (you must wait at least 5 seconds between cupcakes)"
            );
            return false;
        }
    }

    // Get the cupcake balance for the specified user.
    pub fn get_cupcake_balance_for(&self, user_address: Address) -> Uint<256, 4> {
        // Return the user's cupcake balance from storage.
        return self.cupcake_balances.get(user_address);
    }
}
```

### Cargo.toml

```toml
[package]
name = "stylus_cupcake_example"
version = "0.1.7"
edition = "2021"
license = "MIT OR Apache-2.0"
keywords = ["arbitrum", "ethereum", "stylus", "alloy"]

[dependencies]
alloy-primitives = "=0.7.6"
alloy-sol-types = "=0.7.6"
mini-alloc = "0.4.2"
stylus-sdk = "0.6.0"
hex = "0.4.3"

[dev-dependencies]
tokio = { version = "1.12.0", features = ["full"] }
ethers = "2.0"
eyre = "0.6.8"

[features]
export-abi = ["stylus-sdk/export-abi"]

[lib]
crate-type = ["lib", "cdylib"]

[profile.release]
codegen-units = 1
strip = true
lto = true
panic = "abort"
opt-level = "s"
```

---

## .mdx (basic_examples/abi_decode.mdx)
---
title: 'ABI Encode • Stylus by Example'
description: 'A simple solidity ABI encode and decode example'
---

{/* Begin Content */}

# ABI Decode

The `decode` can not be used for `encode_packed` data because it ignores padding when encode. (For more information you can refer to [ABI Encode](./abi_encode))

So here we show an example for using `decode` on data encoded with `abi_encode_sequence`:

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
// This should always return true
pub fn encode_and_decode(
    &self,
    target: Address,
    value: U256,
    func: String,
    data: Bytes,
    timestamp: U256
) -> Result<bool, HasherError> {
    // define sol types tuple
    type TxIdHashType = (SOLAddress, Uint<256>, SOLString, SOLBytes, Uint<256>);
    // because the abi_encode_sequence will return alloy_primitives::Bytes rather than stylus_sdk::bytes, so we need to make sure the input and return types are the same
    let primative_data = alloy_primitives::Bytes::copy_from_slice(&data);
    // set the tuple
    let tx_hash_data = (target, value, func, primative_data, timestamp);
    // encode the tuple
    let tx_hash_data_encode = TxIdHashType::abi_encode_sequence(&tx_hash_data);

    let validate = true;

    // Check the result
    match TxIdHashType::abi_decode_sequence(&tx_hash_data_encode, validate) {
        Ok(res) => Ok(res == tx_hash_data),
        Err(_) => {
            return Err(HasherError::DecodedFailed(DecodedFailed{}));
        },
    }
}
```

# Full Example code:

### src/lib.rs

```rust

#![cfg_attr(not(any(feature = "export-abi", test)), no_main)]
extern crate alloc;


/// Import items from the SDK. The prelude contains common traits and macros.
use stylus_sdk::{alloy_primitives::{U256, Address}, prelude::*};
// Because the naming of `alloy_primitives` and `alloy_sol_types` is the same, we need to rename the types in `alloy_sol_types`.
use alloy_sol_types::{sol_data::{Address as SOLAddress, *}, SolType, sol};


// Define error
sol! {
    error DecodedFailed();
}

// Error types for the MultiSig contract
#[derive(SolidityError)]
pub enum DecoderError{
    DecodedFailed(DecodedFailed)
}

#[storage]
#[entrypoint]
pub struct Decoder;


/// Declare that `Decoder` is a contract with the following external methods.
#[public]
impl Decoder {
    // This should always return true
    pub fn encode_and_decode(
        &self,
        address: Address,
        amount: U256
    ) -> Result<bool, DecoderError> {
        // define sol types tuple
        type TxIdHashType = (SOLAddress, Uint<256>);
        // set the tuple
        let tx_hash_data = (address, amount);
        // encode the tuple
        let tx_hash_data_encode = TxIdHashType::abi_encode_params(&tx_hash_data);

        let validate = true;

        // Check the result
        match TxIdHashType::abi_decode_params(&tx_hash_data_encode, validate) {
            Ok(res) => Ok(res == tx_hash_data),
            Err(_) => {
                return Err(DecoderError::DecodedFailed(DecodedFailed{}));
            },
        }
    }

}
```

### Cargo.toml

```rust
[package]
name = "stylus-decode-hashing"
version = "0.1.0"
edition = "2021"

[dependencies]
alloy-primitives = "=0.7.6"
alloy-sol-types = "=0.7.6"
mini-alloc = "0.4.2"
stylus-sdk = "0.5.1"

[features]
export-abi = ["stylus-sdk/export-abi"]
debug = ["stylus-sdk/debug"]

[lib]
crate-type = ["lib", "cdylib"]

[profile.release]
codegen-units = 1
strip = true
lto = true
panic = "abort"
opt-level = "s"

```

---

## .mdx (basic_examples/abi_encode.mdx)
---
title: 'ABI Decode • Stylus by Example'
description: 'A simple solidity ABI encode and decode example'
---

{/* Begin Content */}

# ABI Encode

The `ABI Encode` has 2 types which are [`encode`](https://docs.soliditylang.org/en/latest/abi-spec.html#strict-encoding-mode) and [`encode_packed`](https://docs.soliditylang.org/en/latest/abi-spec.html#non-standard-packed-mode).

- `encode` will concatenate all values and add padding to fit into 32 bytes for each values.
- `encode_packed` will concatenate all values in the exact byte representations without padding. (For example, `encode_packed("a", "bc") == encode_packed("ab", "c")`)

Suppose we have a tuple of values: `(target, value, func, data, timestamp)` to encode, and their `alloy primitives type` are `(Address, U256, String, Bytes, U256)`.

Firstly we need to import those types we need from `alloy_primitives`, `stylus_sdk::abi` and `alloc::string`:

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
// Import items from the SDK. The prelude contains common traits and macros.
use stylus_sdk::{alloy_primitives::{U256, Address, FixedBytes}, abi::Bytes, prelude::*};
// Import String from alloc
use alloc::string::String;
```

Secondly because we will use the method [`abi_encode_sequence`](https://docs.rs/alloy-sol-types/latest/alloy_sol_types/trait.SolValue.html#method.abi_encode_sequence) and [`abi_encode_packed`](https://docs.rs/alloy-sol-types/latest/alloy_sol_types/trait.SolValue.html#method.abi_encode_packed) under `alloy_sol_types` to encode data, we also need to import the types from `alloy_sol_types`:

```rust
// Becauce the naming of alloy_primitives and alloy_sol_types is the same, so we need to re-name the types in alloy_sol_types
use alloy_sol_types::{sol_data::{Address as SOLAddress, String as SOLString, Bytes as SOLBytes, *}, SolType};
```

## encode

Then `encode` them:

```rust
// define sol types tuple
type TxIdHashType = (SOLAddress, Uint<256>, SOLString, SOLBytes, Uint<256>);
// set the tuple
let tx_hash_data = (target, value, func, data, timestamp);
// encode the tuple
let tx_hash_bytes = TxIdHashType::abi_encode_sequence(&tx_hash_data);
```

## encode_packed

There are 2 methods to `encode_packed` data:

1. `encode_packed` them:

```rust
// define sol types tuple
type TxIdHashType = (SOLAddress, Uint<256>, SOLString, SOLBytes, Uint<256>);
// set the tuple
let tx_hash_data = (target, value, func, data, timestamp);
// encode the tuple
let tx_hash_data_encode_packed = TxIdHashType::abi_encode_packed(&tx_hash_data);
```

2. We can also use the following method to `encode_packed` them:

```rust
let tx_hash_data_encode_packed = [&target.to_vec(), &value.to_be_bytes_vec(), func.as_bytes(), &data.to_vec(), &timestamp.to_be_bytes_vec()].concat();
```

# Full Example code:

### src/main.rs

```rust
// Allow `cargo stylus export-abi` to generate a main function.
#![cfg_attr(not(feature = "export-abi"), no_main)]
extern crate alloc;


/// Import items from the SDK. The prelude contains common traits and macros.
use stylus_sdk::{alloy_primitives::{U256, Address, FixedBytes}, abi::Bytes, prelude::*};
use alloc::string::String;
// Becauce the naming of alloy_primitives and alloy_sol_types is the same, so we need to re-name the types in alloy_sol_types
use alloy_sol_types::{sol_data::{Address as SOLAddress, String as SOLString, Bytes as SOLBytes, *}, SolType};
use sha3::{Digest, Keccak256};

// Define some persistent storage using the Solidity ABI.
// `Encoder` will be the entrypoint.
#[storage]
#[entrypoint]
pub struct Encoder;

impl Encoder {
    fn keccak256(&self, data: Bytes) -> FixedBytes<32> {
        // prepare hasher
        let mut hasher = Keccak256::new();
        // populate the data
        hasher.update(data);
        // hashing with keccack256
        let result = hasher.finalize();
        // convert the result hash to FixedBytes<32>
        let result_vec = result.to_vec();
        FixedBytes::<32>::from_slice(&result_vec)
    }
}

/// Declare that `Encoder` is a contract with the following external methods.
#[public]
impl Encoder {

     // Encode the data and hash it
     pub fn encode(
        &self,
        target: Address,
        value: U256,
        func: String,
        data: Bytes,
        timestamp: U256
    ) -> Vec<u8> {
        // define sol types tuple
        type TxIdHashType = (SOLAddress, Uint<256>, SOLString, SOLBytes, Uint<256>);
        // set the tuple
        let tx_hash_data = (target, value, func, data, timestamp);
        // encode the tuple
        let tx_hash_data_encode = TxIdHashType::abi_encode_params(&tx_hash_data);
        tx_hash_data_encode
    }

    // Packed encode the data and hash it, the same result with the following one
    pub fn packed_encode(
        &self,
        target: Address,
        value: U256,
        func: String,
        data: Bytes,
        timestamp: U256
    )-> Vec<u8> {
        // define sol types tuple
        type TxIdHashType = (SOLAddress, Uint<256>, SOLString, SOLBytes, Uint<256>);
        // set the tuple
        let tx_hash_data = (target, value, func, data, timestamp);
        // encode the tuple
        let tx_hash_data_encode_packed = TxIdHashType::abi_encode_packed(&tx_hash_data);
        tx_hash_data_encode_packed
    }

    // Packed encode the data and hash it, the same result with the above one
    pub fn packed_encode_2(
        &self,
        target: Address,
        value: U256,
        func: String,
        data: Bytes,
        timestamp: U256
    )-> Vec<u8> {
        // set the data to arrary and concat it directly
        let tx_hash_data_encode_packed = [&target.to_vec(), &value.to_be_bytes_vec(), func.as_bytes(), &data.to_vec(), &timestamp.to_be_bytes_vec()].concat();
        tx_hash_data_encode_packed
    }


    // The func example: "transfer(address,uint256)"
    pub fn encode_with_signature(
        &self,
        func: String,
        address: Address,
        amount: U256
    ) -> Vec<u8> {
        type TransferType = (SOLAddress, Uint<256>);
        let tx_data = (address, amount);
        let data = TransferType::abi_encode_params(&tx_data);
        // Get function selector
        let hashed_function_selector = self.keccak256(func.as_bytes().to_vec().into());
        // Combine function selector and input data (use abi_packed way)
        let calldata = [&hashed_function_selector[..4], &data].concat();
        calldata
    }

}
```

### Cargo.toml

```toml
[package]
name = "stylus-encode-hashing"
version = "0.1.7"
edition = "2021"
license = "MIT OR Apache-2.0"
keywords = ["arbitrum", "ethereum", "stylus", "alloy"]

[dependencies]
alloy-primitives = "=0.7.6"
alloy-sol-types = "=0.7.6"
mini-alloc = "0.4.2"
stylus-sdk = "0.6.0"
hex = "0.4.3"
sha3 = "0.10"

[dev-dependencies]
tokio = { version = "1.12.0", features = ["full"] }
ethers = "2.0"
eyre = "0.6.8"

[features]
export-abi = ["stylus-sdk/export-abi"]

[lib]
crate-type = ["lib", "cdylib"]

[profile.release]
codegen-units = 1
strip = true
lto = true
panic = "abort"
opt-level = "s"
```

---

## .mdx (basic_examples/bytes_in_bytes_out.mdx)
---
title: 'Bytes In, Bytes Out • Stylus by Example'
description: 'A simple bytes in, bytes out Arbitrum Stylus Rust contract that shows a minimal `entrypoint` function.'
---

{/* Begin Content */}

# Bytes In, Bytes Out

This is a simple bytes in, bytes out contract that shows a minimal `entrypoint` function (denoted by the `#[entrypoint]` proc macro). If your smart contract just has one primary function, like computing a cryptographic hash, this can be a great model because it strips out the SDK and acts like a pure function or Unix-style app.

### src/main.rs

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
#![cfg_attr(not(feature = "export-abi"), no_main)]

extern crate alloc;
use alloc::vec::Vec;

use stylus_sdk::stylus_proc::entrypoint;

#[entrypoint]
fn user_main(input: Vec<u8>) -> Result<Vec<u8>, Vec<u8>> {
    Ok(input)
}
```

### Cargo.toml

```toml
[package]
name = "bytes_in_bytes_out"
version = "0.1.7"
edition = "2021"

[dependencies]
stylus-sdk = "0.6.0"

[features]
export-abi = ["stylus-sdk/export-abi"]

[profile.release]
codegen-units = 1
strip = true
lto = true
panic = "abort"
opt-level = "s"

[workspace]
```

---

## .mdx (basic_examples/constants.mdx)
---
title: 'Constants • Stylus by Example'
description: "How to declare constants in your Rust smart contracts using Arbitrum's Stylus SDK"
---

{/* Begin Content */}

# Constants

Constants are values that are bound to a name and cannot change. They are always immutable. In Rust, constants are declared with the `const` keyword. Unlike variables declared with the `let` keyword, constants _must_ be annotated with their type.

Constants are valid for the entire length of the transaction. They are essentially _inlined_ wherever they are used, meaning that their value is copied directly into whatever context invokes them.

Since their value is hardcoded, they can save on gas cost as their value does not need to be fetched from storage.

## Learn More

- [Rust docs - Constant items](https://doc.rust-lang.org/reference/items/constant-items.html)
- [Solidity docs - Constant variables](https://docs.soliditylang.org/en/v0.8.19/contracts.html#constant)

### src/lib.rs

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
// Only run this as a WASM if the export-abi feature is not set.
#![cfg_attr(not(any(feature = "export-abi", test)), no_main)]
extern crate alloc;

use alloc::vec;
use alloc::vec::Vec;

use stylus_sdk::alloy_primitives::Address;
use stylus_sdk::prelude::*;
use stylus_sdk::storage::StorageAddress;

const OWNER: &str = "0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045";

#[storage]
#[entrypoint]
pub struct Contract {
    owner: StorageAddress,
}

#[public]
impl Contract {
    pub fn init(&mut self) -> Result<(), Vec<u8>> {
        // Parse the const &str as a local Address variable
        let owner_address = Address::parse_checksummed(OWNER, None).expect("Invalid address");

        // Save the result as the owner
        self.owner.set(owner_address);

        Ok(())
    }
    pub fn owner(&self) -> Result<Address, Vec<u8>> {
        let owner_address = self.owner.get();

        Ok(owner_address)
    }
}

```

### Cargo.toml

```toml
[package]
name = "stylus_constants_example"
version = "0.1.7"
edition = "2021"
license = "MIT OR Apache-2.0"
keywords = ["arbitrum", "ethereum", "stylus", "alloy"]

[dependencies]
alloy-primitives = "=0.7.6"
alloy-sol-types = "=0.7.6"
mini-alloc = "0.4.2"
stylus-sdk = "0.6.0"
hex = "0.4.3"

[dev-dependencies]
tokio = { version = "1.12.0", features = ["full"] }
ethers = "2.0"
eyre = "0.6.8"

[features]
export-abi = ["stylus-sdk/export-abi"]

[lib]
crate-type = ["lib", "cdylib"]

[profile.release]
codegen-units = 1
strip = true
lto = true
panic = "abort"
opt-level = "s"

```

---

## .mdx (basic_examples/errors.mdx)
---
title: 'Errors • Stylus by Example'
description: 'How to define and use Errors on Stylus Rust smart contracts'
---

{/* Begin Content */}

# Errors

In Rust Stylus contracts, error handling is a crucial aspect of writing robust and reliable smart contracts. Rust differentiates between recoverable and unrecoverable errors. Recoverable errors are represented using the `Result` type, which can either be `Ok`, indicating success, or `Err`, indicating failure. This allows developers to manage errors gracefully and maintain control over the flow of execution. Unrecoverable errors are handled with the `panic!` macro, which stops execution, unwinds the stack, and returns a dataless error.

In Stylus contracts, error types are often explicitly defined, providing clear and structured ways to handle different failure scenarios. This structured approach promotes better error management, ensuring that contracts are secure, maintainable, and behave predictably under various conditions. Similar to Solidity and EVM, errors in Stylus will undo all changes made to the state during a transaction by reverting the transaction. Thus, there are two main types of errors in Rust Stylus contracts:

- **Recoverable Errors**: The Stylus SDK provides features that make using recoverable errors in Rust Stylus contracts convenient. This type of error handling is strongly recommended for Stylus contracts.
- **Unrecoverable Errors**: These can be defined similarly to Rust code but are not recommended for smart contracts if recoverable errors can be used instead.

## Learn More

- [Solidity docs: Expressions and Control Structures](https://docs.soliditylang.org/en/latest/control-structures.html)
- [`#[derive(SolidityError)]`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/derive.SolidityError.html)
- [`alloy_sol_types::SolError`](https://docs.rs/alloy-sol-types/latest/alloy_sol_types/trait.SolError.html)
- [`Error handling: Rust book`](https://doc.rust-lang.org/book/ch09-00-error-handling.html)

## Recoverable Errors

Recoverable errors are represented using the `Result` type, which can either be `Ok`, indicating success, or `Err`, indicating failure. The Stylus SDK provides tools to define custom error types and manage recoverable errors effectively.

#### Example: Recoverable Errors

Here's a simplified Rust Stylus contract demonstrating how to define and handle recoverable errors:

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
#![cfg_attr(not(feature = "export-abi"), no_main)]
extern crate alloc;


use alloy_sol_types::sol;
use stylus_sdk::{abi::Bytes, alloy_primitives::{Address, U256}, call::RawCall, prelude::*};

#[storage]
#[entrypoint]
pub struct MultiCall;

// Declare events and Solidity error types
sol! {
    error ArraySizeNotMatch();
    error CallFailed(uint256 call_index);
}

#[derive(SolidityError)]
pub enum MultiCallErrors {
    ArraySizeNotMatch(ArraySizeNotMatch),
    CallFailed(CallFailed),
}

#[public]
impl MultiCall {
    pub fn multicall(
        &self,
        addresses: Vec<Address>,
        data: Vec<Bytes>,
    ) -> Result<Vec<Bytes>, MultiCallErrors> {
        let addr_len = addresses.len();
        let data_len = data.len();
        let mut results: Vec<Bytes> = Vec::new();
        if addr_len != data_len {
            return Err(MultiCallErrors::ArraySizeNotMatch(ArraySizeNotMatch {}));
        }
        for i in 0..addr_len {
            let result: Result<Vec<u8>, Vec<u8>> =
                RawCall::new().call(addresses[i], data[i].to_vec().as_slice());
            let data = match result {
                Ok(data) => data,
                Err(_data) => return Err(MultiCallErrors::CallFailed(CallFailed { call_index: U256::from(i) })),
            };
            results.push(data.into())
        }
        Ok(results)
    }
}
```

- **Using `SolidityError` Derive Macro**: The `#[derive(SolidityError)]` attribute is used for the `MultiCallErrors` enum, automatically implementing the necessary traits for error handling.
- **Defining Errors**: Custom errors `ArraySizeNotMatch` and `CallFailed` is declared in `MultiCallErrors` enum. `CallFailed` error includes a `call_index` parameter to indicate which call failed.
- **ArraySizeNotMatch Error Handling**: The `multicall` function returns `ArraySizeNotMatch` if the size of addresses and data vectors are not equal.
- **CallFailed Error Handling**: The `multicall` function returns a `CallFailed` error with the index of the failed call if any call fails. Note that we're using match to check if the result of the call is an error or a return data. We'll describe match pattern in the further sections.

## Unrecoverable Errors

Here are various ways to handle such errors in the `multicall` function, which calls multiple addresses and panics in different scenarios:

### Using `panic!`

Directly panics if the call fails, including the index of the failed call.

```rust
        for i in 0..addr_len {
            let result = RawCall::new().call(addresses[i], data[i].to_vec().as_slice());
            let data = match result {
                Ok(data) => data,
                Err(_data) => panic!("Call to address {:?} failed at index {}", addresses[i], i),
            };
            results.push(data.into());
}
```

Handling Call Failure with `panic!`: The function panics if any call fails and the transaction will be reverted without any data.

### Using `unwrap`

Uses `unwrap` to handle the result, panicking if the call fails.

```rust
        for i in 0..addr_len {
            let result = RawCall::new().call(addresses[i], data[i].to_vec().as_slice()).unwrap();
            results.push(result.into());
}
```

Handling Call Failure with `unwrap`: The function uses `unwrap` to panic if any call fails, including the index of the failed call.

### Using `match`

Uses a `match` statement to handle the result of `call`, panicking if the call fails.

```rust
        for i in 0..addr_len {
            let result = RawCall::new().call(addresses[i], data[i].to_vec().as_slice());
            let data = match result {
                Ok(data) => data,
                Err(_data) => return Err(MultiCallErrors::CallFailed(CallFailed { call_index: U256::from(i) })),
            };
            results.push(data.into());
}
```

Handling Call Failure with `match`: The function uses a `match` statement to handle the result of `call`, returning error if any call fails.

### Using the `?` Operator

Uses the `?` operator to propagate the error if the call fails, including the index of the failed call.

```rust
        for i in 0..addr_len {
            let result = RawCall::new().call(addresses[i], data[i].to_vec().as_slice())
                .map_err(|_| MultiCallErrors::CallFailed(CallFailed { call_index: U256::from(i) }))?;
            results.push(result.into());
}
```

Handling Call Failure with `?` Operator: The function uses the `?` operator to propagate the error if any call fails, including the index of the failed call.

Each method demonstrates a different way to handle unrecoverable errors in the `multicall` function of a Rust Stylus contract, providing a comprehensive approach to error management.

**Note** that as mentioned above, it is strongly recommended to use custom error handling instead of unrecoverable error handling.

## Boilerplate

### src/lib.rs

The lib.rs code can be found at the top of the page in the recoverable error example section.

### Cargo.toml

```toml
[package]
name = "stylus-multicall-contract"
version = "0.1.7"
edition = "2021"

[dependencies]
alloy-primitives = "=0.7.6"
alloy-sol-types = "=0.7.6"
stylus-sdk = "0.6.0"
hex = "0.4.3"

[dev-dependencies]
tokio = { version = "1.12.0", features = ["full"] }
ethers = "2.0"
eyre = "0.6.8"

[features]
export-abi = ["stylus-sdk/export-abi"]

[[bin]]
name = "stylus-multicall-contract"
path = "src/main.rs"

[lib]
crate-type = ["lib", "cdylib"]

```

---

## .mdx (basic_examples/events.mdx)
---
title: 'Events • Stylus by Example'
description: 'How to log events to the chain using the Arbitrum Stylus Rust SDK.'
---

{/* Begin Content */}

# Events

Events allow for data to be logged publicly to the blockchain. Log entries provide the contract's address, a series of up to four topics, and some arbitrary length binary data. The Stylus Rust SDK provides a few ways to publish event logs described below.

## Learn More

- [Solidity docs: Events](https://docs.soliditylang.org/en/v0.8.19/abi-spec.html#events)
- [`stylus_sdk::evm::log`](https://docs.rs/stylus-sdk/latest/stylus_sdk/evm/fn.log.html)
- [`alloy_sol_types::SolEvent`](https://docs.rs/alloy-sol-types/0.3.1/alloy_sol_types/trait.SolEvent.html)

## Log

Using the `evm::log` function in the Stylus SDK is the preferred way to log events. It ensures that an event will be logged in a Solidity ABI-compatible format. The `log` function takes any type that implements Alloy `SolEvent` trait. It's not recommended to attempt to implement this trait on your own. Instead, make use of the provided `sol!` macro to declare your Events and their schema using Solidity-style syntax to declare the parameter types. Alloy will create ABI-compatible Rust types which you can instantiate and pass to the `evm::log` function.

### Log Usage

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
// sol! macro event declaration
// Up to 3 parameters can be indexed.
// Indexed parameters helps you filter the logs efficiently
sol! {
    event Log(address indexed sender, string message);
    event AnotherLog();
}

#[storage]
#[entrypoint]
pub struct Events {}

#[public]
impl Events {
fn user_main(_input: Vec<u8>) -> ArbResult {
    // emits a 'Log' event, defined above in the sol! macro
    evm::log(Log {
        sender: Address::from([0x11; 20]),
        message: "Hello world!".to_string(),
    });

    // no data, but 'AnotherLog' event will still emit to the chain
    evm::log(AnotherLog {});

    Ok(vec![])
}
}
```

## Raw Log

The `evm::raw_log` affordance offers the ability to send anonymous events that do not necessarily conform to the Solidity ABI. Instead, up to four raw 32-byte indexed topics are published along with any arbitrary bytes appended as data.

**NOTE**: It's still possible to achieve Solidity ABI compatibility using this construct. To do so you'll have to manually compute the ABI signature for the event, [following the equation set in the Solidity docs](https://docs.soliditylang.org/en/v0.8.19/abi-spec.html#events). The result of that should be assigned to `TOPIC_0`, the first topic in the slice passed to `raw_log`.

### Raw Log Usage

```rust
// set up local variables
let user = Address::from([0x22; 20]);
let balance = U256::from(10_000_000);

// declare up to 4 topics
// topics must be of type FixedBytes<32>
let topics = &[user.into_word()];

// store non-indexed data in a byte Vec
let mut data: Vec<u8> = vec![];
// to_be_bytes means 'to big endian bytes'
data.extend_from_slice(balance.to_be_bytes::<32>().to_vec().as_slice());

// unwrap() here 'consumes' the Result
evm::raw_log(topics.as_slice(), data.as_ref()).unwrap();
```

## Result

Combining the above examples into the boiler plate provided below this section, deploying to a Stylus chain and then invoking the deployed contract will result in the following three events logged to the chain:

### logs

```json
[
  {
    "address": "0x6cf4a18ac8efd6b0b99d3200c4fb9609dd60d4b3",
    "topics": [
      "0x0738f4da267a110d810e6e89fc59e46be6de0c37b1d5cd559b267dc3688e74e0",
      "0x0000000000000000000000001111111111111111111111111111111111111111"
    ],
    "data": "0x0000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000000c48656c6c6f20776f726c64210000000000000000000000000000000000000000",
    "blockHash": "0xfef880025dc87b5ab4695a0e1a6955dd7603166ecba79ce0f503a568b2ec8940",
    "blockNumber": "0x94",
    "transactionHash": "0xc7318dae2164eb441fb80f5b869f844e3e97ae83c24a4639d46ec4d915a30818",
    "transactionIndex": "0x1",
    "logIndex": "0x0",
    "removed": false
  },
  {
    "address": "0x6cf4a18ac8efd6b0b99d3200c4fb9609dd60d4b3",
    "topics": ["0xfe1a3ad11e425db4b8e6af35d11c50118826a496df73006fc724cb27f2b99946"],
    "data": "0x",
    "blockHash": "0xfef880025dc87b5ab4695a0e1a6955dd7603166ecba79ce0f503a568b2ec8940",
    "blockNumber": "0x94",
    "transactionHash": "0xc7318dae2164eb441fb80f5b869f844e3e97ae83c24a4639d46ec4d915a30818",
    "transactionIndex": "0x1",
    "logIndex": "0x1",
    "removed": false
  },
  {
    "address": "0x6cf4a18ac8efd6b0b99d3200c4fb9609dd60d4b3",
    "topics": ["0x0000000000000000000000002222222222222222222222222222222222222222"],
    "data": "0x0000000000000000000000000000000000000000000000000000000000989680",
    "blockHash": "0xfef880025dc87b5ab4695a0e1a6955dd7603166ecba79ce0f503a568b2ec8940",
    "blockNumber": "0x94",
    "transactionHash": "0xc7318dae2164eb441fb80f5b869f844e3e97ae83c24a4639d46ec4d915a30818",
    "transactionIndex": "0x1",
    "logIndex": "0x2",
    "removed": false
  }
]
```

## Boilerplate

### src/lib.rs

```rust
// Only run this as a WASM if the export-abi feature is not set.
#![cfg_attr(not(any(feature = "export-abi", test)), no_main)]
extern crate alloc;

use alloc::vec::Vec;
use alloc::{string::ToString, vec};

use stylus_sdk::alloy_primitives::U256;
use stylus_sdk::{alloy_primitives::Address, alloy_sol_types::sol, evm, prelude::*, ArbResult};

// sol! macro event declaration
// Up to 3 parameters can be indexed.
// Indexed parameters helps you filter the logs by the indexed parameter
sol! {
    event Log(address indexed sender, string message);
    event AnotherLog();
}

#[storage]
#[entrypoint]
pub struct Events {}

#[public]
impl Events {
fn user_main(_input: Vec<u8>) -> ArbResult {
    // emits a 'Log' event, defined above in the sol! macro
    evm::log(Log {
        sender: Address::from([0x11; 20]),
        message: "Hello world!".to_string(),
    });

    // no data, but event will still log to the chain
    evm::log(AnotherLog {});

    // set up local variables
    let user = Address::from([0x22; 20]);
    let balance = U256::from(10_000_000);

    // declare up to 4 topics
    // topics must be of type FixedBytes<32>
    let topics = &[user.into_word()];

    // store non-indexed data in a byte Vec
    let mut data: Vec<u8> = vec![];
    // to_be_bytes means 'to big endian bytes'
    data.extend_from_slice(balance.to_be_bytes::<32>().to_vec().as_slice());

    // unwrap() here 'consumes' the Result
    evm::raw_log(topics.as_slice(), data.as_ref()).unwrap();

    Ok(Vec::new())
}
}
```

### Cargo.toml

```toml
[package]
name = "stylus_events_example"
version = "0.1.7"
edition = "2021"
license = "MIT OR Apache-2.0"
keywords = ["arbitrum", "ethereum", "stylus", "alloy"]

[dependencies]
alloy-primitives = "=0.7.6"
alloy-sol-types = "=0.7.6"
mini-alloc = "0.4.2"
stylus-sdk = "0.6.0"
hex = "0.4.3"

[dev-dependencies]
tokio = { version = "1.12.0", features = ["full"] }
ethers = "2.0"
eyre = "0.6.8"

[features]
export-abi = ["stylus-sdk/export-abi"]

[lib]
crate-type = ["lib", "cdylib"]

[profile.release]
codegen-units = 1
strip = true
lto = true
panic = "abort"
opt-level = "s"
```

---

## .mdx (basic_examples/function.mdx)
---
title: 'Function • Stylus by Example'
description: 'How to define and use internal and external functions in Stylus, and how to return multiple values from functions.'
---

{/* Begin Content */}

# Functions

Functions are a fundamental part of any programming language, including Stylus, enabling you to encapsulate logic into reusable components.

This guide covers the syntax and usage of functions, including internal and external functions, and how to return multiple values.

## Learn More

- [Rust docs - Functions](https://doc.rust-lang.org/reference/items/functions.html)
- [Solidity docs - Functions](https://solidity-by-example.org/function/)

## Overview

A function in Stylus consists of a name, a set of parameters, an optional return type, and a body.

Just as with storage, Stylus methods are Solidity ABI equivalent. This means that contracts written in different programming languages are fully interoperable.

Functions are declared with the `fn` keyword. Parameters allow the function to accept inputs, and the return type specifies the output of the function. If no return type is specified, the function returns `void`.

Following is an example of a function `add` that takes two `uint256` values and returns their sum.

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
fn add(a: uint256, b: uint256) -> uint256 {
    return a + b;
}
```

## Function Parameters

Function parameters are the inputs to a function. They are specified as a list of `IDENTIFIER: Type` pairs, separated by commas.

In this example, the function `add_numbers` takes two `u32` parameters, `a` and `b` and returns the sum of the two numbers.

```rust
fn add_numbers(a: u32, b: u32) -> u32 {
    a + b
}
```

## Return Types

Return types in functions are an essential part of defining the behavior and expected outcomes of your smart contract methods.

Here, we explain the syntax and usage of return types in Stylus with general examples.

### Basic Syntax

A function with a return type in Stylus follows this basic structure. The return type is specified after the `->` arrow.
Values are returned using the `return` keyword or implicitly as the last expression of the function. In Rust and Stylus, the last expression in a function is implicitly returned, so the `return` keyword is often omitted.

```rust
pub fn function_name(&self) -> ReturnType {
    // Function body
}
```

## Examples

**Function returning a String:** This `get_greeting` function returns a `String`. The return type is specified as `String` after the `->` arrow.

```rust
pub fn get_greeting() -> String {
    "Hello, Stylus!".into()
}
```

**Function returning an Integer:** This `get_number` function returns an unsigned 32-bit integer (`u32`).

```rust
pub fn get_number() -> u32 {
    42
}
```

**Function returning a Result with `Ok` and `Err` variants:** The `perform_operation` function returns a `Result<u32, CustomError>`.
The `Result` type is used for functions that can return either a success value (`Ok`) or an error (`Err`). In this case, it returns `Ok(value)` on success and an error variant of `CustomError` on failure.

```rust
pub enum CustomError {
    ErrorVariant,
}

pub fn perform_operation(value: u32) -> Result<u32, CustomError> {
    if value > 0 {
        Ok(value)
    } else {
        Err(CustomError::ErrorVariant)
    }
}
```

## Public Functions

Public functions are those that can be called by other contracts.

To define a public function in a Stylus contract, you use the `#[public]` macro. This macro ensures that the function is accessible from outside the contract.

Previously, all public methods were required to return a `Result` type with `Vec<u8>` as the error type. This is now optional. Specifically, if a method is "infallible" (i.e., it cannot produce an error), it does not need to return a Result type. Here's what this means:

- Infallible methods: Methods that are guaranteed not to fail (no errors possible) do not need to use the `Result` type. They can return their result directly without wrapping it in `Result`.

- Optional error handling: The `Result` type with `Vec<u8>` as the error type is now optional for methods that cannot produce an error.

In the following example, `owner` is a public function that returns the contract owner's address. Since this function is infallible (i.e., it cannot produce an error), it does not need to return a `Result` type.

```rust
#[external]
impl Contract {
    // Define an external function to get the owner of the contract
    pub fn owner(&self) -> Address {
        self.owner.get()
    }
}
```

## Internal Functions

Internal functions are those that can only be called within the contract itself. These functions are not exposed to external calls.

To define an internal function, you simply include it within your contract's implementation without the `#[public]` macro.

The choice between public and internal functions depends on the desired level of accessibility and interaction within and across contracts.

In the followinge example, `set_owner` is an internal function that sets a new owner for the contract. It is only callable within the contract itself.

```rust
impl Contract {
    // Define an internal function to set a new owner
    pub fn set_owner(&mut self, new_owner: Address) {
        self.owner.set(new_owner);
    }
}
```

To mix public and internal functions within the same contract, you should use two separate `impl` blocks with the same contract name. Public functions are defined within an `impl` block annotated with the `#[public]` attribute, signifying that these functions are part of the contract's public interface and can be invoked from outside the contract.
In contrast, internal functions are placed within a separate `impl` block that does not have the `#[public]` attribute, making them internal to the contract and inaccessible to external entities.

### src/lib.rs

```rust
// Only run this as a WASM if the export-abi feature is not set.
#![cfg_attr(not(any(feature = "export-abi", test)), no_main)]
extern crate alloc;

use alloc::vec;
use stylus_sdk::alloy_primitives::Address;
use stylus_sdk::prelude::*;
use stylus_sdk::storage::StorageAddress;

use stylus_sdk::alloy_primitives::U256;
use stylus_sdk::storage::StorageU256;
use stylus_sdk::console;


#[storage]
#[entrypoint]
pub struct ExampleContract {
    owner: StorageAddress,
    data: StorageU256,
}

#[public]
impl ExampleContract {
    // External function to set the data
    pub fn set_data(&mut self, value: U256) {
        self.data.set(value);
    }

    // External function to get the data
    pub fn get_data(&self) -> U256 {
        self.data.get()
    }

    // External function to get the contract owner
    pub fn get_owner(&self) -> Address {
        self.owner.get()
    }
}

impl ExampleContract {
    // Internal function to set a new owner
    pub fn set_owner(&mut self, new_owner: Address) {
        self.owner.set(new_owner);
    }

    // Internal function to log data
    pub fn log_data(&self) {
        let _data = self.data.get();
        console!("Current data is: {:?}", _data);
    }
}
```

### Cargo.toml

```toml
[package]
name = "stylus-functions"
version = "0.1.0"
edition = "2021"

[dependencies]
alloy-primitives = "=0.7.6"
alloy-sol-types = "=0.7.6"
mini-alloc = "0.4.2"
stylus-sdk = "0.6.0"
hex = "0.4.3"
sha3 = "0.10.8"

[features]
export-abi = ["stylus-sdk/export-abi"]
debug = ["stylus-sdk/debug"]

[lib]
crate-type = ["lib", "cdylib"]

[profile.release]
codegen-units = 1
strip = true
lto = true
panic = "abort"
opt-level = "s"
```

---

## .mdx (basic_examples/function_selector.mdx)
---
title: 'Function selector • Stylus by Example'
description: "How to compute the encoded function selector of a contract's function using the Arbitrum Stylus Rust SDK."
---

{/* Begin Content */}

# Function selector

When a smart contract is called, the first 4 bytes of the calldata sent as part of the request are called the "function selector", and identify which function of the smart contract to call.

You can compute a specific function selector by using the `function_selector!` macro.

Here's an example that computes the selector of a function named `foo`:

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
function_selector!("foo") // returns 0xc2985578
```

Functions usually take a number of arguments that you need to pass in order for the call to be successful. For example, here's the signature of a function that takes 2 arguments, an address and a uint256:

```solidity
function transfer(address recipient, uint256 amount) external returns (bool);
```

To compute the selector for this function, pass the types of the arguments to the `function_selector` macro:

```rust
function_selector!("transfer", Address, U256) // returns 0xa9059cbb
```

`function_selector` will return a byte array containing the encoded function selector.

## Learn More

- [`stylus_sdk::function_selector`](https://docs.rs/stylus-sdk/latest/stylus_sdk/macro.function_selector.html)

---

## .mdx (basic_examples/hashing.mdx)
---
title: 'Hasing with keccak256 • Stylus by Example'
description: 'A simple solidity ABI encode and decode example'
---

{/* Begin Content */}

## Hashing with keccak256

Keccak256 is a cryptographic hash function that takes an input of an arbitrary length and produces a fixed-length output of 256 bits.

Keccak256 is a member of the [SHA-3](https://en.wikipedia.org/wiki/SHA-3) family of hash functions.

keccak256 computes the Keccak-256 hash of the input.

Some use cases are:

- Creating a deterministic unique ID from a input
- Commit-Reveal scheme
- Compact cryptographic signature (by signing the hash instead of a larger input)

Here we will use [`stylus-sdk::crypto::keccak`](https://docs.rs/stylus-sdk/latest/stylus_sdk/crypto/fn.keccak.html) to calculate the keccak256 hash of the input data:

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
pub fn keccak<T: AsRef<[u8]>>(bytes: T) -> B256
```

# Full Example code:

### src/main.rs

```rust
// Only run this as a WASM if the export-abi feature is not set.
#![cfg_attr(not(any(feature = "export-abi", test)), no_main)]
extern crate alloc;

/// Import items from the SDK. The prelude contains common traits and macros.
use stylus_sdk::{alloy_primitives::{U256, Address, FixedBytes}, abi::Bytes, prelude::*, crypto::keccak};
use alloc::string::String;
use alloc::vec::Vec;
// Becauce the naming of alloy_primitives and alloy_sol_types is the same, so we need to re-name the types in alloy_sol_types
use alloy_sol_types::{sol_data::{Address as SOLAddress, String as SOLString, Bytes as SOLBytes, *}, SolType};
use alloy_sol_types::sol;

// Define error
sol! {
    error DecodedFailed();
}

// Error types for the MultiSig contract
#[derive(SolidityError)]
pub enum HasherError{
    DecodedFailed(DecodedFailed)
}

#[solidity_storage]
#[entrypoint]
pub struct Hasher {
}
/// Declare that `Hasher` is a contract with the following external methods.
#[public]
impl Hasher {

    // Encode the data and hash it
    pub fn encode_and_hash(
        &self,
        target: Address,
        value: U256,
        func: String,
        data: Bytes,
        timestamp: U256
    ) -> FixedBytes<32> {
        // define sol types tuple
        type TxIdHashType = (SOLAddress, Uint<256>, SOLString, SOLBytes, Uint<256>);
        // set the tuple
        let tx_hash_data = (target, value, func, data, timestamp);
        // encode the tuple
        let tx_hash_data_encode = TxIdHashType::abi_encode_sequence(&tx_hash_data);
        // hash the encoded data
        keccak(tx_hash_data_encode).into()
    }

    // This should always return true
    pub fn encode_and_decode(
        &self,
        address: Address,
        amount: U256
    ) -> Result<bool, HasherError> {
        // define sol types tuple
        type TxIdHashType = (SOLAddress, Uint<256>);
        // set the tuple
        let tx_hash_data = (address, amount);
        // encode the tuple
        let tx_hash_data_encode = TxIdHashType::abi_encode_sequence(&tx_hash_data);

        let validate = true;

        // Check the result
        match TxIdHashType::abi_decode_sequence(&tx_hash_data_encode, validate) {
            Ok(res) => Ok(res == tx_hash_data),
            Err(_) => {
                return Err(HasherError::DecodedFailed(DecodedFailed{}));
            },
        }
    }

    // Packed encode the data and hash it, the same result with the following one
    pub fn packed_encode_and_hash_1(
        &self,
        target: Address,
        value: U256,
        func: String,
        data: Bytes,
        timestamp: U256
    )-> FixedBytes<32> {
        // define sol types tuple
        type TxIdHashType = (SOLAddress, Uint<256>, SOLString, SOLBytes, Uint<256>);
        // set the tuple
        let tx_hash_data = (target, value, func, data, timestamp);
        // encode the tuple
        let tx_hash_data_encode_packed = TxIdHashType::abi_encode_packed(&tx_hash_data);
        // hash the encoded data
        keccak(tx_hash_data_encode_packed).into()
    }

    // Packed encode the data and hash it, the same result with the above one
    pub fn packed_encode_and_hash_2(
        &self,
        target: Address,
        value: U256,
        func: String,
        data: Bytes,
        timestamp: U256
    )-> FixedBytes<32> {
        // set the data to arrary and concat it directly
        let tx_hash_data_encode_packed = [&target.to_vec(), &value.to_be_bytes_vec(), func.as_bytes(), &data.to_vec(), &timestamp.to_be_bytes_vec()].concat();
        // hash the encoded data
        keccak(tx_hash_data_encode_packed).into()
    }


    // The func example: "transfer(address,uint256)"
    pub fn encode_with_signature(
        &self,
        func: String,
        address: Address,
        amount: U256
    ) -> Vec<u8> {
        type TransferType = (SOLAddress, Uint<256>);
        let tx_data = (address, amount);
        let data = TransferType::abi_encode_sequence(&tx_data);
        // Get function selector
        let hashed_function_selector: FixedBytes<32> = keccak(func.as_bytes().to_vec()).into();
        // Combine function selector and input data (use abi_packed way)
        let calldata = [&hashed_function_selector[..4], &data].concat();
        calldata
    }

    // The func example: "transfer(address,uint256)"
    pub fn encode_with_signature_and_hash(
        &self,
        func: String,
        address: Address,
        amount: U256
    ) -> FixedBytes<32> {
        type TransferType = (SOLAddress, Uint<256>);
        let tx_data = (address, amount);
        let data = TransferType::abi_encode_sequence(&tx_data);
        // Get function selector
        let hashed_function_selector: FixedBytes<32> = keccak(func.as_bytes().to_vec()).into();
        // Combine function selector and input data (use abi_packed way)
        let calldata = [&hashed_function_selector[..4], &data].concat();
        keccak(calldata).into()
    }
}
```

### Cargo.toml

```toml
[package]
name = "stylus-encode-hashing"
version = "0.1.0"
edition = "2021"

[dependencies]
alloy-primitives = "=0.7.6"
alloy-sol-types = "=0.7.6"
mini-alloc = "0.4.2"
stylus-sdk = "0.6.0"
hex = "0.4.3"
sha3 = "0.10.8"

[features]
export-abi = ["stylus-sdk/export-abi"]
debug = ["stylus-sdk/debug"]

[lib]
crate-type = ["lib", "cdylib"]

[profile.release]
codegen-units = 1
strip = true
lto = true
panic = "abort"
opt-level = "s"
```

---

## .mdx (basic_examples/hello_world.mdx)
---
title: 'Hello World • Stylus by Example'
description: 'This example shows how to use the `console!` macro from the Arbitrum Stylus Rust SDK to print output to the terminal for debugging.'
---

{/* Begin Content */}

# Hello World

Using the `console!` macro from the `stylus_sdk` allows you to print output to the terminal for debugging purposes. To view the output, you'll need to run a local Stylus dev node as described in the [Arbitrum docs](https://docs.arbitrum.io/stylus/how-tos/local-stylus-dev-node) and **_set the debug feature flag_** as shown in line 7 of the `Cargo.toml` file below.

The `console!` macro works similar to the built-in `println!` macro that comes with Rust.

### Examples

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
// Out: Stylus says: 'hello there!'
console!("hello there!");
// Out: Stylus says: 'format some arguments'
console!("format {} arguments", "some");

let local_variable = "Stylus";
// Out: Stylus says: 'Stylus is awesome!'
console!("{local_variable} is awesome!");
// Out: Stylus says: 'When will you try out Stylus?'
console!("When will you try out {}?", local_variable);
```

### src/main.rs

```rust
#![cfg_attr(not(feature = "export-abi"), no_main)]

extern crate alloc;


use stylus_sdk::{console, prelude::*, stylus_proc::entrypoint, ArbResult};

#[storage]
#[entrypoint]
pub struct Hello;


#[public]
impl Hello {
    fn user_main(_input: Vec<u8>) -> ArbResult {
        // Will print 'Stylus says: Hello Stylus!' on your local dev node
        // Be sure to add "debug" feature flag to your Cargo.toml file as
        // shown below.
        console!("Hello Stylus!");
        Ok(Vec::new())
    }
}
```

### Cargo.toml

```toml
[package]
name = "stylus_hello_world"
version = "0.1.7"
edition = "2021"
license = "MIT OR Apache-2.0"
keywords = ["arbitrum", "ethereum", "stylus", "alloy"]

[dependencies]
alloy-primitives = "=0.7.6"
alloy-sol-types = "=0.7.6"
mini-alloc = "0.4.2"
stylus-sdk = { version = "0.6.0", features = ["debug"] }
hex = "0.4.3"
sha3 = "0.10"

[dev-dependencies]
tokio = { version = "1.12.0", features = ["full"] }
ethers = "2.0"
eyre = "0.6.8"

[features]
export-abi = ["stylus-sdk/export-abi"]

[lib]
crate-type = ["lib", "cdylib"]

[profile.release]
codegen-units = 1
strip = true
lto = true
panic = "abort"
opt-level = "s"
```

---

## .mdx (basic_examples/inheritance.mdx)
---
title: 'Inheritance • Stylus by Example'
description: 'How to leverage inheritance using the Arbitrum Stylus Rust SDK.'
---

{/* Begin Content */}

# Inheritance

The Stylus Rust SDK replicates the composition pattern of Solidity. The `#[public]` macro provides the [Router](https://docs.rs/stylus-sdk/latest/stylus_sdk/abi/trait.Router.html) trait, which can be used to connect types via inheritance, via the `#[inherit]` macro.

**Please note:** Stylus doesn't support contract multi-inheritance yet.

Let's see an example:

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
#[public]
#[inherit(Erc20)]
impl Token {
    pub fn mint(&mut self, amount: U256) -> Result<(), Vec<u8>> {
        ...
    }
}

#[public]
impl Erc20 {
    pub fn balance_of() -> Result<U256> {
        ...
    }
}
```

In the above code, we can see how `Token` inherits from `Erc20`, meaning that it will inherit the public methods available in `Erc20`. If someone called the `Token` contract on the function `balanceOf`, the function `Erc20.balance_of()` would be executed.

Additionally, the inheriting type must implement the [Borrow](https://doc.rust-lang.org/core/borrow/trait.Borrow.html) trait for borrowing data from the inherited type. In the case above, `Token` should implement `Borrow<Erc20>`. For simplicity, `#[storage]` and `sol_storage!` provide a `#[borrow]` annotation that can be used instead of manually implementing the trait:

```rust
sol_storage! {
    #[entrypoint]
    pub struct Token {
        #[borrow]
        Erc20 erc20;
        ...
    }

    pub struct Erc20 {
        ...
    }
}
```

## Methods search order

A type can inherit multiple other types (as long as they use the `#[public]` macro). Since execution begins in the type that uses the `#[entrypoint]` macro, that type will be first checked when searching a specific method. If the method is not found in that type, the search will continue in the inherited types, in order of inheritance. If the method is not found in any of the inherited methods, the call will revert.

Let's see an example:

```rust
#[public]
#[inherit(B, C)]
impl A {
    pub fn foo() -> Result<(), Vec<u8>> {
        ...
    }
}

#[public]
impl B {
    pub fn bar() -> Result<(), Vec<u8>> {
        ...
    }
}

#[public]
impl C {
    pub fn bar() -> Result<(), Vec<u8>> {
        ...
    }

    pub fn baz() -> Result<(), Vec<u8>> {
        ...
    }
}
```

In the code above:

- calling `foo()` will search the method in `A`, find it, and execute `A.foo()`
- calling `bar()` will search the method in `A` first, then in `B`, find it, and execute `B.bar()`
- calling `baz()` will search the method in `A`, `B` and finally `C`, so it will execute `C.baz()`

Notice that `C.bar()` won't ever be reached, since the inheritance goes through `B` first, which has a method named `bar()` too.

Finally, since the inherited types can also inherit other types themselves, keep in mind that method resolution finds the first matching method by [Depth First Search](https://en.wikipedia.org/wiki/Depth-first_search).

## Overriding methods

Because methods are checked in the inherited order, if two types implement the same method, the one in the higher level in the hierarchy will override the one in the lower levels, which won’t be callable. This allows for patterns where the developer imports a crate implementing a standard, like ERC-20, and then adds or overrides just the methods they want to without modifying the imported ERC-20 type.

**Important warning**: The Stylus Rust SDK does not currently contain explicit `override` or `virtual` keywords for explicitly marking override functions. It is important, therefore, to carefully ensure that contracts are only overriding the functions.

Let's see an example:

```rust
#[public]
#[inherit(B, C)]
impl A {
    pub fn foo() -> Result<(), Vec<u8>> {
        ...
    }
}

#[public]
impl B {
    pub fn foo() -> Result<(), Vec<u8>> {
        ...
    }

    pub fn bar() -> Result<(), Vec<u8>> {
        ...
    }
}
```

In the example above, even though `B` has an implementation for `foo()`, calling `foo()` will execute `A.foo()` since the method is searched first in `A`.

## Learn more

- [`Arbitrum documentation`](https://docs.arbitrum.io/stylus/reference/rust-sdk-guide#inheritance-inherit-and-borrow)
- [`inheritance, #[inherit] and #[borrow]`](https://docs.rs/stylus-sdk/latest/stylus_sdk/prelude/attr.public.html#inheritance-inherit-and-borrow)
- [`Router trait`](https://docs.rs/stylus-sdk/latest/stylus_sdk/abi/trait.Router.html)
- [`Borrow trait`](https://doc.rust-lang.org/core/borrow/trait.Borrow.html)
- [`BorrowMut trait`](https://doc.rust-lang.org/core/borrow/trait.BorrowMut.html)

---

## .mdx (basic_examples/primitive_data_types.mdx)
---
title: 'Primitive Data Types • Stylus by Example'
description: 'Defines some of the basic primitives used in Arbitrum Stylus Rust smart contracts and how they map to compatible Solidity constructs.'
---

{/* Begin Content */}

# Primitive Data Types

The **Stylus SDK** makes use of the popular **Alloy** library (from the developers of **ethers-rs** and **Foundry**) to represent various native Solidity types as Rust types and to seamlessly convert between them when needed. These are needed since there are a number of custom types (like address) and large integers that are not natively supported in Rust.

In this section, we'll focus on the following types:

- `U256`
- `I256`
- `Address`
- `Boolean`
- `Bytes`

More in-depth documentation about the available methods and types in the Alloy library can be found in their docs. It also helps to cross-reference with Solidity docs if you don't already have a solid understanding of those types.

## Learn More

- [Alloy docs (v0.7.6)](https://docs.rs/alloy-primitives/0.7.6/alloy_primitives/index.html)
  - [`Address`](https://docs.rs/alloy-primitives/0.7.6/alloy_primitives/struct.Address.html)
  - [`Signed`](https://docs.rs/alloy-primitives/0.7.6/alloy_primitives/struct.Signed.html)
  - [`Uint`](https://docs.rs/ruint/1.10.1/ruint/struct.Uint.html)
- [Stylus Rust SDK](https://docs.rs/stylus-sdk/latest/stylus_sdk/index.html)
  - [`Bytes`](https://docs.rs/stylus-sdk/latest/stylus_sdk/abi/struct.Bytes.html)
- [Solidity docs (v0.8.19)](https://docs.soliditylang.org/en/v0.8.19/types.html)

## Integers

Alloy defines a set of convenient Rust types to represent the typically sized integers used in Solidity. The type `U256` represents a 256-bit _unsigned_ integer, meaning it cannot be negative. The range for a `U256` number is 0 to 2^256 - 1.

Negative numbers are allowed for I types, such as `I256`. These represent signed integers.

- `U256` maps to `uint256` ... `I256` maps to `int256`
- `U128` maps to `uint128` ... `I128` maps to `int128`
- ...
- `U8` maps to `uint8` ... `I8` maps to `int8`

### Integer Usage

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
// Unsigned
let eight_bit: U8 = U8::from(1);
let two_fifty_six_bit: U256 = U256::from(0xff_u64);

// Out: Stylus says: '8-bit: 1 | 256-bit: 255'
console!("8-bit: {} | 256-bit: {}", eight_bit, two_fifty_six_bit);

// Signed
let eight_bit: I8 = I8::unchecked_from(-1);
let two_fifty_six_bit: I256 = I256::unchecked_from(0xff_u64);

// Out: Stylus says: '8-bit: -1 | 256-bit: 255'
console!("8-bit: {} | 256-bit: {}", eight_bit, two_fifty_six_bit);
```

### Expanded Integer Usage

```rust
// Use `try_from` if you're not sure it'll fit
let a = I256::try_from(20003000).unwrap();
// Or parse from a string
let b = "100".parse::<I256>().unwrap();
// With hex characters
let c = "-0x138f".parse::<I256>().unwrap();
// Underscores are ignored
let d = "1_000_000".parse::<I256>().unwrap();

// Math works great
let e = a * b + c - d;
// Out: Stylus says: '20003000 * 100 + -5007 - 1000000 = 1999294993'
console!("{} * {} + {} - {} = {}", a, b, c, d, e);

// Useful constants
let f = I256::MAX;
let g = I256::MIN;
let h = I256::ZERO;
let i = I256::MINUS_ONE;

// Stylus says: '5789...9967, -5789...9968, 0, -1'
console!("{f}, {g}, {h}, {i}");
// As hex: Stylus says: '0x7fff...ffff, 0x8000...0000, 0x0, 0xffff...ffff'
console!("{:#x}, {:#x}, {:#x}, {:#x}", f, g, h, i);
```

## Address

Ethereum addresses are 20 bytes in length, or 160 bits. Alloy provides a number of helper utilities for converting to addresses from strings, bytes, numbers, and addresses.

### Address Usage

```rust
// From a 20 byte slice, all 1s
let addr1 = Address::from([0x11; 20]);
// Out: Stylus says: '0x1111111111111111111111111111111111111111'
console!("{addr1}");

// Use the address! macro to parse a string as a checksummed address
let addr2 = address!("d8da6bf26964af9d7eed9e03e53415d37aa96045");
// Out: Stylus says: '0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045'
console!("{addr2}");

// Format compressed addresses for output
// Out: Stylus says: '0xd8dA…6045'
console!("{addr2:#}");
```

## Boolean

Use native Rust primitives where it makes sense and where no equivalent Alloy primitive exists.

### Boolean Usage

```rust
let frightened: bool = true;
// Out: Stylus says: 'Boo! Did I scare you?'
console!("Boo! Did I scare you?");

let response = match frightened {
    true => "Yes!".to_string(),
    false => "No!".to_string(),
};

// Out: Stylus says: 'Yes!'
console!("{response}");
```

## Bytes

The Stylus SDK provides this wrapper type around `Vec<u8>` to represent a `bytes` value in Solidity.

```rust
let vec = vec![108, 27, 56, 87];
let b = Bytes::from(vec);
// Out: Stylus says: '0x6c1b3857'
console!(String::from_utf8_lossy(b.as_slice()));

let b = Bytes::from(b"Hello!".to_vec());
// Out: Stylus says: 'Hello!'
console!(String::from_utf8_lossy(b.as_slice()));
```

Note: Return the `Bytes` type on your Rust function if you want to return the ABI `bytes memory` type.

## Boilerplate

### src/lib.rs

```rust
#![cfg_attr(not(any(feature = "export-abi", test)), no_main)]
extern crate alloc;
use alloc::{string::ToString, vec::Vec};

use stylus_sdk::{
    alloy_primitives::{address, Address, I256, I8, U256, U8}, console, prelude::*, ArbResult
};

#[storage]
#[entrypoint]
pub struct Data {

}


#[public]
impl Data {
fn user_main(_input: Vec<u8>) -> ArbResult {
    // Use native Rust primitives where they make sense
    // and where no equivalent Alloy primitive exists
    let frightened: bool = true;
    // Out: Stylus says: 'Boo! Did I scare you?'
    console!("Boo! Did I scare you?");

    let _response = match frightened {
        true => "Yes!".to_string(),
        false => "No!".to_string(),
    };

    // Out: Stylus says: 'Yes!'
    console!("{_response}");

    // U256 stands for a 256-bit *unsigned* integer, meaning it cannot be
    // negative. The range for a U256 number is 0 to 2^256 - 1. Alloy provides
    // a set of unsigned integer types to represent the various sizes available
    // in the EVM.
    //    U256 maps to uint256
    //    U128 maps to uint128
    //    ...
    //    U8 maps to uint8
    let _eight_bit: U8 = U8::from(1);
    let _two_fifty_six_bit: U256 = U256::from(0xff_u64);

    // Out: Stylus says: '8-bit: 1 | 256-bit: 255'
    console!("8-bit: {} | 256-bit: {}", _eight_bit, _two_fifty_six_bit);

    // Negative numbers are allowed for I types. These represent signed integers.
    //    I256 maps to int256
    //    I128 maps to int128
    //    ...
    //    I8 maps to int8
    let _eight_bit: I8 = I8::unchecked_from(-1);
    let _two_fifty_six_bit: I256 = I256::unchecked_from(0xff_u64);

    // Out: Stylus says: '8-bit: -1 | 256-bit: 255'
    console!("8-bit: {} | 256-bit: {}", _eight_bit, _two_fifty_six_bit);

    // Additional usage of integers

    // Use `try_from` if you're not sure it'll fit
    let a = I256::try_from(20003000).unwrap();
    // Or parse from a string
    let b = "100".parse::<I256>().unwrap();
    // With hex characters
    let c = "-0x138f".parse::<I256>().unwrap();
    // Underscores are ignored
    let d = "1_000_000".parse::<I256>().unwrap();

    // Math works great
    let _e = a * b + c - d;
    // Out: Stylus says: '20003000 * 100 + -5007 - 1000000 = 1999294993'
    console!("{} * {} + {} - {} = {}", a, b, c, d, _e);

    // Useful constants
    let _f = I256::MAX;
    let _g = I256::MIN;
    let _h = I256::ZERO;
    let _i = I256::MINUS_ONE;

    // Stylus says: '5789...9967, -5789...9968, 0, -1'
    console!("{_f}, {_g}, {_h}, {_i}");
    // As hex: Stylus says: '0x7fff...ffff, 0x8000...0000, 0x0, 0xffff...ffff'
    console!("{:#x}, {:#x}, {:#x}, {:#x}", _f, _g, _h, _i);

    // Ethereum addresses are 20 bytes in length, or 160 bits. Alloy provides a number of helper utilities for converting to addresses from strings, bytes, numbers, and addresses

    // From a 20 byte slice, all 1s
    let _addr1 = Address::from([0x11; 20]);
    // Out: Stylus says: '0x1111111111111111111111111111111111111111'
    console!("{_addr1}");

    // Use the address! macro to parse a string as a checksummed address
    let _addr2 = address!("d8da6bf26964af9d7eed9e03e53415d37aa96045");
    // Out: Stylus says: '0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045'
    console!("{_addr2}");

    // Format compressed addresses for output
    // Out: Stylus says: '0xd8dA…6045'
    console!("{_addr2:#}");

    Ok(Vec::new())
}
}
```

### Cargo.toml

```toml
[package]
name = "stylus_data_example"
version = "0.1.7"
edition = "2021"
license = "MIT OR Apache-2.0"
keywords = ["arbitrum", "ethereum", "stylus", "alloy"]

[dependencies]
alloy-primitives = "=0.7.6"
alloy-sol-types = "=0.7.6"
mini-alloc = "0.4.2"
stylus-sdk = "0.6.0"
hex = "0.4.3"

[dev-dependencies]
tokio = { version = "1.12.0", features = ["full"] }
ethers = "2.0"
eyre = "0.6.8"

[features]
export-abi = ["stylus-sdk/export-abi"]

[lib]
crate-type = ["lib", "cdylib"]

[profile.release]
codegen-units = 1
strip = true
lto = true
panic = "abort"
opt-level = "s"

```

---

## .mdx (basic_examples/sending_ether.mdx)
---
title: 'Sending Ether • Stylus by Example'
description: "How to send Ether in your Rust smart contracts using Arbitrum's Stylus SDK"
---

{/* Begin Content */}

# Sending Ether

We have three main ways to send Ether in Rust Stylus: using the `transfer_eth` method, using low level `call` method, and sending value while calling an external contract.

It's important to note that the `transfer_eth` method in Rust Stylus invokes the recipient contract, which may subsequently call other contracts. All the gas is supplied to the recipient, which it may burn. Conversely, the transfer method in Solidity is capped at 2300 gas. In Rust Stylus, you can cap the gas by using the low-level call method with a specified gas. An example of this is provided in the code on bottom of the page.

These two methods are exactly equivalent under the hood:

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
transfer_eth(recipient, value)?;

call(Call::new_in(self).value(value), recipient, &[])?;
```

## Where to Send Ether

1. **Externally Owned Account (EOA) Addresses**: Directly send Ether to an EOA address.

2. **Solidity Smart Contracts with Receive Function (No Calldata)**: Send Ether to a Solidity smart contract that has a `receive` function without providing any calldata.

3. **Solidity Smart Contracts with Fallback Function (With Calldata)**: Send Ether to a Solidity smart contract that has a `fallback` function by providing the necessary calldata.

4. **Smart Contracts with Payable Methods (both Solidity and Stylus)**: Send Ether to smart contracts that have defined payable methods. Payable methods are identified by the `payable` modifier in Solidity, and the `#[payable]` macro in Rust.

Below you can find examples for each of these methods and how to define them in a Rust Stylus smart contract using the Stylus SDK:

### `src/lib.rs`

```rust
// Only run this as a WASM if the export-abi feature is not set.
#![cfg_attr(not(any(feature = "export-abi", test)), no_main)]
extern crate alloc;

use alloy_primitives::Address;
use stylus_sdk::{
    abi::Bytes,
    call::{call, transfer_eth, Call},
    msg::{self},
    prelude::*,
};

sol_interface! {
    interface ITarget {
        function receiveEther() external payable;
    }
}

#[storage]
#[entrypoint]
pub struct SendEther {}

#[public]
impl SendEther {
    // Transfer Ether using the transfer_eth method
    // This can be used to send Ether to an EOA or a Solidity smart contract that has a receive() function implemented
    #[payable]
    pub fn send_via_transfer(to: Address) -> Result<(), Vec<u8>> {
        transfer_eth(to, msg::value())?;
        Ok(())
    }

    // Transfer Ether using a low-level call
    // This can be used to send Ether to an EOA or a Solidity smart contract that has a receive() function implemented
    #[payable]
    pub fn send_via_call(&mut self, to: Address) -> Result<(), Vec<u8>> {
        call(Call::new_in(self).value(msg::value()), to, &[])?;
        Ok(())
    }

    // Transfer Ether using a low-level call with a specified gas limit
    // This can be used to send Ether to an EOA or a Solidity smart contract that has a receive() function implemented
    #[payable]
    pub fn send_via_call_gas_limit(&mut self, to: Address, gas_amount: u64) -> Result<(), Vec<u8>> {
        call(
            Call::new_in(self).value(msg::value()).gas(gas_amount),
            to,
            &[],
        )?;
        Ok(())
    }

    // Transfer Ether using a low-level call with calldata
    // This can be used to call a Solidity smart contract's fallback function and send Ether along with calldata
    #[payable]
    pub fn send_via_call_with_call_data(
        &mut self,
        to: Address,
        data: Bytes,
    ) -> Result<(), Vec<u8>> {
        call(Call::new_in(self).value(msg::value()), to, data.as_slice())?;
        Ok(())
    }

    // Transfer Ether to another smart contract via a payable method on the target contract
    // The target contract can be either a Solidity smart contract or a Stylus contract that has a receiveEther function, which is a payable function
    #[payable]
    pub fn send_to_stylus_contract(&mut self, to: Address) -> Result<(), Vec<u8>> {
        let target = ITarget::new(to);
        let config = Call::new_in(self).value(msg::value());
        target.receive_ether(config)?;
        Ok(())
    }
}
```

### `Cargo.toml`

```toml
[package]
name = "stylus_sending_ether_example"
version = "0.1.7"
edition = "2021"
license = "MIT OR Apache-2.0"
keywords = ["arbitrum", "ethereum", "stylus", "alloy"]

[dependencies]
alloy-primitives = "=0.7.6"
alloy-sol-types = "=0.7.6"
mini-alloc = "0.4.2"
stylus-sdk = "0.6.0"
hex = "0.4.3"

[dev-dependencies]
tokio = { version = "1.12.0", features = ["full"] }
ethers = "2.0"
eyre = "0.6.8"

[features]
export-abi = ["stylus-sdk/export-abi"]

[lib]
crate-type = ["lib", "cdylib"]

[profile.release]
codegen-units = 1
strip = true
lto = true
panic = "abort"
opt-level = "s"

```

---

## .mdx (basic_examples/variables.mdx)
---
title: 'Variables • Stylus by Example'
description: 'An explanation of the types of variables available as part of the  Arbitrum Stylus Rust SDK and how it differs from Solidity.'
---

{/* Begin Content */}

# Variables

In Solidity, there are 3 types of variables: local, state, and global. Local variables are not stored on the blockchain, while state variables are (and incur a much higher cost as a result). This is true of Arbitrum Stylus Rust smart contracts as well, although how they're defined is quite different.

In Rust, **local variables** are just ordinary variables you assign with `let` or `let mut` statements. Local variables are far cheaper than state variables, even on the EVM, however, Stylus local variables are more than 100x cheaper to allocate in memory than their Solidity equivalents.

Unlike Solidity, Rust was not built inherently with the blockchain in mind. It is a general purpose programming language. We therefore define specific _storage_ types to explicitly denote values intended to be stored permanently as part of the contract's state. **State variables** cost the same to store as their Solidity equivalents.

**Global variables** in Solidity, such as `msg.sender` and `block.timestamp`, are available as function calls pulled in from the `stylus_sdk` with their Rust equivalents being `msg::sender()` and `block::timestamp()`, respectively. These variables provide information about the blockchain or the active transaction.

## Learn more

- [Rust Docs - Variables and Mutability](https://doc.rust-lang.org/book/ch03-01-variables-and-mutability.html)
- [Stylus SDK Rust Docs - Storage](https://docs.rs/stylus-sdk/latest/stylus_sdk/storage/index.html)
- [Stylus SDK Guide - Storage](https://docs.arbitrum.io/stylus/reference/rust-sdk-guide#storage)
- [Solidity docs - state variables](https://docs.soliditylang.org/en/v0.8.19/structure-of-a-contract.html#state-variables)
- [Solidity docs - global variables](https://docs.soliditylang.org/en/v0.8.19/cheatsheet.html#global-variables)

### src/lib.rs

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
// Only run this as a WASM if the export-abi feature is not set.
#![cfg_attr(not(any(feature = "export-abi", test)), no_main)]
extern crate alloc;

use stylus_sdk::alloy_primitives::{U16, U256};
use stylus_sdk::prelude::*;
use stylus_sdk::storage::{StorageAddress, StorageBool, StorageU256};
use stylus_sdk::{block, console, msg};

#[storage]
#[entrypoint]
pub struct Contract {
    initialized: StorageBool,
    owner: StorageAddress,
    max_supply: StorageU256,
}

#[public]
impl Contract {
    // State variables are initialized in an `init` function.
    pub fn init(&mut self) -> Result<(), Vec<u8>> {
        // We check if contract has been initialized before.
        // We return if so, we initialize if not.
        let initialized = self.initialized.get();
        if initialized {
            return Ok(());
        }
        self.initialized.set(true);

        // We set the contract owner to the caller,
        // which we get from the global msg module
        self.owner.set(msg::sender());
        self.max_supply.set(U256::from(10_000));

        Ok(())
    }

    pub fn do_something() -> Result<(), Vec<u8>> {
        // Local variables are not saved to the blockchain
        // 16-bit Rust integer
        let _i = 456_u16;
        // 16-bit int inferred from U16 Alloy primitive
        let _j = U16::from(123);

        // Here are some global variables
        let _timestamp = block::timestamp();
        let _amount = msg::value();

        console!("Local variables: {_i}, {_j}");
        console!("Global variables: {_timestamp}, {_amount}");

        Ok(())
    }
}

```

### Cargo.toml

```toml
[package]
name = "stylus_variable_example"
version = "0.1.7"
edition = "2021"
license = "MIT OR Apache-2.0"
keywords = ["arbitrum", "ethereum", "stylus", "alloy"]

[dependencies]
alloy-primitives = "=0.7.6"
alloy-sol-types = "=0.7.6"
mini-alloc = "0.4.2"
stylus-sdk = "0.6.0"
hex = "0.4.3"

[dev-dependencies]
tokio = { version = "1.12.0", features = ["full"] }
ethers = "2.0"
eyre = "0.6.8"

[features]
export-abi = ["stylus-sdk/export-abi"]

[lib]
crate-type = ["lib", "cdylib"]

[profile.release]
codegen-units = 1
strip = true
lto = true
panic = "abort"
opt-level = "s"

```

---

## .mdx (basic_examples/vm_affordances.mdx)
---
title: 'VM affordances • Stylus by Example'
description: 'How to access VM affordances using the Arbitrum Stylus Rust SDK.'
---

{/* Begin Content */}

# VM affordances

The Stylus Rust SDK contains several modules for interacting with the Virtual Machine (VM), which can be imported from `stylus_sdk`.

Let's see an example:

import NotForProductionBannerPartial from '../../partials/_not-for-production-banner-partial.mdx';

<NotForProductionBannerPartial />

```rust
use stylus_sdk::{msg};

let callvalue = msg::value();
```

This page lists the modules that are available, as well as the methods within those modules.

## block

Allows you to inspect the current block:

- `basefee`: gets the basefee of the current block
- `chainid`: gets the unique chain identifier of the Arbitrum chain
- `coinbase`: gets the coinbase of the current block, which on Arbitrum chains is the L1 batch poster's address
- `gas_limit`: gets the gas limit of the current block
- `number`: gets a bounded estimate of the L1 block number at which the sequencer sequenced the transaction. See [Block gas limit, numbers and time](https://docs.arbitrum.io/build-decentralized-apps/arbitrum-vs-ethereum/block-numbers-and-time) for more information on how this value is determined
- `timestamp`: gets a bounded estimate of the Unix timestamp at which the sequencer sequenced the transaction. See [Block gas limit, numbers and time](https://docs.arbitrum.io/build-decentralized-apps/arbitrum-vs-ethereum/block-numbers-and-time) for more information on how this value is determined

```rust
use stylus_sdk::{block};

let basefee = block::basefee();
let chainid = block::chainid();
let coinbase = block::coinbase();
let gas_limit = block::gas_limit();
let number = block::number();
let timestamp = block::timestamp();
```

## contract

Allows you to inspect the contract itself:

- `address`: gets the address of the current program
- `args`: reads the invocation's calldata. The entrypoint macro uses this under the hood
- `balance`: gets the balance of the current program
- `output`: writes the contract's return data. The entrypoint macro uses this under the hood
- `read_return_data`: copies the bytes of the last EVM call or deployment return result. Note: this function does not revert if out of bounds, but rather will copy the overlapping portion
- `return_data_len`: returns the length of the last EVM call or deployment return result, or 0 if neither have happened during the program's execution

```rust
use stylus_sdk::{contract};

let address = contract::address();
contract::args();
let balance = contract::balance();
contract::output();
contract::read_return_data();
contract::return_data_len();
```

## crypto

Allows you to access VM-accelerated cryptographic functions:

- `keccak`: efficiently computes the [keccak256](https://en.wikipedia.org/wiki/SHA-3) hash of the given preimage

```rust
use stylus_sdk::{crypto};
use stylus_sdk::alloy_primitives::address;

let preimage = address!("361594F5429D23ECE0A88E4fBE529E1c49D524d8");
let hash = crypto::keccak(&preimage);
```

## evm

Allows you to access affordances for the Ethereum Virtual Machine:

- `gas_left`: gets the amount of gas remaining. See [Ink and Gas](https://docs.arbitrum.io/stylus/concepts/stylus-gas) for more information on Stylus's compute pricing
- `ink_left`: gets the amount of ink remaining. See [Ink and Gas](https://docs.arbitrum.io/stylus/concepts/stylus-gas) for more information on Stylus's compute pricing
- `log`: emits a typed alloy log
- `pay_for_memory_grow`: this function exists to force the compiler to import this symbol. Calling it will unproductively consume gas
- `raw_log`: emits an EVM log from its raw topics and data. Most users should prefer the alloy-typed [raw_log](https://docs.rs/stylus-sdk/latest/stylus_sdk/evm/fn.raw_log.html)

```rust
use stylus_sdk::{evm};

let gas_left = evm::gas_left();
let ink_left = evm::ink_left();
evm::log(...);
evm::pay_for_memory_grow();
evm::raw_log(...);
```

Here's an example of how to emit a Transfer log:

```rust
sol! {
    event Transfer(address indexed from, address indexed to, uint256 value);
    event Approval(address indexed owner, address indexed spender, uint256 value);
}

fn foo() {
   ...
   evm::log(Transfer {
      from: Address::ZERO,
      to: address,
      value,
   });
}
```

## msg

Allows you to inspect the current call

- `reentrant`: whether the current call is reentrant
- `sender`: gets the address of the account that called the program. For normal L2-to-L2 transactions the semantics are equivalent to that of the EVM's [CALLER](https://www.evm.codes/#33) opcode, including in cases arising from [DELEGATE_CALL](https://www.evm.codes/#f4)
- `value`: gets the ETH value in wei sent to the program

```rust
use stylus_sdk::{msg};

let reentrant = msg::reentrant();
let sender = msg::sender();
let value = msg::value();
```

## tx

Allows you to inspect the current transaction

- `gas_price`: gets the gas price in wei per gas, which on Arbitrum chains equals the basefee
- `gas_to_ink`: converts evm gas to ink. See [Ink and Gas](https://docs.arbitrum.io/stylus/concepts/stylus-gas) for more information on Stylus's compute-pricing model
- `ink_price`: gets the price of ink in evm gas basis points. See [Ink and Gas](https://docs.arbitrum.io/stylus/concepts/stylus-gas) for more information on Stylus's compute-pricing model
- `ink_to_gas`: converts ink to evm gas. See [Ink and Gas](https://docs.arbitrum.io/stylus/concepts/stylus-gas) for more information on Stylus's compute-pricing model
- `origin`: gets the top-level sender of the transaction. The semantics are equivalent to that of the EVM's [ORIGIN](https://www.evm.codes/#32) opcode

```rust
use stylus_sdk::{tx};

let gas_price = tx::gas_price();
let gas_to_ink = tx::gas_to_ink();
let ink_price = tx::ink_price();
let ink_to_gas = tx::ink_to_gas();
let origin = tx::origin();
```

## Learn More

- [`Arbitrum documentation`](https://docs.arbitrum.io/stylus/reference/rust-sdk-guide#evm-affordances)
- [`Stylus SDK modules`](https://docs.rs/stylus-sdk/latest/stylus_sdk/index.html#modules)

---

# run-arbitrum-node Folder

## .mdx (01-overview.mdx)
---
title: 'Arbitrum nodes: an overview'
description: Learn more about what type of ARb node one needs to run.
author-objective: Build a quickstart that helps readers understand why they might want to run a specific type of an Arbitrum node.
reader-audience: Moderately-technical readers who are familiar with command lines, but not Ethereum / Arbitrum infrastructure
reader-task: Learn about the different types of Arbitrum nodes and understand the benefits and trade-offs of each type.
content_type: overview
---

import PublicPreviewBannerPartial from '../partials/_public-preview-banner-partial.mdx';
import { VanillaAdmonition } from '@site/src/components/VanillaAdmonition/';

<VanillaAdmonition type="note">

There is no protocol-level incentive to run an Arbitum full node. If you’re interested in accessing an Arbitrum chain but don’t want to set up a node locally, see our [RPC endpoints and providers](/build-decentralized-apps/reference/01-node-providers.mdx) to get RPC access to fully managed nodes hosted by a third-party provider.

</VanillaAdmonition>

:::caution API security disclaimer

When exposing API endpoints to the Internet or any untrusted/hostile network, the following risks may arise:

- **Increased risk of crashes due to Out-of-Memory (OOM)**:
  Exposing endpoints increases the risk of OOM crashes.
- **Increased risk of not keeping up with chain progression**:
  Resource starvation (IO or CPU) may occur, leading to an inability to keep up with chain progression.

We strongly advise against exposing API endpoints publicly. Users considering such exposure should exercise caution and implement the right measures to enhance resilience.

:::

To be able to _interact with_ or _build applications on_ any of the Arbitrum chains, you need access to the corresponding Arbitrum node. Options are:

- You can use [ third party node providers ](/build-decentralized-apps/reference/01-node-providers.mdx) to get RPC access to fully-managed nodes
- You can run your own Arbitrum node, especially if you want always to know the state of the Arbitrum chain

The rest of this series focuses on the second approach: running your own Arbitrum node.

:::

To be able to _interact with_ or _build applications on_ any of the Arbitrum chains, you need access to the corresponding Arbitrum node. Options are:

When interacting with the Arbitrum network, users have the option to run either a full node or an archive node. There are distinct advantages to running an Arbitrum full node. In this quick start, we will explore the reasons why a user may prefer to run a full node instead of an archive node. By understanding the benefits and trade-offs of each node type, users can make an informed decision based on their specific requirements and objectives.

### Considerations for running an Arbitrum full node

- **Transaction validation and security**: Running a full node allows users to independently validate transactions and verify the state of the Arbitrum blockchain. Users can have complete confidence in the authenticity and integrity of the transactions they interact with.
- **Reduced trust requirements**: By running a full node, users can interact with the Arbitrum network without relying on third-party services or infrastructure. This independence reduces the need to trust external entities and mitigates the risk of potential centralized failures or vulnerabilities.
- **Lower resource requirements**: Compared to archive nodes, full nodes generally require fewer resources such as storage and computational power. These requirements make it more accessible to users with limited hardware capabilities or those operating in resource-constrained environments.

For detailed instructions, read [how to run an Arbitrum full node](/run-arbitrum-node/02-run-full-node.mdx).

### Considerations for running an Arbitrum archive node

While full nodes offer numerous advantages, there are situations where running an archive node may be more appropriate. Archive nodes store the complete history of the Arbitrum network, making them suitable for users who require access to extensive historical data or advanced analytical purposes. However, it's important to note that archive nodes are more resource-intensive, requiring significant storage capacity and computational power.

For detailed instructions, read [how to run an Arbitrum archive node](/run-arbitrum-node/more-types/01-run-archive-node.mdx).

### Considerations for running an Arbitrum classic node

The significance of running an Arbitrum classic node is mainly applicable to individuals with specific needs for an archive node and access to classic-related commands.

For detailed instructions, read [how to run an Arbitrum classic node](/run-arbitrum-node/more-types/03-run-classic-node.mdx).

### Considerations for running a feed relay

If you are running a single node, there is no requirement to set up a feed relay. However, if you have multiple nodes, it is highly recommended to have a single feed relay per data center. This setup offers several advantages, including reducing ingress fees and enhancing network stability.

Soon, feed endpoints will mandate compression using a custom dictionary. Therefore, if you plan to connect to a feed using anything other than a standard node, it is strongly advised to run a local feed relay. This local feed relay will ensure that you have access to an uncompressed feed by default, maintaining optimal performance and compatibility.

For detailed instructions, read [how to run an Arbitrum feed relay](/run-arbitrum-node/run-feed-relay.mdx).

---

## .mdx (02-run-full-node.mdx)
---
title: 'How to run a full node for an Arbitrum or Arbitrum chain'
description: Learn how to run an Arbitrum node on your local machine
sidebar_position: 1
content_type: how-to
---

:::info

If you’re interested in accessing an Arbitrum chain but don’t want to set up your own node, see our [Node Providers](/build-decentralized-apps/reference/01-node-providers.mdx) to get RPC access to fully managed nodes hosted by a third-party provider.

:::

This how-to provides step-by-step instructions for running a full node for Arbitrum on your local machine.

## Prerequisites

In addition to the hardware requirements, the following prerequisites will be necessary when initially setting up your node. It is essential not to skip over these items. You would benefit by copying and pasting them into a notepad or text editor, as you will need to combine them with other commands and configuration/parameter options when you initially run your Arbitrum node.

### Minimum hardware configuration

The following are the minimum hardware requirements to set up a <a data-quicklook-from="arbitrum-nitro">Nitro</a> full node (not archival):

| Resource     | Recommended                                   |
| :----------- | :-------------------------------------------- |
| RAM          | 16 GB                                         |
| CPU          | 4 core CPU (for AWS, a `t3 xLarge` instance)  |
| Storage type | NVMe SSD drives are recommended               |
| Storage size | Depends on the chain and its traffic overtime |

Please note that:

- The minimum requirements for RAM and CPU listed here are recommended for nodes that handle a limited number of RPC requests. For nodes that need to process multiple simultaneous requests, both the RAM size and the number of CPU cores should be increased to accommodate higher levels of traffic.
- Single core performance is important. If the node is falling behind and a single core is 100% busy, the recommendation is to upgrade to a faster processor
- The minimum storage requirements will change over time as the chain grows. Using more than the minimum requirements to run a robust full node is recommended.

### Recommended Nitro version

:::caution

Although there are beta and release candidate versions of the Arbitrum Nitro software, use only the release version when running your node. Running beta or RC versions is not supported and might lead to unexpected behaviors and/or database corruption.

:::

Latest [Docker image](https://hub.docker.com/r/offchainlabs/nitro-node/tags): <code>@@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@</code>

### Database snapshots

::::info Snapshots availability

Database snapshots are available and located in the [snapshot explorer](https://snapshot-explorer.arbitrum.io/) for Arbitrum One, Arbitrum Nova, and Arbitrum Sepolia. Database snapshots for other Arbitrum chains may be available at the discretion of the team running the chain. Please get in touch with them if you're interested in using a database snapshot for their chains.

::::

Supplying a database snapshot when starting your node for the first time is required for Arbitrum One (to provide information from the Classic era) but is optional for other chains. Supplying a database snapshot on the first run will provide the state and data for that chain up to a specific block, allowing the node to sync faster to the head of the chain.

We provide a summary of the available parameters here, but we recommend reading the [complete guide](/run-arbitrum-node/nitro/03-nitro-database-snapshots.mdx) if you plan to use snapshots.

- Use the parameter `--init.latest <snapshot type>` (accepted values: `archive`, `pruned`, `genesis`) to instruct your node to download the corresponding snapshot from the configured URL
- Optionally, use the parameter `--init.latest-base` to set the base URL when searching for the latest snapshot
- Note that these parameters get ignored if a database already exists
- When running more than one node, it's easier to manually download the different parts of the snapshot, join them into a single archive, and host it locally for your nodes. Please see [Downloading the snapshot manually](/run-arbitrum-node/nitro/03-nitro-database-snapshots.mdx#downloading-the-snapshot-manually) for instructions on how to do that.

### Required parameters

The following list contains all the parameters needed to configure your node. Select the appropriate option depending on the chain you want to run your node for.

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import DAOChainsParameters from './partials/run-full-node/_dao-chains-parameters.mdx';
import OrbitChainsParameters from './partials/run-full-node/_orbit-chains-parameters.mdx';
import DAOChainsExample from './partials/run-full-node/_dao-chains-example.mdx';
import OrbitChainsExample from './partials/run-full-node/_orbit-chains-example.mdx';

<div className="dynamic-content-tabs">
  <Tabs className="tabgroup" defaultValue={null}>
    <TabItem value="dao-governed-chains" label="Arbitrum One, Nova, Sepolia">
      <DAOChainsParameters />
    </TabItem>
    <TabItem value="orbit-chains" label="Arbitrum chains (Orbit)">
      <OrbitChainsParameters />
    </TabItem>
  </Tabs>
</div>

## Putting it into practice: run a node

:::warning Caution

If you are running more than on node, you should [run a feed relay](/run-arbitrum-node/run-feed-relay.mdx).

:::

- To ensure the database persists across restarts, mount an external volume when running the Docker image. Use the mount point `/home/user/.arbitrum` within the Docker image.
- Here is an example of how to run `nitro-node`:

<div className="dynamic-content-tabs">
  <Tabs className="tabgroup" defaultValue={null}>
    <TabItem value="dao-governed-chains-example" label="Arbitrum One, Nova, Sepolia">
      <DAOChainsExample />
    </TabItem>
    <TabItem value="orbit-chains-example" label="Arbitrum chains (Orbit)">
      <OrbitChainsExample />
    </TabItem>
  </Tabs>
</div>

- Note that it is important that `/some/local/dir/arbitrum` already exists; otherwise, the directory might be created with `root` as owner, and the Docker container won't be able to write to it.
- Note that if you are running a node for the parent chain (e.g., Ethereum for Arbitrum One or Nova) on localhost, you may need to add `--network host` right after `docker run` to use Docker host-based networking
- When shutting down the Docker image, it is important to allow a graceful shutdown to save the current state to disk. Here is an example of how to do a graceful shutdown of all Docker images currently running

  ```shell
  docker stop --time=1800 $(docker ps -aq)
  ```

### Important ports

| Protocol          | Default port |
| :---------------- | :----------- |
| `RPC`/`http`      | `8547`       |
| `RPC`/`websocket` | `8548`       |
| `Sequencer Feed`  | `9642`       |

- Please note: the `RPC`/`websocket` protocol requires some ports to be enabled, you can use the following flags:
  - `--ws.port=8548`
  - `--ws.addr=0.0.0.0`
  - `--ws.origins=\*`

### Note on permissions

- The Docker image is configured to run as non-root UID 1000. This configuration means if you are running in Linux or OSX and you are getting permission errors when trying to run the Docker image, run this command to allow all users to update the persistent folders:

  ```shell
  mkdir /data/arbitrum
  chmod -fR 777 /data/arbitrum
  ```

### Watchtower mode

- By default, the full node runs in Watchtower mode, meaning that it watches the onchain assertions and, if it disagrees with them, logs an error containing the string `found incorrect assertion in watchtower mode`. For a BoLD-enabled chain like Arbitrum One or Arbitrum Nova if you are running Nitro before v3.6.0, the `--node.bold.enable=true` flag should be set to ensure your node can monitor for onchain assertions properly.
- Setting this flag is not required as your node will continue to operate correctly, validate the Arbitrum One/Nova chain, and serve RPC requests as usual, regardless of this flag.
- Note that watchtower mode adds a small amount of execution and memory overhead. You can deactivate this mode using the parameter `--node.staker.enable=false`.

### Pruning

- Pruning a full node refers to removing older, unnecessary data from the local copy of the blockchain that the node maintains, thereby saving disk space and slightly improving the node's efficiency. Pruning will remove all states from blocks older than the latest 128.
- You can activate pruning by using the parameter `--init.prune` and using "full" or "validator" as the value (depending on the type of node you are running). Note that this process occurs when the node starts and will not serve RPC requests during pruning.

### Transaction prechecker

- Enabling the transaction prechecker will add extra checks before your node forwards `eth_sendRawTransaction` to the Sequencer endpoint.
- Below, we list the flags to set up the prechecker:

import OptionalTxPrecheckerParameters from './partials/run-full-node/_tx-prechecker-parameters.mdx';

<OptionalTxPrecheckerParameters />

### Optional parameters

Below, we listed the most commonly used parameters when running a node. You can also use the flag `--help` for a comprehensive list of the available parameters.

import OptionalParameters from './partials/run-full-node/_optional-parameters.mdx';

<OptionalParameters />

---

## .mdx (03-run-local-full-chain-simulation.mdx)
---
title: 'How to run a local full chain simulation'
description: This page provides instructions for setting up a complete local development environment for testing Arbitrum contracts in a fully simulated environment.
author: jose-franco
sme: jose-franco
sidebar_position: 6
content_type: how-to
---

## Overview

A local full-chain simulation allows you to deploy and test smart contracts in a fully controlled environment. This how-to walks you through the process of setting up and running a complete development environment on your local machine, including a Nitro node, a dev-mode Geth parent chain, and multiple instances with different roles.

Note that the node is now Stylus-enabled by default, and the setup instructions remain the same as for running a Stylus dev node.

## Step 1. Install prerequisites

You'll need [Docker](https://docs.docker.com/get-docker/) and [docker compose](https://docs.docker.com/compose/) to run your node. Follow the instructions on their site to install them.

## Step 2. Clone the [nitro-testnode](https://github.com/OffchainLabs/nitro-testnode) repo

<p>You'll need the `release` branch.</p>

```bash
  `git clone -b release --recurse-submodules https://github.com/OffchainLabs/nitro-testnode.git && cd nitro-testnode`
```

## Step 3. Run your node

```bash
./test-node.bash --init
```

## Step 4. Successive runs

To relaunch the node after the first installation, run the following command.

```bash
./test-node.bash
```

:::info Clear local data

Running the --init flag will clear all chain data and redeploy!

:::

## Rollup contract addresses and chain configuration

You can obtain the rollup chain configuration by running the following command. The chain configuration also includes the addresses of the core contracts.

```bash
docker exec nitro-testnode-sequencer-1 cat /config/l2_chain_info.json
```

You can find other available configuration files by running:

```bash
docker exec nitro-testnode-sequencer-1 ls /config
```

## Token bridge

An parent-child chain token bridge can be deployed by using the parameter `--tokenbridge`. The list of contracts can be found by running:

```bash
docker compose run --entrypoint sh tokenbridge -c "cat l1l2_network.json"
```

## Running an L3 chain

An L3 chain can be deployed on top of the child chain (L2), by using the parameter `--l3node`. Its chain configuration can be found by running:

```bash
docker exec nitro-testnode-sequencer-1 cat /config/l3_chain_info.json
```

When deploying an L3 chain, the following parameters are also available:

`--l3-fee-token`: Uses a custom gas token for the L3 (symbol $APP), deployed on L2 at address `0x9b7c0fcc305ca36412f87fd6bd08c194909a7d4e`
`--l3-token-bridge`: Deploys an L2-L3 token bridge. The list of contracts can be found by running `docker compose run --entrypoint sh tokenbridge -c "cat l2l3_network.json"`.

## Additional arguments

You can find a list of additional arguments to use with `test-node.bash` by using `--help`.

```bash
./test-node.bash --help
```

## Helper scripts

The repository includes a set of helper scripts for basic actions like funding accounts or bridging funds. You can see a list of the available scripts by running:

```bash
./test-node.bash script --help
```

If you want to see information of a particular script, you can add the name of the script to the help command.

```bash
./test-node.bash script send-l1 --help
```

Here's an example of how to run the script that funds an address on L2. Replace `0x11223344556677889900` with the address you want to fund.

```bash
./test-node.bash script send-l2 --to address_0x11223344556677889900 --ethamount 5
```

## Blockscout

Nitro comes with a local [Blockscout](https://www.blockscout.com/) block explorer. To access it, add the param `--blockscout` when running your node.

```bash
./test-node.bash --blockscout
```

The block explorer will be available at `http://localhost:4000`

## Default endpoints and addresses

Node RPC endpoints are available at:

| Node                  | Chain id | RPC endpoint                                      |
| --------------------- | -------- | ------------------------------------------------- |
| L1 geth devnet        | 1337     | `http://localhost:8545`                           |
| L2 nitro devnet       | 412346   | `http://localhost:8547` and `ws://localhost:8548` |
| L3 nitro (if enabled) | 333333   | `http://localhost:3347`                           |

Some important addresses:

| Role                                             | Public address                               | Private key                                                          |
| ------------------------------------------------ | -------------------------------------------- | -------------------------------------------------------------------- |
| Sequencer                                        | `0xe2148eE53c0755215Df69b2616E552154EdC584f` | `0xcb5790da63720727af975f42c79f69918580209889225fa7128c92402a6d3a65` |
| Validator                                        | `0x6A568afe0f82d34759347bb36F14A6bB171d2CBe` | `0x182fecf15bdf909556a0f617a63e05ab22f1493d25a9f1e27c228266c772a890` |
| L2 rollup owner                                  | `0x5E1497dD1f08C87b2d8FE23e9AAB6c1De833D927` | `0xdc04c5399f82306ec4b4d654a342f40e2e0620fe39950d967e1e574b32d4dd36` |
| L3 rollup owner (if enabled)                     | `0x863c904166E801527125D8672442D736194A3362` | `0xecdf21cb41c65afb51f91df408b7656e2c8739a5877f2814add0afd780cc210e` |
| L3 sequencer (if enabled)                        | `0x3E6134aAD4C4d422FF2A4391Dc315c4DDf98D1a5` | `0x90f899754eb42949567d3576224bf533a20857bf0a60318507b75fcb3edc6f5f` |
| Dev account (prefunded with ETH in all networks) | `0x3f1Eae7D46d88F08fc2F8ed27FCb2AB183EB2d0E` | `0xb6b15c8cb491557369f3c7d2c287b053eb229daa9c22138887752191c9520659` |

You can fund other addresses by using the scripts `send-l1` and `send-l2` as explained [here](#helper-scripts).

:::caution Private keys publicly known

Do not use any of these addresses in a production environment.

:::

## Optional parameters

Here, We show a list of the parameters that might be useful when running a local devnode. You can also use the flag `./test-node.bash --help` to get them.

import OptionalDevNodeCLIFlagsPartial from '../partials/_local-devnet-flags.mdx';

<OptionalDevNodeCLIFlagsPartial />

---

## .mdx (04-l1-ethereum-beacon-chain-rpc-providers.mdx)
---
title: 'L1 Ethereum beacon chain RPC providers'
author: dlee
---

:::info Note
This reference document provides an overview of Ethereum beacon chain RPC providers for Arbitrum validators to use for accessing blob data following Ethereum's Dencun upgrade in March 2024. The list curated here is **not comprehensive and in no way does Offchain Labs endorse or benefit from your use of any of these providers.**
:::

Following [Ethereum's Dencun upgrade in March 2024](https://eips.ethereum.org/EIPS/eip-7569), child blockchains like Arbitrum will be able to roll up and post batches of transaction data on Ethereum in the form of a new transaction format called a Blob. This Blob data will be part of the beacon chain and is fully downloadable by all consensus nodes. This means that data stored in blobs are inaccessible by the EVM, unlike calldata.

### What does this mean for node operators?

To run a node for a child Arbitrum chain (i.e., Arbitrum One, Arbitrum Nova, and L3 Arbitrum chains), your node will need access to blob data to sync up to the latest state of your Arbitrum child chain. Blob data on Ethereum is stored on the beacon chain and is inaccessible to the EVM, hence why dedicated RPC endpoints for the beacon chain will be required after the Dencun upgrade. You can find more details on node requirements in the [Run a full node guide](/run-arbitrum-node/02-run-full-node.mdx).

Furthermore, new node operators joining a network or node operators who come online following an extended period of offline time will require access to _historical_ blob data to sync up to the latest state of their Arbitrum chain.

### List of Ethereum beacon chain RPC providers

| Provider                                                                    | Mainnet Beacon chain APIs? | Mainnet Historical blob data? | Sepolia Beacon chain APIs? |
| --------------------------------------------------------------------------- | -------------------------- | ----------------------------- | -------------------------- |
| [Ankr](https://www.ankr.com/docs/rpc-service/chains/chains-api/eth-beacon/) | ✅                         | ✅                            | ✅                         |
| [Chainbase](https://chainbase.com/)                                         | ✅                         |                               |                            |
| [Chainstack](https://docs.chainstack.com/reference/beacon-chain)            | ✅                         | ✅                            | ✅                         |
| [Conduit](https://conduit.xyz/)\*                                           | ✅                         | ✅                            |                            |
| [BlastAPI](https://blastapi.io/public-api/ethereum)                         |                            |                               |                            |
| [Nirvana Labs](https://nirvanalabs.io)                                      | ✅                         | ✅                            |                            |
| [NodeReal](https://nodereal.io/)                                            | ✅                         |                               |                            |
| [QuickNode](https://www.quicknode.com/docs/ethereum)                        | ✅                         | ✅                            | ✅                         |
| [dRPC](https://drpc.org/chainlist/eth-beacon-chain)                         | ✅                         | ✅                            | ✅                         |

Please reach out to these teams individually if you need assistance with setting up your validator with any of the above providers.

**Case-by-case basis, please contact them directly for help**

---

## .mdx (05-run-nitro-dev-node.mdx)
---
title: 'How to run a local Nitro dev node'
description: 'This page provides instructions for setting up and running a local Nitro dev node for contract testing and development.'
author: mahsamoosavi
sme: mahsamoosavi
target_audience: 'Developers deploying smart contracts using Stylus'
content_type: how-to
sidebar_position: 5
---

## Overview

This page provides step-by-step instructions for setting up and running a local Nitro node in `--dev` mode. This mode is ideal for developers who want to quickly test contracts using a single node, as it offers a simpler and faster setup compared to more complex environments.

While some teams use `nitro-testnode` for testing cross-layer messaging, which involves launching both Geth as parent chain and Nitro as child chain, this setup can be more complex and time-consuming. If your primary goal is to test contracts on a local node without needing cross-layer interactions, Nitro's `--dev` mode offers a lightweight and efficient alternative.

However, if you need more advanced functionality—such as cross-layer messaging, working with both the parent and child chains, or testing interactions between different layers—`nitro-testnode` is the preferred option. The testnode setup allows you to simulate a full parent-child chain environment, which is critical for those scenarios. See here for instructions.

Note that Nitro `--dev` mode is ideal for Stylus contract testing, as it is much lighter and faster to set up than the full nitro-testnode environment.

## Prerequisites

Before beginning, ensure the following is installed and running on your machine:

- Docker: Required to run the Nitro dev node in a container. Install Docker by following [the official installation guide](https://docs.docker.com/get-started/get-docker/) for your operating system.
- cast: A command-line tool from Foundry for interacting with Ethereum smart contracts. You can install it via Foundry by following [the installation instructions](https://book.getfoundry.sh/getting-started/installation).
- jq: A lightweight JSON parsing tool used to extract contract addresses from the script output. Install jq by following [the official installation guide](https://jqlang.github.io/jq/download/) for your operating system.

## Clone the [nitro-devnode](https://github.com/OffchainLabs/nitro-devnode) repository

Use the following command to clone the repository:

```shell
git clone https://github.com/OffchainLabs/nitro-devnode.git
cd nitro-devnode
```

## Run the dev node script:

Run the script to start the Nitro dev node, deploy the Stylus `Cache Manager` contract, and register it as a WASM cache manager using the default development account:

```shell
./run-dev-node.sh
```

The script will:

- Start the Nitro dev node in the background using Docker.
- Deploy the Stylus `Cache Manager` contract on the local Nitro network.
- Register the `Cache Manager` contract as a WASM cache manager.

## Development account (used by default)

In `--dev` mode, the script uses a pre-funded development account by default. This account is pre-funded with `ETH` in all networks and is used to deploy contracts, interact with the chain, and assume chain ownership.

- Address: `0x3f1Eae7D46d88F08fc2F8ed27FCb2AB183EB2d0E`
- Private key: `0xb6b15c8cb491557369f3c7d2c287b053eb229daa9c22138887752191c9520659`

You don’t need to set up a private key manually unless you prefer using your own key.

## Chain ownership in `--dev` mode

In Nitro `--dev` mode, the default chain owner is set to `0x0000000000000000000000000000000000000000`. However, you can use the `ArbDebug` precompile to set the chain owner. This precompile includes the `becomeChainOwner()` function, which can be called to assume ownership of the chain.

Chain ownership is important because it allows the owner to perform certain critical functions within the Arbitrum environment, such as:

- Adding or removing other chain owners
- Setting the parent and child chain base fees directly
- Adjusting the gas pricing inertia and backlog tolerance
- Modifying the computational speed limit and transaction gas limits
- Managing network and infrastructure fee accounts

The script automatically sets the chain owner to the pre-funded dev account before registering the `Cache Manager` contract. Here’s how the `becomeChainOwner()` function is called within the script:

```shell
cast send 0x00000000000000000000000000000000000000FF "becomeChainOwner()" --private-key 0xb6b15c8cb491557369f3c7d2c287b053eb229daa9c22138887752191c9520659 --rpc-url http://127.0.0.1:8547
```

This step ensures that the dev account has ownership of the chain, which is necessary to register the `Cache Manager` as a WASM cache manager.

At the end of the process, you'll have the Nitro `dev` mode running with the necessary components deployed. This environment is ready for testing and interacting with your contracts, including those written in Stylus, using the deployed `Cache Manager` to support enhanced functionality for Stylus-based smart contracts.

---

## .mdx (06-troubleshooting.mdx)
---
title: 'Troubleshooting: Run a node'
author: symbolpunk
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Troubleshooting: Run a node

The guidance displayed on this page will change based on your selected configuration:

import MultidimensionalContentControlsPartial from '../partials/_multidimensional-content-controls-partial.mdx';

<MultidimensionalContentControlsPartial />

:::tip Thank you!

At the end of this troubleshooting guide, you'll find a **Generate troubleshooting report** button. Clicking this button will generate a report that includes your selected configuration. You can include this report when asking for help.

**Using this page to generate a troubleshooting report is helpful** because it gathers the information that we need in order to resolve your issue.

:::

### Step 1: Try the troubleshooting checklist

If you're running into unexpected outputs or errors, the following checklist may help you independently resolve your issue.

<div className="hide-tabs">
  <div className="checklist">
    <div className="task">
      <div className="input-container">
        <input id="tc-1" type="checkbox" />
        <span className="done"></span>
      </div>
      <div className="guidance-container">
        <label htmlFor="tc-1">1. Select an Operating system, Network, and Node type above</label>
        <p>The guidance displayed on this page will change based on your selected configuration.</p>
      </div>
    </div>
    <div className="task">
      <div className="input-container">
        <input id="tc-2" type="checkbox" />
        <span className="done"></span>
      </div>
      <div className="guidance-container">
        <label htmlFor="tc-2">2. Review the docs</label>
        <Tabs
          className="tabgroup-with-label node-type-tabgroup"
          groupId="node-type"
          defaultValue="full-node"
          values={[
            { label: 'Node type:', value: 'label' },
            { label: 'Full node', value: 'full-node' },
            { label: 'Archive node', value: 'archive-node' },
            { label: 'Validator node', value: 'validator-node' },
          ]}
        >
          <TabItem className="unclickable-element" value="label"></TabItem>
          <TabItem value="full-node">
            <Tabs
              className="tabgroup-with-label network-tabgroup"
              groupId="network"
              defaultValue="arb-one-nitro"
              values={[
                { label: 'Network:', value: 'label' },
                { label: 'Arbitrum One (Nitro)', value: 'arb-one-nitro' },
                { label: 'Arbitrum One (Classic)', value: 'arb-one-classic' },
                { label: 'Arbitrum Nova', value: 'arb-nova' },
                { label: 'Arbitrum Sepolia', value: 'arb-sepolia' },
                { label: 'Localhost', value: 'localhost' },
              ]}
            >
              <TabItem className="unclickable-element" value="label"></TabItem>
              <TabItem value="arb-one-nitro">
                <p>
                  The <a href="/run-arbitrum-node/run-full-node">How to run a full node (Nitro)</a>{' '}
                  may address your issue.
                </p>
              </TabItem>
              <TabItem value="arb-one-classic">
                <p>
                  <a href="/run-arbitrum-node/more-types/run-classic-node">
                    How to run a full node (Classic, pre-Nitro)
                  </a>{' '}
                  may address your issue.
                </p>
              </TabItem>
              <TabItem value="arb-nova">
                <p>
                  The <a href="/run-arbitrum-node/run-full-node">How to run a full node (Nitro)</a>{' '}
                  may address your issue.
                </p>
              </TabItem>
              <TabItem value="arb-sepolia">
                <p>
                  The <a href="/run-arbitrum-node/run-full-node">How to run a full node (Nitro)</a>{' '}
                  may address your issue.
                </p>
              </TabItem>
              <TabItem value="localhost">
                <p>
                  The{' '}
                  <a href="/run-arbitrum-node/run-local-dev-node">How to run a local dev node</a>{' '}
                  may address your issue.
                </p>
              </TabItem>
            </Tabs>
          </TabItem>
          <TabItem value="archive-node">
            <p>
              <a href="/run-arbitrum-node/more-types/run-archive-node">
                How to run an archive node
              </a>{' '}
              may address your issue.
            </p>
          </TabItem>
          <TabItem value="validator-node">
            <p>
              <a href="/run-arbitrum-node/more-types/run-validator-node">How to run a validator</a>{' '}
              may address your issue.
            </p>
          </TabItem>
        </Tabs>
      </div>
    </div>
    <div className="task">
      <div className="input-container">
        <input id="tc-3" type="checkbox" />
        <span className="done"></span>
      </div>
      <div className="guidance-container">
        <label htmlFor="tc-3">3. Review the FAQ</label>
        <p>
          Answers to frequently asked questions can be found in{' '}
          <a href="/node-running/faq">Frequently asked questions: Run a node</a>.
        </p>
      </div>
    </div>
  </div>
</div>

### Step 2: Look for your scenario

Common troubleshooting scenarios and solutions are detailed below.

You can check logs by different log types: info, warn, and error.

<Tabs
  className="tabgroup-with-label log-taggroup"
  groupId="logs"
  defaultValue="info"
  values={[
    { label: 'Logs type:', value: 'label' },
    { label: 'Info', value: 'info' },
    { label: 'Warn', value: 'warn' },
    { label: 'Error', value: 'error' },
  ]}
>
  <TabItem className="unclickable-element" value="label"></TabItem>
  <TabItem value="info">
    <table className="small-table" id="troubleshooting-table-info">
      <tbody>
        <tr>
          <th style={{ minWidth: 180 + 'px' }}>Scenario</th>
          <th>Solution</th>
        </tr>
        <tr className="scenario">
          <td>
            You see <code>Unindex transactions</code>.
          </td>
          <td>
            This is expected behavior. You'll see this when your node removes old{' '}
            <code>txlookup</code> indices. This is emitted from the base Geth node, so you'd see the
            same output from a mainnet Geth node.
          </td>
        </tr>
        <tr className="scenario">
          <td>
            You see <code>Head state missing, repairing</code>.
          </td>
          <td>
            This is usually because your node shuts down ungracefully. In most cases, it will
            recover in a few minutes, but if it not, you may have to re-sync your node. Remember to
            shut down your node gracefully with the following command:{' '}
            <code>docker stop —time=1800 $(docker ps -aq)</code>.
          </td>
        </tr>
        <tr className="scenario">
          <td>Your local machine is running out of memory</td>
          <td>
            Nitro (and Geth) can consume a lot of memory depending on the request load. It's
            possible that your machine may run out of memory when receiving tons of requests.
          </td>
        </tr>
        <tr className="scenario">
          <td>
            Your Arbitrum node can’t connect to your parent chain node on{' '}
            <code>localhost:8545</code>
          </td>
          <td>
            This is often because of a Docker port configuration issue. See
            https://stackoverflow.com/questions/43884981/unable-to-connect-localhost-in-docker.
          </td>
        </tr>
        <tr className="scenario">
          <td>
            You specified your snapshot file path via the <code>--init.url</code> parameter, but the
            snapshot file isn't found.
          </td>
          <td>
            This is usually because the snapshot file isn't mounted to your Docker container. Mount
            it and change the file path to your Docker container’s mount point.
          </td>
        </tr>
        <tr className="scenario">
          <td>
            You get <code>403</code> errors from the feed URL.
          </td>
          <td>
            This often happens when Cloudflare attempts to block botnets and other malicious actors
            but accidentally blocks node runners.
          </td>
        </tr>
        <tr className="scenario">
          <td>
            You see <code>latest assertion not yet in our node</code>
          </td>
          <td>
            This usually because your node hasn’t synced to the latest state, it’s a normal
            behavior.
          </td>
        </tr>
        <tr className="scenario">
          <td>
            You see <code>"Post "xxx_url": context deadline exceeded"</code>
          </td>
          <td>
            Please check your parent chain endpoint because there is something going wrong on that
            endpoint; you can check it.
          </td>
        </tr>
        <tr className="scenario">
          <td>
            You see <code>Resuming state snapshot generation</code>
          </td>
          <td>
            This is a normal behavior while the node is catching up to the tip of the chain; once a
            node has been fully synced, "Resuming state snapshot generation" shouldn't be logged
            unless it falls behind again.
          </td>
        </tr>
      </tbody>
    </table>
  </TabItem>
  <TabItem value="warn">
    <table className="small-table" id="troubleshooting-table-warn">
      <tbody>
        <tr>
          <th style={{ minWidth: 180 + 'px' }}>Scenario</th>
          <th>Solution</th>
        </tr>
        <tr className="scenario">
          <td>
            You see{' '}
            <code>
              `error reading inbox` err="sequencer batches out of order; after batch A got batch B”
            </code>
          </td>
          <td>
            This is because you get two discontinuous batches; this might be because of your parent
            chain endpoint issues. You can change to another endpoint and set nitro `
            --init.reorg-to-batch A`
          </td>
        </tr>
        <tr className="scenario">
          <td>
            You see{' '}
            <code>
              `error reading inbox` err="failed to get previous message for pos `x`: leveldb: not
              found”
            </code>
          </td>
          <td>
            This is because your node db crashed and lost some messages. You can try to set
            `--init.reorg-to-message-batch x-1`
          </td>
        </tr>
        <tr className="scenario">
          <td>
            You see{' '}
            <code>`Failed to load snapshot` err="head doesn't match snapshot: have a, want b”</code>
          </td>
          <td>
            This is usually because an ungraceful shutdown caused a corrupted database; try
            restarting the node without a prune flag, and after your node goes back to normal, then
            graceful shut it down and restart to prune it.
          </td>
        </tr>
        <tr className="scenario">
          <td>
            You see{' '}
            <code>
              failed to get blobs: expected at least six blobs for slot `[slot_number]` but only got
              0
            </code>
          </td>
          <td>
            This often happens when you connect to a beacon chain endpoint while the blob you are
            querying is expired. To resolve this error, connect to a beacon endpoint that supports
            historical blob data (see{' '}
            <a href="/run-arbitrum-node/l1-ethereum-beacon-chain-rpc-providers#list-of-ethereum-beacon-chain-rpc-providers">
              List of Ethereum beacon chain RPC providers
            </a>
            ).
          </td>
        </tr>
        <tr className="scenario">
          <td>
            You see <code>P2P server will be useless, neither dialing nor listening</code>
          </td>
          <td>Arbitrum Nitro doesn’t need P2P mode, so you can ignore this log.</td>
        </tr>
      </tbody>
    </table>
  </TabItem>
  <TabItem value="error">
    <table className="small-table" id="troubleshooting-table-warn">
      <tbody>
        <tr>
          <th style={{ minWidth: 180 + 'px' }}>Scenario</th>
          <th>Solution</th>
        </tr>
        <tr className="scenario">
          <td>
            You see <code>no contract code at given address</code>
          </td>
          <td>
            Your parent chain node might not sync to the latest state, please wait after it finishes
            syncing.
          </td>
        </tr>
        <tr className="scenario">
          <td>
            You see{' '}
            <code>
              `staker: error checking latest staked` err="latest assertion of x: globalstate not in
              chain: count a hash b expected c, sendroot d expected f"
            </code>
          </td>
          <td>
            Once it catches up, the node will check the state against the latest confirmed assertion
            bonded onchain; if it doesn’t match, it will log this error. Usually, this is because of
            db corruption, so you might need to re-sync the blockchain; using a snapshot might help:
            https://snapshot-explorer.arbitrum.io/
          </td>
        </tr>
        <tr className="scenario">
          <td>
            You see{' '}
            <code>
              disabling L1 bound as batch posting message is close to the maximum delay
              `blockNumber`
            </code>
            or <code>batch is within reorg resistance margin from layer 1 minimum block or timestamp bounds</code>
          </td>
          <td>
            It indicates that there has been an issue with batch posting on the network. This could
            occur if your batch poster didn't post a batch for an extended period. Common reasons
            include the node being shut down inadvertently or the batch poster running out of funds,
            leading to no new blocks being produced or posted by the batch poster. <br />
            To resolve this, If re-org doesn’t matter, you can just start the batch poster with `--node.batch-poster.reorg-resistance-margin=0`
            and `node.batch-poster.l1-block-bound` to ignore, If it does, you'd want to modify the time
            bounds on the sequencer inbox to allow the sequencer to post a batch containing the transactions
            with the old timestamp
          </td>
        </tr>
        <tr className="scenario">
          <td>
            You see{' '}
            <code>
              on-chain WASM module root did not match with any of the allowed WASM module roots
            </code>
          </td>
          <td>
            Usually, because you are running on an old node version, try to upgrade your node. Also,
            you modify your node’s code; please refer to the continue set.
          </td>
        </tr>
        <tr className="scenario">
          <td>
            You see <code>error acting as staker</code>
          </td>
          <td>
            In most cases, this error is caused by your parent chain endpoint's rate limit or other
            issues, you can check your parent chain endpoint. If the error still persists, please
            ask in our discord node-running [channel](https://discord.gg/arbitrum).
          </td>
        </tr>
      </tbody>
    </table>
  </TabItem>
</Tabs>

<!--
#### Troubleshooting your feed relay

import FeedRelayTroubleshootingPartial from '@site/../arbitrum-docs/node-running/partials/_feed-relay-troubleshooting.md';

<FeedRelayTroubleshootingPartial />
-->

### Step 3: Generate a troubleshooting report

1.  Complete the above troubleshooting checklist.
2.  Fill in the below form.
3.  Click **Generate troubleshooting report**.
4.  Copy and paste the **generated report text** when asking for support on [Discord](https://discord.gg/ZpZuw7p) or any other support channel.

<br />

import { GenerateTroubleshootingReportWidget } from '@site/src/components/GenerateTroubleshootingReportWidget.js';

<GenerateTroubleshootingReportWidget />

<div className="troubleshooting-report-area">
  <p>
    Node startup command (make sure to remove any sensitive information like, i.e., private keys)
  </p>
  <textarea
    id="vn-cmd"
    rows="3"
    placeholder='Paste here the command you use to run your node: "docker run ..."'
  ></textarea>
  <p>Unexpected output</p>
  <span>
    <strong>Tip:</strong> Paste the ~100 lines of output <strong>before and including</strong> the
    unexpected output you're asking about. You can use the following command to get the logs:{' '}
  </span>
  <code>docker logs --tail 100 YOUR_CONTAINER_ID</code>
  <textarea id="output" rows="3" placeholder="Paste your unexpected output here..."></textarea>
  <a id="generate-report" className="generate-report">
    Generate troubleshooting report
  </a>
  <div id="generated-report" className="generated-report">
    Complete the checklist above before generating...
  </div>
</div>

<!-- todo: gpt-n + langchain + pinecone -->

---

## .mdx (arbos-releases/01-overview.mdx)
---
title: 'ArbOS software releases: Overview'
sidebar_label: 'Overview'
sidebar_position: 1
author: dlee
---

:::info

This document provides an overview of Nitro node software releases that upgrade ArbOS. Visit the [Nitro Github repository](https://github.com/OffchainLabs/nitro/releases) for a detailed index of Nitro releases.

:::

Arbitrum chains are powered by Arbitrum nodes running the Nitro software stack. The Nitro software stack includes [ArbOS](https://forum.arbitrum.foundation/t/arbitrum-arbos-upgrades/19695), the child chain EVM hypervisor that facilitates the execution environment of an Arbitrum chain.

Although new Nitro releases are shipped regularly, only a subset of Nitro releases carry ArbOS upgrades. These special Nitro releases are significant because ArbOS upgrades are Arbitrum's equivalent to a ["hard fork"](https://ethereum.org/en/history/) - an upgrade that alters a node's ability to produce valid Arbitrum blocks. This is why validator nodes supporting a public Arbitrum chain (One, Nova) **must update Nitro** whenever a new ArbOS version is released and voted for adoption by the ArbitrumDAO.

:::note

Every Nitro release is backwards compatible. In other words, the latest version of Nitro will support all previous ArbOS releases. This means that your validator's Nitro version must be greater than or equal to the version that includes the latest ArbOS upgrade.

:::

import PublicPreviewBannerPartial from '../../node-running/partials/_upgrade-cadence-recommendations-partial.mdx';

<PublicPreviewBannerPartial />

ArbOS upgrades are carried out by the chain's owner; in the case of Arbitrum One and Nova, the owner is the Arbitrum DAO and so an upgrade will require a governance proposal and vote to pass to complete the upgrade. [This is an example of a Nitro release that contains an ArbOS version bump, specifically to ArbOS 11](https://github.com/OffchainLabs/nitro/releases/tag/v2.2.0).

Visit [How Arbitrum works](/how-arbitrum-works/01-a-gentle-introduction.mdx) to learn more about Nitro's architecture; more information about ArbOS software releases is available on [the Arbitrum DAO forum](https://forum.arbitrum.foundation/t/arbitrum-arbos-upgrades/19695).

## List of available ArbOS releases

- [Callisto (ArbOS 40)](/run-arbitrum-node/arbos-releases/arbos40.mdx)
- [Bianca (ArbOS 32)](/run-arbitrum-node/arbos-releases/arbos32.mdx)
- [Atlas (ArbOS 20)](/run-arbitrum-node/arbos-releases/arbos20.mdx)
- [ArbOS 11](/run-arbitrum-node/arbos-releases/arbos11.mdx)

## Naming and numbering scheme

Beginning with ArbOS 20, ArbOS releases use the name of planetary moons in our solar system, ascending in alphabetical order (i.e., the next ArbOS upgrade after ArbOS 20 "Atlas" will be a planetary moon that begins with the letter "B").

The number used to denote each upgrade will increment by 10, starting from ArbOS 20 (i.e., the next ArbOS upgrade after ArbOS 20 will be ArbOS 31). This was done because there are teams who have customized their Arbitrum chain's [behavior](/launch-arbitrum-chain/05-customize-your-chain/customize-stf.mdx) or [precompiles](/launch-arbitrum-chain/05-customize-your-chain/customize-precompile.mdx) and who may wish to use ArbOS's naming schema between official ArbOS version bumps (e.g., ArbOS 12 could be the name of a customized version of ArbOS for a project's L3 Arbitrum chain).

Note that there may be cases where special optimizations or critical fixes are needed for a specific family of ArbOS releases that will diverge from the standard numbering scheme described above. For example, ArbOS 32 will be the canonical ArbOS version for the “Bianca” family of releases. Node operators and chain owners are expected to upgrade from ArbOS 20 directly to ArbOS 32 (instead of ArbOS 30 or ArbOS 31).

## Network status

To view the status and timeline of network upgrades on Arbitrum One and Nova, [please visit this page](https://docs.arbitrum.foundation/network-upgrades).

## Expectations for Arbitrum chain owners

For Arbitrum (Orbit) chain owners or maintainers: it is important to note that _before_ upgrading your Arbitrum chain(s) to the newest ArbOS release, we strongly encourage waiting at least four weeks after the new ArbOS release becomes active on Arbitrum One and Nova before attempting the upgrade yourself. The rationale behind this short time buffer is to allow the Offchain Labs team to address any upgrade issues or stability concerns that may arise with the initial rollout so that we can minimize the chances of your chain(s) hitting the same or similar issues and to maximize the likelihood of an eventual smooth, seamless upgrade. Arbitrum Orbit chains, as always, can pick up new features & enable new customizations as they see fit. However, this delay ensures a consistent user experience (UX) across all Arbitrum chain owners and managers for these critical upgrades.

Note that enabling an ArbOS upgrade is not as simple as bumping your chain’s Nitro node version. Instead, there are other steps required that are outlined in our docs on [How to upgrade ArbOS on your Arbitrum chain](/launch-arbitrum-chain/02-configure-your-chain/common-configurations/13-arbos-upgrade.mdx). Please be sure to follow them and let us know if you encounter any issues.

## Stay up to date

To stay up to date with proposals, timelines, and statuses of network upgrades to Arbitrum One and Nova:

- Subscribe to the [Arbitrum Node Upgrade Announcement channel on Telegram](https://t.me/arbitrumnodeupgrade)
- Join both the `#dev-announcements` and `#node-runners` Discord channels in the [Arbitrum Discord server](https://discord.gg/arbitrum)
- Follow the official Arbitrum ([`@Arbitrum`](https://twitter.com/arbitrum)) and Arbitrum Developers ([`@ArbitrumDevs`](https://twitter.com/ArbitrumDevs)) X accounts, formerly Twitter.

---

## .mdx (arbos-releases/arbos11.mdx)
---
title: 'ArbOS 11'
sidebar_label: 'ArbOS 11'
sidebar_position: 5
author: dlee
---

ArbOS 11 is shipped via Nitro v2.2.0, which is available on Docker hub with the image tag: `offchainlabs/nitro-node:v2.2.0-f7dc9de`. This release of Nitro is a mandatory upgrade for Arbitrum One and Nova validators. For Arbitrum One and Nova, the ArbOS 11 upgrade requires a governance vote to activate.

Formal release notes can be found [here](https://github.com/OffchainLabs/nitro/releases/tag/v2.2.0).

### Requirements:

- [Nitro v2.2.0](https://github.com/OffchainLabs/nitro/releases/tag/v2.2.0) or higher
- [nitro-contracts v1.1.0](https://github.com/OffchainLabs/nitro-contracts/releases/tag/v1.1.0) or higher
- Wasm module root: `0x6b94a7fc388fd8ef3def759297828dc311761e88d8179c7ee8d3887dc554f3c3`

### High-level description of ArbOS 11 changes

- Addition of all EVM changes made on the parent chain Ethereum as part of the [Shanghai upgrade](https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/shanghai.md#included-eips). This includes:
  - [EIP-3651: Warm COINBASE](https://eips.ethereum.org/EIPS/eip-3651)
  - [EIP-3855: PUSH0 instruction](https://eips.ethereum.org/EIPS/eip-3855)
  - [EIP-3860: Limit and meter initcode](https://eips.ethereum.org/EIPS/eip-3860)
  - [EIP-6049: Deprecate SELFDESTRUCT](https://eips.ethereum.org/EIPS/eip-6049)
- Improvements and fixes for [retryable tickets](/how-arbitrum-works/10-l1-to-l2-messaging.mdx) to ensure that the fee calculation to redeem retryable tickets will take into account both the infrastructure fee and the network fee. The infrastructure fee is the minimum child chain base fee only, while the network fee collects child chain congestion charges. This is important for [AnyTrust chains](/how-arbitrum-works/08-anytrust-protocol.mdx) like Arbitrum Nova because members of the Data Availability Committee (DAC) gets paid a percentage of the infrastructure fee but not the network fee. Previously, the calculations to determine the fee for redeeming retryable tickets did not consider the infrastructure fee.
- Fixes an issue where the [`ArbOwnerPublic` precompile](/build-decentralized-apps/precompiles/02-reference.mdx#arbownerpublic) returned the incorrect list of chain owners. This does not change the parties who are able to perform chain owner actions. As intended, only the Arbitrum DAO is able to take chain owner actions for Arbitrum One and Nova.
- Resolves an issue where the [`arbBlockHash` method](/build-decentralized-apps/precompiles/02-reference.mdx#arbsys) would take up all the gas when reverting. The previous incorrect behavior meant that if a transaction calls `arbBlockHash` with an out-of-range block number, then the transaction would consume all the gas when reverting.
- Addition of the [`L1RewardReceipient`](/build-decentralized-apps/precompiles/02-reference.mdx#arbgasinfo) and [`L1RewardRate`](/build-decentralized-apps/precompiles/02-reference.mdx#arbgasinfo) precompile methods to view the parent chain pricing parameters and make it easier to view the current chain configuration.
- Fix the `ArbOwner` precompile to disallow emitting logs in `STATICCALL` contexts, bringing this in line with how the EVM is expected to behave as `STATICCALL` invocations should never be able to emit logs. The previous incorrect behavior would mean that a log was emitted when a chain owner made a `STATICCALL` on the `ArbOwner` precompile.

### Reference links for ArbOS 11

- [Nitro v2.2.0 Release details on Github](https://github.com/OffchainLabs/nitro/releases/tag/v2.2.0)
- Original DAO proposal: [AIP: ArbOS Version 11](https://forum.arbitrum.foundation/t/aip-arbos-version-11/19696)
- [AIP: ArbOS Version 11 Snapshot Vote](https://snapshot.org/#/arbitrumfoundation.eth/proposal/0xa635e39a2c527f7a1eabf5ea22bdec6f4a265d6c69a06076e65fde0ae0a5941b)
- [Formal Tally (onchain) vote for AIP: ArbOS Version 11](https://www.tally.xyz/gov/arbitrum/proposal/77069694702187027448745871790562515795432836429094222862498991082283032976814)
- [ArbOS 11 Audit Report by Trail of Bits](https://drive.google.com/file/d/1N3197Z7DuqBpu9qdt-GWPewe8HQakfLY/view)

---

## .mdx (arbos-releases/arbos20.mdx)
---
title: 'ArbOS 20 Atlas'
sidebar_label: 'ArbOS 20 Atlas'
sidebar_position: 3
author: dlee
---

ArbOS 20 Atlas is shipped via Nitro v2.3.1, which is available on Docker hub with the image tag: `offchainlabs/nitro-node:v2.3.1-26fad6f`. This release of Nitro is a mandatory upgrade for Arbitrum One and Nova validators. For Arbitrum One and Nova, the ArbOS 20 upgrade requires a governance vote to activate.

### Requirements:

- [Nitro v2.3.1](https://github.com/OffchainLabs/nitro/releases/tag/v2.3.1) or higher
- [nitro-contracts v1.2.1](https://github.com/OffchainLabs/nitro-contracts/releases/tag/v1.2.1) or higher
- Wasm module root: `0x8b104a2e80ac6165dc58b9048de12f301d70b02a0ab51396c22b4b4b802a16a4`
- Access to the [Ethereum Beacon Chain APIs](https://ethereum.github.io/beacon-APIs/#/), either from your own self-managed parent chain Ethereum node or from a 3rd party provider like [those on this list](/run-arbitrum-node/04-l1-ethereum-beacon-chain-rpc-providers.mdx).

### High-level description of ArbOS 20 changes

ArbOS 20 is an upgrade to enable Arbitrum's support for the parent chain Ethereum's [Dencun upgrade](https://eips.ethereum.org/EIPS/eip-7569) scheduled for March 2024. As a result, all of the ArbOS specific changes revolve around implementing the majority of the [Cancun EIPs](https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/cancun.md) on Arbitrum:

- Enable Arbitrum chains to batch and post transaction data in the form of Blobs to the parent chain Ethereum, to support [EIP-4844](https://eips.ethereum.org/EIPS/eip-4844). This includes updates to the Sequencer Inbox contract to support posting transactions in the form of blobs, updating Nitro's fraud prover to support proving additional hashes (KZG and SHA256 preimages), and updates to the core Nitro node software to handle parsing data from EIP-4844 blobs.
- Addition of the `TSTORE` and `TLOAD` EVM opcodes introduced in [EIP-1153](https://eips.ethereum.org/EIPS/eip-1153) offering a cheaper option than storage for data that’s discarded at the end of a transaction.
- Addition of the `MCOPY` EVM opcode introduced in [EIP-5656](https://eips.ethereum.org/EIPS/eip-5656) for cheaper memory copying.
- Changes to the `SELFDESTRUCT` EVM opcode to reflect the behavior on the parent chain Ethereum, as outlined in [EIP-6780](https://eips.ethereum.org/EIPS/eip-6780).
- Addition of a batch poster manager role that will have the ability to grant and revoke batch-posting affordances. This role is assigned to the operator of the sequencer to allow the batch poster manager perform key rotations for the batch posters. The DAO will continue to have the ability to revoke the seqauencer role, meaning there is no change to the current system's trust model since the DAO ca update the batch poster manager at any time (along with any batch posters).
- Increasing the max block height that a batch can be posted, relative to the current block, to 64 bringing this in line with Ethereum's finality guarantees. The current value of 12 was set prior to the Ethereum merge and could mean that a small parent chain reorg can cause an otherwise valid batch to revert.
- Fix Sequencer Inbox bug: when posting a batch, the Sequencer provides the "newMessageCount” value as a parameter; if the Sequencer is malicious, it can provide the max uint256 value which in turn would make subsequent calls to forceInclusion revert with an overflow error. Atlas’s upgrade to the Sequencer inbox includes a [change](https://github.com/OffchainLabs/nitro-contracts/blob/dcc51066b26b84cb157cbeba2f9f492ab33f9093/src/bridge/SequencerInbox.sol#L327)) in which forceInclusion does not modify the message count, fixing this bug. This bug had been disclosed to Arbitrum RaaS providers and to the Arbitrum DAO Security Council.

### Special notes on ArbOS 20: Atlas support for EIP-4844

- Upgrading to **the Atlas ArbOS release will require access to the parent chain's Ethereum beacon chain endpoints to retrieve blob data. For nodes of a chain that come online 18 days after Atlas gets activated on their chain will need access to historical data to sync up to the latest state.** If you are not operating your own Ethereum consensus client, [please visit this page to view a list of beacon chain RPC providers](/run-arbitrum-node/04-l1-ethereum-beacon-chain-rpc-providers.mdx) where you can access blob data.
- Applications on Arbitrum will not have to be modified or take any explicit action to get the benefits of using EIP-4844 (i.e., the whole chain opts-in with ArbOS 20 “Atlas”).
- ArbOS 20 “Atlas” adds support for Arbitrum chains to send data in a blob storage format to data availability layers, like the parent chain Ethereum, that support the blob transaction type. This includes Arbitrum One and Arbitrum Nova. ArbOS 20 “Atlas” does not add support for Arbitrum chains to receive data in a blob storage format. This means that an L3 Arbitrum chain on top of an Arbitrum L2 will use calldata when posting L3 transaction data to the underlying L2. The child chain (L2) Arbitrum chain will then be able to post data to a parent chain data availability layer like Ethereum using blobs.
- There currently aren’t estimates on what the end-user gas savings of using blob data will be. This topic is something being actively worked on and monitored. Without Mainnet data, the estimates for blob gas prices will not be accurate enough to reliably predict the cost reductions that users will experience - and even with Mainnet data, the savings will vary by use case (i.e., no current way to predict the price impacts from all blob gas market participants yet). In general, however, the use of blobs will reduce the cost of using Arbitrum L2s. To learn more about what EIP-4844 will mean for the child chain users, please checkout this [blog post on Medium by Offchain Lab's Co-foudner and Chief Scientist Ed Felten](https://medium.com/offchainlabs/eip-4844-what-does-it-mean-for-l2-users-5e86ebc4c028).

### Block explorers

Below is a non-comprehensive list of explorers that support querying and viewing blob data on Ethereum that get posted by Arbitrum child chain chains.

- [Blockscout](https://www.blockscout.com/). For self-deployment, blobs are supported as of blockscout v6.2.0 and blockscout-frontend v1.2.6.
- [Arbiscan](https://arbiscan.io/)
- [Blobscan](https://blobscan.com/)
- [Beaconcha.in](https://beaconcha.in/)

### Additional requirement for Arbitrum L2 chain operators: enabling blob batch posting

This section maps to [Step 4 in the guide on _How to upgrade ArbOS on your Arbitrum L2 chain_](/launch-arbitrum-chain/02-configure-your-chain/common-configurations/13-arbos-upgrade.mdx#step-4-enable-arbos-specific-configurations-or-feature-flags-not-always-required) and contains additional instructions for Arbitrum L2 chain operators for ArbOS 20 Atlas. Specifically, the details below are meant to help Arbitrum L2 chain operators enable blob batch posting to L1 Ethereum following their successful upgrade to the ArbOS 20 Atlas release.

:::caution
Before proceeding, make sure you have successfully completed Steps 1 through 3 of the guide on [How to upgrade ArbOS on your Arbitrum chain](/launch-arbitrum-chain/02-configure-your-chain/common-configurations/13-arbos-upgrade.mdx).

To enable the posting of transaction data in Blobs to L1 Ethereum, please refer to the [Enable post-4844 blobs](/launch-arbitrum-chain/02-configure-your-chain/common-configurations/14-enable-post-4844-blobs.mdx) section of the Arbitrum chain configuration guide.

### Reference links for ArbOS 20 Atlas

- [Nitro v2.3.1 Release details on Github](https://github.com/OffchainLabs/nitro/releases/tag/v2.3.1)
- Original DAO proposal: [AIP: ArbOS Version 20 "Atlas"](https://forum.arbitrum.foundation/t/aip-arbos-version-20-atlas/20957)
- [AIP: ArbOS Version 20 "Atlas" Snapshot Vote](https://snapshot.org/#/arbitrumfoundation.eth/proposal/0x813a366e287a872ada13d4f8348e771c7aa2d8c3cb00b2be31539ceab5627513)
- [Formal Tally (onchain) vote for AIP: ArbOS Version 20](https://www.tally.xyz/gov/arbitrum/proposal/46905320292877192134536823079608810426433248493109520384601548724615383601450)
- [ArbOS 20 Atlas Audit Report by Trail of Bits](https://github.com/trailofbits/publications/blob/master/reviews/2024-02-offchainlabsarbos-securityreview.pdf)

---

## .mdx (arbos-releases/arbos32.mdx)
---
title: 'ArbOS 32 Bianca'
sidebar_label: 'ArbOS 32 Bianca'
sidebar_position: 2
author: dlee
---

:::caution

Please upgrade directly to ArbOS 32 from ArbOS 20 and not to ArbOS 30 or ArbOS 31. The ArbOS 32 release builds upon ArbOS 30 and ArBbOS 31 and includes critical fixes & optimizations coming out of rigorous testing and feedback from Stylus teams. ArbOS 32 “Bianca” will be the canonical ArbOS version for the “Bianca” family of releases.

Future versions of Nitro may remove support for Arbitrum chains which have historically upgraded to, and remain on, ArbOS 30 or ArbOS 31. Due to this, we highly recommend upgrading immediately and directly to ArbOS 32.

:::

The minimum Nitro version that supports ArbOS 32 "Bianca" is [Nitro v3.3.1](https://github.com/OffchainLabs/nitro/releases/tag/v3.3.1), which is available on Docker hub with the image tag: `offchainlabs/nitro-node:v3.3.1-e326369`. This release of Nitro is a mandatory upgrade for Arbitrum One and Nova validators. For Arbitrum One and Nova, the ArbOS 32 "Bianca" upgrade required a governance vote to activate.

Please note that it is important that you only run the Nitro v3.3.1 against trusted databases. If you want to use an untrusted database, you can first remove the `wasm` directory if it exists (it might be inside the `nitro` folder). Otherwise, the database may have malicious, unvalidated code that can result in remote code execution. This is also mitigated by ensuring you run the Arbitrum Nitro node inside Docker.

The Arbitrum docs will remain the canonical home for information regarding ArbOS releases, with more details found on the [ArbOS Software Releases Overview page](./01-overview.mdx).

### Requirements:

- [Nitro v3.3.1](https://github.com/OffchainLabs/nitro/releases/tag/v3.3.1) or higher
- [nitro-contracts v2.1.0](https://github.com/OffchainLabs/nitro-contracts/releases/tag/v2.1.0) or higher
- WASM module root: `0x184884e1eb9fefdc158f6c8ac912bb183bf3cf83f0090317e0bc4ac5860baa39`

### High-level description of ArbOS 32 changes

ArbOS 32 Bianca is a major upgrade for Arbitrum chains. As a refresher, ArbOS upgrades can be treated as Arbitrum’s equivalent of a hard fork - more can be read about this subject over in [Arbitrum ArbOS upgrades](https://forum.arbitrum.foundation/t/arbitrum-arbos-upgrades/19695). Please note that ArbOS 21 Bianca is an upgrade that builds upon [ArbOS 20 Atlas](./arbos20.mdx).

ArbOS 32 Bianca brings many features, improvements, and bug fixes to Arbitrum chains. A full list of changes can be found in the Nitro release notes for [Nitro v3.3.1](https://github.com/OffchainLabs/nitro/releases/tag/v3.3.1) or higher (as Nitro 3.3.1 is the endorsed Nitro node version for ArbOS 32 Bianca). Highlighted below are a few of the most impactful and critical features that are introduced with ArbOS 32 Bianca:

- Addition and subsequent activation of [Stylus](../../stylus/gentle-introduction.mdx) on Arbitrum chains through the addition of a new WebAssembly-based (WASM) virtual machine that runs alongside the EVM. Stylus enables developers to write smart contracts in new programming languages that compile to WASM, like Rust, that are more efficient and safer than Solidity smart contracts while retaining complete interoperability.
- Adding support for [RIP-7212](https://github.com/ethereum/RIPs/blob/master/RIPS/rip-7212.md) decreases the costs of verifying the secp256r1 curve onchain [by 99% when compared to current implementations](https://www.alchemy.com/blog/what-is-rip-7212), making secp256r1 verification more feasible for everyday use and enabling dApp developers and protocols to offer their users improved UX on Arbitrum One and Arbitrum Nova. Without this precompile, verifying this signature onchain is extremely expensive. Passkey-based wallets offer better security than a typical EOA and seamless cross-device support. Many wallets, notably apps using embedded wallets, have been requesting this feature for over a year.
- [Only relevant to Arbitrum Nova] Updated the transaction fee router contracts on Arbitrum Nova to allow for fees collected to be automatically sent to the ArbitrumDAO Treasury on Arbitrum One. Currently, the ArbitrumDAO receives Arbitrum Nova transaction fees that are sent to an ArbitrumDAO-controlled address that requires a constitutional proposal to move, which is less efficient. This change is specific to Arbitrum Nova and is not expected to impact Arbitrum chains.
- Introduction of a new Fast Withdrawals feature for Arbitrum chains to achieve fast finality. This feature allows for transactions processed by a committee of validators to be unanimously confirmed as quickly as 15 minutes, as opposed to the default 6.4-day challenge period. While any Arbitrum chain can adopt Fast Withdrawals, we only recommend Fast Withdrawals for AnyTrust chains. Note that to enable this feature, separate steps must be followed (below).

### Additional requirement for Arbitrum chains who wish to take advantage of the Stylus Cache Manager

:::tip Stylus Cache Manager
It is strongly recommended that teams upgrading to ArbOS 32 also spend the time following the instructions described below to deploy and enable the Stylus Cache Manager. Even if your team does not intend to build with Stylus in the immediate term, enabling the Cache Manager ensures that future usage of Arbitrum Stylus on your chain is smooth and provides a consistent UX with the developer experience of building with Arbitrum Stylus on Arbitrum One.
:::

Specific to Stylus and ArbOS 32 "Bianca", we have developed a caching strategy that stores frequently accessed contracts in memory to reduce the costs and time associated with contract execution from repeated initializations. Check out the [Stylus caching strategy docs](../../stylus/how-tos/caching-contracts.mdx) to learn more.

In order to take advantage of this caching strategy, an additional step is required to deploy and enable it's use on your Arbitrum chain.

### Additional requirement for Arbitrum chains who wish to enable Fast Withdrawals

After you have upgraded your Arbitrum chain (Orbit) to ArbOS 32 "Bianca" (i.e., you have fully completed [Step 3 in the "How to upgrade ArbOS on your Arbitrum chain" guide](/launch-arbitrum-chain/02-configure-your-chain/common-configurations/13-arbos-upgrade.mdx#step-3-schedule-the-arbos-version-upgrade) for your Arbitrum chain), please follow [these additional instructions](https://github.com/OffchainLabs/orbit-actions/tree/main/scripts/foundry/fast-confirm) in the `orbit-actions` repository to deploy the Safe contract for the fast confirmation committee and set the Safe contract to be both the validator and fast confirmer on your rollup, note that Fast Withdrawals is disabled by default unless explicitly set up and enabled by the Arbitrum chain owner/maintainer.

### Reference links for ArbOS 32 Bianca

- [Nitro v3.3.1](https://github.com/OffchainLabs/nitro/releases/tag/v3.3.1)
- [ArbOS 32 "Bianca" onchain Tally vote](https://www.tally.xyz/gov/arbitrum/proposal/108288822474129076868455956066667369439381709547570289793612729242368710728616)
- [AIP: Activate Stylus and Enable Next-Gen WebAssembly Smart Contracts (ArbOS 32)](https://forum.arbitrum.foundation/t/aip-activate-stylus-and-enable-next-gen-webassembly-smart-contracts-arbos-30/22970)
- [AIP: Support RIP-7212 for Account Abstraction Wallets (ArbOS 32)](https://forum.arbitrum.foundation/t/aip-support-rip-7212-for-account-abstraction-wallets-arbos-30/23298)
- [AIP: Nova Fee Router Proposal (ArbOS 32)](https://forum.arbitrum.foundation/t/aip-nova-fee-router-proposal-arbos-30/23310)
- [Arbitrum Stylus Audit Report by Trail of Bits](../../audit-reports.mdx)

---

## .mdx (arbos-releases/arbos40.mdx)
---
title: 'ArbOS 40 Callisto'
sidebar_label: 'ArbOS 40 Callisto'
sidebar_position: 2
author: dlee
---

:::caution
[EIP-2537](https://eips.ethereum.org/EIPS/eip-2537) is not enabled in ArbOS 40 Callisto. This means that the precompiled contracts for certain operations on the BLS12-381 elliptic curve are not supported. However, these precompiles will be proposed for inclusion in the next ArbOS release.
:::

The minimum Nitro version that supports ArbOS 40 "Callisto" is [Nitro v3.6.5](https://github.com/OffchainLabs/nitro/releases/tag/v3.6.5), which is available on Docker Hub with the image tag `offchainlabs/nitro-node:v3.6.5-89cef87`. This release of Nitro is a mandatory upgrade for Arbitrum One and Nova validators. For Arbitrum One and Nova, the ArbOS 40 "Callisto" upgrade required a governance vote to activate.

Please note that it is important to run Nitro v3.6.5 only against trusted databases. If you want to use an untrusted database, you can first remove the `wasm` directory if it exists (potentially inside the `nitro` folder). Otherwise, the database may have malicious, unvalidated code that can result in remote code execution. Avoiding unvalidated code is also mitigated by ensuring you run the Arbitrum Nitro node inside Docker.

The Arbitrum docs will remain the canonical home for information regarding ArbOS releases, with more details found on the [ArbOS Software Releases Overview page](./01-overview.mdx).

As a refresher, ArbOS upgrades get treated as Arbitrum's equivalent of a hard fork. To learn more, refer to the [Arbitrum ArbOS upgrades](https://forum.arbitrum.foundation/t/arbitrum-arbos-upgrades/19695). Please note that ArbOS 40 Callisto is an upgrade that builds upon [ArbOS 32 Bianca](./arbos32.mdx).

### Requirements:

- [Nitro v3.6.5](https://github.com/OffchainLabs/nitro/releases/tag/v3.6.5) or higher
- [nitro-contracts v3.1.0](https://github.com/OffchainLabs/nitro-contracts/releases/tag/v3.1.0) or higher
- WASM module root: `0xdb698a2576298f25448bc092e52cf13b1e24141c997135d70f217d674bbeb69a`

:::caution

If your chain is not ready to activate BoLD, please use [nitro-contracts v2.1.3](https://github.com/OffchainLabs/nitro-contracts/releases/tag/v2.1.3) instead; v3.0.0 or higher cannot be used without activating BoLD.

:::

### High-level description of ArbOS 40 changes

ArbOS 40 Callisto is an upgrade to enable Arbitrum's support for the parent chain Ethereum's [Pectra upgrade](https://ethereum.org/en/roadmap/pectra/) scheduled for [May 7, 2025 at epoch `364032`](https://blog.ethereum.org/en/2025/04/23/pectra-mainnet). As a result, the majority of the ArbOS-specific changes revolve around implementing the relevant [Prague EIPs](https://eips.ethereum.org/EIPS/eip-7600) on Arbitrum chains.

Please see below for the list of all changes included in ArbOS 40 Callisto:

#### [EIP-7702: Set EOA Account code](https://eips.ethereum.org/EIPS/eip-7702)

EIP-7702 introduces a new transaction type that allows Externally Owned Accounts (EOAs) to set executable code, adding account-abstraction functionality to EOAs such as delegation, batching, sponsorship, and privilege de-escalation. In terms of batching, multiple operations can be combined (i.e., token approval and token spend) in an atomic transaction. Transaction sponsorship or paymaster support is extendable to EOAs. Discrete permissioning is configurable using sub-keys.

#### [EIP-2537: Precompile for BLS12-381 curve operations](https://eips.ethereum.org/EIPS/eip-2537)

:::caution
This EIP and its specified precompiles are part of ArbOS 40 Callisto but are _not_ enabled. However, these precompiles will be proposed for inclusion in the next ArbOS release.
:::

This EIP introduces precompiles for performing cryptographic operations on the BLS12-381 curve, focusing on enhancing the efficiency and security of these operations. This cryptographic primitive provides 120+ bits of security for operations over pairing-friendly curves, compared to the existing BN254 precompile, which offers only 80 bits of security. BLS signature verification is the primary use case for this EIP, although many other applications that rely on point additions, multiplications, and pairing operations stand to benefit from this proposal; examples include zkSNARKS, cross-chain interactions, randomness beacons, and vector commitments.

#### [EIP-2935: Serve historical block hashes from state](https://eips.ethereum.org/EIPS/eip-2935)

This EIP proposes storing a wider window of block hashes in the storage of a dedicated system contract. Bundling historical block hashes within the state enables efficient data retrieval for applications that require extended access to historical block hashes, like stateless clients. If approved, ArbOS 40 will adapt this EIP to the L2 and store the same number of L2 block hashes that are generated in the time it takes for 8192 L1 blocks to build - this is approximately 27 hours' worth of L2 block hashes.

#### Minor Stylus fix to correct caching behavior for contracts that do not exist ([#2998](https://github.com/OffchainLabs/nitro/pull/2998))

Currently, Stylus will cache results from calling account_code and account_code_size for a contract that does not exist. We would like to propose a fix to address this so that the call returns the correct information that properly reflects the latest state of the contract’s code or code size. This change will not increment the Stylus version, so re-activation of already deployed Stylus contracts is not required.

### Pectra changes that are not included in the proposed ArbOS 40 Callisto Upgrade

Support and implementation for the following EIPs are not planned to be part of ArbOS 40 Callisto:

- All Ethereum Consensus Layer (CL) Pectra changes (EIP-6610, EIP-7002, EIP-7251, EIP-7549, EIP-7691) because Arbitrum chains do not have a beacon chain and therefore do not have a peer-to-peer layer like Ethereum does.
- [EIP-7623](https://eips.ethereum.org/EIPS/eip-7623): Increase calldata cost: because block size variance is less of a concern on Arbitrum chains. This lack of support is due to two reasons: first, Arbitrum chains do not require nodes to send blocks over the network through their peer-to-peer layer; instead, they rely on the parent chain’s RPC to retrieve block data. Secondly, because Arbitrum block sizes are already limited to ~100KB, so increasing calldata cost is not expected to reduce Arbitrum block sizes.- [EIP-7685](https://eips.ethereum.org/EIPS/eip-7685): General purpose execution layer requests: because Arbitrum chains do not have a beacon chain and, therefore, there is nothing to request from the EL on Arbitrum chains.
- [EIP-7840](https://eips.ethereum.org/EIPS/eip-7840): Add blob schedule to EL configuration files: because Arbitrum chains do not support posting blobs on the rollup (but otherwise still does support posting blobs to Ethereum L1).

### Special note about ArbOS 40 Callisto for chains who have not yet upgraded to use Arbitrum BoLD

While ArbOS 40 Callisto will be compatible with both `nitro-contracts 3.1.0` and `nitro-contracts 2.1.3`, only chains that have Arbitrum BoLD enabled can use `nitro-contracts 3.x`. This requirement means that if your chain has not yet upgraded to use BoLD, please only use `nitro-contracts 2.1.3` for your ArbOS 40 Callisto upgrade.

### Reference links for ArbOS 40 Callisto

- [Nitro v3.6.5](https://github.com/OffchainLabs/nitro/releases/tag/v3.6.5)
- [nitro-contracts v3.1.0](https://github.com/OffchainLabs/nitro-contracts/releases/tag/v3.1.0)
- [nitro-contracts v2.1.3](https://github.com/OffchainLabs/nitro-contracts/releases/tag/v2.1.3) (only relevant for Arbitrum chains that do not have BoLD enabled yet)
- [README for how to upgrade your rollup contracts to support ArbOS 40 on your chain](https://github.com/OffchainLabs/orbit-actions?tab=readme-ov-file#nitro-contracts-upgrades)
- [AIP: ArbOS Version 40 Callisto Forum Post](https://forum.arbitrum.foundation/t/constitutional-aip-arbos-version-40-callisto/28436)
- [Temperature check vote on Snapshot for ArbOS 40 Callisto](https://snapshot.box/#/s:arbitrumfoundation.eth/proposal/0x7cc26491a070c74c1a4ec5a9892571d31eb690015936a35b52c0d3a97bd5497f)
- [ArbOS 40 Audit Report, from Trail of Bits](https://github.com/trailofbits/publications/blob/master/reviews/2025-05-offchainlabs-arbos40nitro-securityreview.pdf)

---

## .mdx (data-availability-committees/01-get-started.mdx)
---
title: 'How to configure a Data Availability Committee: Introduction'
description: Learn what's needed to configure a Data Availability Committee for your chain
author: jose-franco
sidebar_position: 1
content_type: overview
---

<p>
  <a data-quicklook-from="arbitrum-anytrust-protocol">AnyTrust</a> chains rely on an external Data
  Availability Committee (DAC) to store data and provide it on-demand instead of using its{' '}
  <a data-quicklook-from="parent-chain">parent chain</a> as the Data Availability (DA) layer. The
  members of the DAC run a Data Availability Server (DAS) to handle these operations.
</p>

This section offers information and a series of how-to guides to help you along the process of setting up a Data Availability Committee. These guides target two audiences: Committee members who wish to deploy a Data Availability Server, and chain owners who wish to configure their chain with the information of the Committee.

Before following the guides in this section, you should be familiar with how the AnyTrust protocol works and the role of the DAC in the protocol. Refer to the [AnyTrust Protocol](/how-arbitrum-works/08-anytrust-protocol.mdx) documentation to learn more.

## If you are a DAC member

Committee members will need to run a DAS. To do that, they will first need to generate a pair of keys and deploy a DAS. They may also choose to deploy an additional mirror DAS. Find more information in [How to deploy a DAS](/run-arbitrum-node/data-availability-committees/02-deploy-das.mdx) and [How to deploy a mirror DAS](/run-arbitrum-node/data-availability-committees/03-deploy-mirror-das.mdx).

Here's a basic checklist of actions to complete for DAC members:

- [Deploy a DAS](/run-arbitrum-node/data-availability-committees/02-deploy-das.mdx). Send the following information to the chain owner:
  - Public BLS key
  - The https URL for the RPC endpoint which includes some random string (e.g., das.your-chain.io/rpc/randomstring123), communicated through a secure channel
  - The https URL for the REST endpoint (e.g., das.your-chain.io/rest)
- [Deploy a mirror DAS](/run-arbitrum-node/data-availability-committees/03-deploy-mirror-das.mdx) if you want to complement your setup with a mirror DAS. Send the following information to the chain owner:
  - The https URL for the REST endpoint (e.g., das.your-chain.io/rest)

## If you are a chain owner

Chain owners will need to gather the information from the committee members to craft the necessary data to update their chain and the batch poster (more information in [How to configure the DAC in your chain](/run-arbitrum-node/data-availability-committees/04-configure-dac.mdx)). They might also want to test each DAS individually, by following the testing guides available in [How to deploy a DAS](/run-arbitrum-node/data-availability-committees/02-deploy-das.mdx#testing-the-das) and [How to deploy a mirror DAS](/run-arbitrum-node/data-availability-committees/03-deploy-mirror-das.mdx#testing-the-das).

Here's a basic checklist of actions to complete for chain owners:

- Gather the following information from every member of the committee:
  - Public BLS Key
  - URL of the RPC endpoint
  - URL(s) of the REST endpoint(s)
- Ensure that at least one DAS is running as an [archive DAS](/run-arbitrum-node/data-availability-committees/02-deploy-das.mdx#archive-da-servers)
- [Generate the keyset and keyset hash](/run-arbitrum-node/data-availability-committees/04-configure-dac.mdx#step-1-generate-the-keyset-and-keyset-hash-with-all-the-information-from-the-servers) with all the information from the servers
- [Update the `SequencerInbox` contract](/run-arbitrum-node/data-availability-committees/04-configure-dac.mdx#step-2-update-the-sequencerinbox-contract)
- [Craft the new configuration for the batch poster](/run-arbitrum-node/data-availability-committees/04-configure-dac.mdx#step-3-craft-the-new-configuration-for-the-batch-poster)
- [Craft the new configuration for your chain's nodes](/run-arbitrum-node/data-availability-committees/04-configure-dac.mdx#step-4-craft-the-new-configuration-for-your-chains-nodes)

## Ask for help

Configuring a DAC might be a complex process. If you need help setting it up, don't hesitate to ask us on [Discord](https://discord.gg/arbitrum).

---

## .mdx (data-availability-committees/02-deploy-das.mdx)
---
title: 'How to deploy a Data Availability Server (DAS)'
description: This how-to will help you deploy a Data Availability Server (DAS)
author: Mehdi Salehi, jose-franco
sidebar_position: 2
content_type: how-to
---

<p>
  <a data-quicklook-from="arbitrum-anytrust-protocol">AnyTrust</a> chains rely on an external Data
  Availability Committee (DAC) to store data and provide it on-demand instead of using its{' '}
  <a data-quicklook-from="parent-chain">parent chain</a> as the Data Availability (DA) layer. The
  members of the DAC run a Data Availability Server (DAS) to handle these operations.
</p>

In this how-to, you'll learn how to deploy a DAS that exposes:

1. **An RPC interface** that the sequencer uses to store batches of data on the DAS.
2. **An HTTP REST interface** that lets the DAS respond to requests for those batches of data.

For more information related to configuring a DAC, refer to the [Introduction](/run-arbitrum-node/data-availability-committees/01-get-started.mdx).

This how-to assumes that you're familiar with:

- The DAC's role in the AnyTrust protocol. Refer to [Inside AnyTrust](/how-arbitrum-works/08-anytrust-protocol.mdx) for a refresher.
- [Kubernetes](https://kubernetes.io/). The examples in this guide use Kubernetes to containerize your DAS.

## How does a DAS work?

A Data Availability Server (DAS) allows storage and retrieval of transaction data batches for an AnyTrust chain. It's the software that the members of the DAC run in order to provide the Data Availability service.

DA servers accept time-limited requests to store data batches from the sequencer of an AnyTrust chain, and return a signed certificate promising to store that data during the established time. They also respond to requests to retrieve the data batches.

## Configuration options

When setting up a DAS, there are certain options you can configure to suit your infrastructure needs:

### Interfaces available in a DAS

There are two main interfaces that can be enabled in a DAS: an **RPC interface** to store data in the DAS, intended to be used only by the AnyTrust Sequencer; and a **REST interface** that supports only `GET` operations and is intended for public use.

DA servers listen on two primary interfaces:

1.  Its **RPC interface** listens for `das_store` RPC messages coming from the sequencer. Messages are signed by the sequencer, and the DAS checks this signature upon receipt.
2.  Its **REST interface** respond to HTTP `GET` requests pointed at `/get-by-hash/<hex encoded data hash>`. This uses the hash of the data batch as a unique identifier, and will always return the same data for a given hash.

### Storage options

A DAS can be configured to use one or more of four storage backends:

- [AWS S3](https://aws.amazon.com/s3/) bucket
- Files on local disk
- (**EXPERIMENTAL**) [Google Cloud Storage](https://cloud.google.com/storage) bucket
- (**DEPRECATED**) [Badger](https://dgraph.io/docs/badger/) database on local disk

::::warning Google Cloud Storage is experimental

The Google Cloud Storage option (set with `google-cloud-storage`) is experimental and hasn't been tested thoroughly. It is recommended to not rely solely on this storage option and to use it alongside other storage options.

::::

::::warning Local Badger database deprecated

The local Badger DB storage option (set with `local-db-storage`) has been deprecated and should be replaced with the local files storage option (set with `local-file-storage`).

A migration tool has been included in Nitro to migrate all data from the local badger db to local files. You can activate it by using the parameter `--data-availability.migrate-local-db-to-file-storage`.

::::

If more than one option is selected, store requests must succeed to all of them for it to be considered successful, while retrieve requests only require one of them to succeed.

If there are other storage backends you'd like us to support, send us a message on [Discord](https://discord.gg/arbitrum), or contribute directly to the [Nitro repository](https://github.com/OffchainLabs/nitro/).

### Caching

An in-memory cache can be enabled to avoid needing to access underlying storage for retrieve requests.

Requests sent to the REST interface (to retrieve data from the DAS) always return the same data for a given hash, so the result is cacheable. It also contains a `cache-control` header specifying that the object is immutable and to cache it for up to 28 days.

### State synchronization

DA servers also have an optional REST aggregator which, when a data batch is not found in cache or storage, requests that batch to other REST servers defined in a list and stores that batch upon receiving it. This is how a DAS that misses storing a batch (the AnyTrust protocol doesn't require all of them to report success in order to post the batch's certificate to the parent chain) can automatically repair gaps in the data it stores, and also how a [mirror DAS](#running-a-mirror-das) can sync its data. A public list of REST endpoints is published online, which the DAS can be configured to download and use, and additional endpoints can be specified in the configuration.

## How to deploy the DAS

### Step 0: Prerequisites

Gather the following information:

- The latest Nitro docker image: `@@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@`
- An RPC endpoint for the <a data-quicklook-from="parent-chain">parent chain</a>. It is recommended to use a [third-party provider RPC](/build-decentralized-apps/reference/01-node-providers.mdx#third-party-rpc-providers) or [run your own node](/run-arbitrum-node/02-run-full-node.mdx) to prevent being rate limited.
- The `SequencerInbox` contract address in the parent chain.

- If you wish to configure a [REST aggregator for your DAS](#state-synchronization), you'll need the URL where the list of REST endpoints is kept.

#### Hardware requirements

- **Data Availability Server (DAS)** – A single CPU and 1 GiB of RAM can comfortably handle normal DAS duties. CPU spikes are rare and memory usage stays well below 1 GiB—even with an in-memory cache enabled.

- **Mirror DAS** – Mirrors do even less work. When a CDN (e.g., Cloudflare, Fastly, CloudFront) sits in front, most requests never reach the node.

**Note** that CDN is mandatory for any publicly reachable REST endpoint (mirror or main DAS). Without a CDN absorbing traffic, you need beefier hardware and you leave the server open to DoS attacks.

:::info Heads-up
If you crank up the in-memory cache or co-host other services, consider bumping the memory to 2 GiB for safety. Disk (or S3) requirements scale with your retention policy: archive nodes need space for the full history, while non-archive nodes can offload older data via lifecycle rules.

:::

<!--
In terms of hardware requirements, these are the minimum specs that your infrastructure should comply with to run a stable DAS:

- RAM:
- CPU:

Depending on the storage backend you use for your DAS, you'll need to configure the appropriate infrastructure: for example, an S3 bucket if you choose S3, or a local volume if you choose any of the local backend options. When using a local volume, it is recommended to use …
-->

### Step 1: Generate the BLS keypair

Next, we'll generate a BLS keypair. The private key will be used to sign the <a data-quicklook-from="data-availability-certificate">Data Availability Certificates (DACert)</a> when receiving requests to store data, and the public key will be used to prove that the DACert was signed by the DAS. The BLS private key is sensitive and care must be taken to ensure it is generated and stored in a safe environment.

The BLS keypair must be generated using the `datool keygen` utility. Later, it will be passed to the DAS by file or command line.

When running the key generator, we'll specify the `--dir` parameter with the absolute path to the directory inside the volume to store the keys in.

Here's an example of how to use the `datool keygen` utility inside Docker and store the key that will be used by the DAS in the next step.

```shell
docker run -v $(pwd)/bls_keys:/data/keys --entrypoint datool \
@@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@ keygen --dir /data/keys
```

### Step 2: Deploy the DAS

To run the DAS, we'll use the `daserver` tool and we'll configure the following parameters:

| Parameter                                   | Description                                                                                                     |
| ------------------------------------------- | --------------------------------------------------------------------------------------------------------------- |
| --data-availability.parent-chain-node-url   | RPC endpoint of a parent chain node                                                                             |
| --data-availability.sequencer-inbox-address | Address of the `SequencerInbox` in the parent chain                                                             |
| --data-availability.key.key-dir             | The absolute path to the directory inside the volume to read the BLS keypair ('das_bls.pub' and 'das_bls') from |
| --enable-rpc                                | Enables the HTTP-RPC server listening on --rpc-addr and --rpc-port                                              |
| --rpc-addr                                  | HTTP-RPC server listening interface (default "localhost")                                                       |
| --rpc-port                                  | (Optional) HTTP-RPC server listening port (default 9876)                                                        |
| --enable-rest                               | Enables the REST server listening on --rest-addr and --rest-port                                                |
| --rest-addr                                 | REST server listening interface (default "localhost")                                                           |
| --rest-port                                 | (Optional) REST server listening port (default 9877)                                                            |
| --log-level                                 | Log level: 1 - ERROR, 2 - WARN, 3 - INFO, 4 - DEBUG, 5 - TRACE (default 3)                                      |

To enable caching, you can use the following parameters:

| Parameter                                | Description                                                                        |
| ---------------------------------------- | ---------------------------------------------------------------------------------- |
| --data-availability.local-cache.enable   | Enables local in-memory caching of sequencer batch data                            |
| --data-availability.local-cache.capacity | Maximum number of entries (up to 64KB each) to store in the cache. (default 20000) |

To enable the REST aggregator, use the following parameters:

| Parameter                                                                   | Description                                                                                                                                                                                 |
| --------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| --data-availability.rest-aggregator.enable                                  | Enables retrieval of sequencer batch data from a list of remote REST endpoints                                                                                                              |
| --data-availability.rest-aggregator.online-url-list                         | A URL to a list of URLs of REST DAS endpoints that is checked at startup. This option is additive with the urls option                                                                      |
| --data-availability.rest-aggregator.urls                                    | List of URLs including 'http://' or 'https://' prefixes and port numbers to REST DAS endpoints. This option is additive with the online-url-list option                                     |
| --data-availability.rest-aggregator.sync-to-storage.check-already-exists    | When using a REST aggregator, checks if the data already exists in this DAS's storage. Must be disabled for fast sync with an IPFS backend (default true)                                   |
| --data-availability.rest-aggregator.sync-to-storage.eager                   | When using a REST aggregator, eagerly syncs batch data to this DAS's storage from the REST endpoints, using the parent chain as the index of batch data hashes; otherwise only syncs lazily |
| --data-availability.rest-aggregator.sync-to-storage.eager-lower-bound-block | When using a REST aggregator that's eagerly syncing, starts indexing forward from this block from the parent chain. Only used if there is no sync state.                                    |
| --data-availability.rest-aggregator.sync-to-storage.retention-period        | When using a REST aggregator, period to retain the synced data (defaults to forever)                                                                                                        |
| --data-availability.rest-aggregator.sync-to-storage.state-dir               | When using a REST aggregator, directory to store the sync state in, i.e., the block number currently synced up to, so that it doesn't sync from scratch each time                           |

Finally, for the storage backends you wish to configure, use the following parameters. Toggle between the different options to see all available parameters.

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import S3Parameters from '../../node-running/how-tos/data-availability-committee/partials/parameters/_s3-parameters.mdx';
import LocalFilesParameters from '../../node-running/how-tos/data-availability-committee/partials/parameters/_local-files-parameters.mdx';
import GoogleCloudStorageParameters from '../../node-running/how-tos/data-availability-committee/partials/parameters/_gcs-parameters.mdx';
import LocalBadgerDBParameters from '../../node-running/how-tos/data-availability-committee/partials/parameters/_local-badger-db-parameters.mdx';

<div className="dynamic-content-tabs">
  <Tabs className="tabgroup" defaultValue={null}>
    <TabItem value="s3-bucket" label="AWS S3 bucket">
      <S3Parameters />
    </TabItem>
    <TabItem value="local-files" label="Local files">
      <LocalFilesParameters />
    </TabItem>
    <TabItem value="gcs-bucket" label="(Experimental) Google Cloud Storage">
      <GoogleCloudStorageParameters />
    </TabItem>
    <TabItem value="badger-db" label="(Deprecated) Local Badger database">
      <LocalBadgerDBParameters />
    </TabItem>
  </Tabs>
</div>

Here's an example `daserver` command for a DAS that:

- Enables both interfaces: RPC and REST
- Enables local cache
- Enables a [REST aggregator](#state-synchronization)
- Enables AWS S3 bucket storage
- Enables local files storage

```shell
daserver
    --data-availability.parent-chain-node-url "<YOUR PARENT CHAIN RPC ENDPOINT>"
    --data-availability.sequencer-inbox-address "<ADDRESS OF SEQUENCERINBOX ON PARENT CHAIN>"
    --data-availability.key.key-dir /home/user/data/keys
    --enable-rpc
    --rpc-addr '0.0.0.0'
    --log-level 3
    --enable-rest
    --rest-addr '0.0.0.0'
    --data-availability.local-cache.enable
    --data-availability.rest-aggregator.enable
    --data-availability.rest-aggregator.online-url-list "<URL TO LIST OF REST ENDPOINTS>"
    --data-availability.s3-storage.enable
    --data-availability.s3-storage.access-key "<YOUR ACCESS KEY>"
    --data-availability.s3-storage.bucket "<YOUR BUCKET>"
    --data-availability.s3-storage.region "<YOUR REGION>"
    --data-availability.s3-storage.secret-key "<YOUR SECRET KEY>"
    --data-availability.s3-storage.object-prefix "<YOUR OBJECT KEY PREFIX>/"
    --data-availability.local-file-storage.enable
    --data-availability.local-file-storage.data-dir /home/user/data/das-data
```

And here's an example of how to use a k8s deployment to run that command:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
name: das-server
spec:
replicas: 1
selector:
    matchLabels:
    app: das-server
strategy:
    rollingUpdate:
    maxSurge: 0
    maxUnavailable: 50%
    type: RollingUpdate
template:
    metadata:
    labels:
        app: das-server
    spec:
    containers:
    - command:
        - bash
        - -c
        - |
        mkdir -p /home/user/data/badgerdb
        /usr/local/bin/daserver --data-availability.parent-chain-node-url "<YOUR PARENT CHAIN RPC ENDPOINT>" --data-availability.sequencer-inbox-address "<ADDRESS OF SEQUENCERINBOX ON PARENT CHAIN>" --data-availability.key.key-dir /home/user/data/keys --enable-rpc --rpc-addr '0.0.0.0' --log-level 3 --enable-rest --rest-addr '0.0.0.0' --data-availability.local-cache.enable --data-availability.rest-aggregator.enable --data-availability.rest-aggregator.online-url-list "<URL TO LIST OF REST ENDPOINTS>" --data-availability.s3-storage.enable --data-availability.s3-storage.access-key "<YOUR ACCESS KEY>" --data-availability.s3-storage.bucket "<YOUR BUCKET>" --data-availability.s3-storage.region "<YOUR REGION>" --data-availability.s3-storage.secret-key "<YOUR SECRET KEY>" --data-availability.s3-storage.object-prefix "<YOUR OBJECT KEY PREFIX>/" --data-availability.s3-storage.discard-after-timeout false --data-availability.local-file-storage.enable --data-availability.local-file-storage.data-dir /home/user/data/das-data
        image: @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@
        imagePullPolicy: Always
        resources:
        limits:
            cpu: "4"
            memory: 10Gi
        requests:
            cpu: "4"
            memory: 10Gi
        ports:
        - containerPort: 9876
        hostPort: 9876
        protocol: TCP
        - containerPort: 9877
        hostPort: 9877
        protocol: TCP
        volumeMounts:
        - mountPath: /home/user/data/
        name: data
        readinessProbe:
        failureThreshold: 3
        httpGet:
            path: /health/
            port: 9877
            scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
    volumes:
    - name: data
        persistentVolumeClaim:
        claimName: das-server
```

## Archive DA servers

Archive DA servers are servers that don't discard any data after expiring. Each DAC should have at the very least one archive DAS to ensure all historical data is available.

To activate the "archive mode" in your DAS, set the parameter `discard-after-timeout` to `false` in your storage method. For example:

```shell
--data-availability.local-db-storage.discard-after-timeout=false
```

:::note

The `s3-storage` **doesn't support** expiration of data, instead that can be enabled from user side by setting `Expiration` as a rule in Lifecycle Configuration of the corresponding S3 bucket. More information available at [AWS Expiring objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-expire-general-considerations.html).

:::

:::note

The `local-file-storage` doesn't discard data after expiring by default, but expiration can be enabled with `enable-expiry`.

:::

Archive servers should make use of the `--data-availability.rest-aggregator.sync-to-storage` options described above to pull in any data that they don't have.

## Helm charts

A helm chart is available at [ArtifactHUB](https://artifacthub.io/packages/helm/offchainlabshelm/das). It supports running a DAS by providing the BLS key and the parameters for your server. Find more information in the [OCL community Helm charts repository](https://github.com/OffchainLabs/community-helm-charts/tree/main/charts/das).

## Testing the DAS

Once the DAS is running, we can test if everything is working correctly using the following methods.

### Test 1: RPC health check

The RPC interface enabled in the DAS has a health check for the underlying storage that can be invoked by using the RPC method `das_healthCheck` that returns a status `200` if the DAS is active.

**Example**:

```shell
curl -X POST \
     -H 'Content-Type: application/json' \
     -d '{"jsonrpc":"2.0","id":0,"method":"das_healthCheck","params":[]}' \
     <YOUR RPC ENDPOINT>
```

### Test 2: Store and retrieve data

The RPC interface of the DAS validates that requests to store data are signed by the sequencer's ECDSA key, identified via a call to the `SequencerInbox` contract on the parent chain. It can also be configured to accept store requests signed with another ECDSA key of your choosing. This could be useful for running load tests, canaries, or troubleshooting your own infrastructure.

Using this facility, a load test could be constructed by writing a script to store arbitrary amounts of data at an arbitrary rate; a canary could be constructed to store and retrieve data on some interval. We show here a short guide on how to do that.

#### Step 1: Generate an ECDSA keypair

First we'll generate an ECDSA keypair with `datool keygen`. Create a folder inside `/some/local/dir` to store the ECDSA keypair, for example `/some/local/dir/keys`. Then run `datool keygen`:

```shell
datool keygen --dir /some/local/dir/keys --ecdsa
```

You can also use the `docker run` command as follows:

```shell
docker run --rm -it -v /some/local/dir:/home/user/data --entrypoint datool @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@ keygen --dir /home/user/data/keys --ecdsa
```

#### Step 2: Change the DAS configuration and restart the server

Add the following configuration parameter to `daserver`:

```shell
--data-availability.extra-signature-checking-public-key /some/local/dir/keys/ecdsa.pub
```

OR

```shell
--data-availability.extra-signature-checking-public-key "0x<contents of ecdsa.pub>"
```

And then restart it.

#### Step 3: Store data signed with the ECDSA private key

Now you can use the `datool` utility to send store requests signed with the ECDSA private key:

```shell
datool client rpc store  --url http://localhost:9876 --message "Hello world" --signing-key /some/local/dir/keys/ecdsa
```

OR

```shell
datool client rpc store  --url http://localhost:9876 --message "Hello world" --signing-key "0x<contents of ecdsa>"
```

You can also use the `docker run` command:

```shell
docker run --rm -it -v /some/local/dir:/home/user/data --network="host" --entrypoint datool @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@ client rpc store --url http://localhost:9876 --message "Hello world" --signing-key /home/user/data/keys/ecdsa
```

The above command will output the `Hex Encoded Data Hash` which can then be used to retrieve the data in the next step.

#### Step 4: Retrieve the stored data

Use again the `datool` to retrieve the stored data. Notice that to perform this step you must have the REST interface enabled in the DAS:

```shell
datool client rest getbyhash --url http://localhost:9877 --data-hash 0xDataHash
```

You can also use the `docker run` command:

```shell
docker run --rm -it --network="host" --entrypoint datool @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@ client rest getbyhash --url http://localhost:9877 --data-hash 0xDataHash
```

If we set `0xDataHash` to `0x052cca0e379137c975c966bcc69ac8237ac38dc1fcf21ac9a6524c87a2aab423` (from the previous step), then the result should be: `Message: Hello world`

The retention period defaults to 24 hours, but can be configured when calling `datool client rpc store` with the parameter `--das-retention-period` and the number of milliseconds for the retention period.

### Test 3: REST health check

The REST interface has a health check on the path `/health` which will return a status `200` if the underlying storage is working, otherwise `503`.

Example:

```shell
curl -I <YOUR REST ENDPOINT>/health
```

### Test 4: Retrieve data from a batch poster transaction

You can also do a test to retrieve the transaction data posted by a batch poster transaction. The transaction will contain both keyset and data hash information in its `data` field in method `addSequencerL2BatchFromOrigin(uint256 sequenceNumber, bytes data,uint256 afterDelayedMessagesRead, address gasRefunder,uint256 prevMessageCount,uint256 newMessageCount)`.

After you decode a batch poster transaction and get its `data` within the function data, you can continue to decode the `data` as follows:

The first part (1 byte) is the `header flag`, which is used to specify which type of batch it is. Here we need to check if it has bit `0x80` (For example, `0x88` and `0x80` are both valid, but `0x55` is wrong).

The second part (32 bytes) is the keyset hash. You can learn more about what keyset is [here](/how-arbitrum-works/08-anytrust-protocol.mdx#keysets).

The third part (32 bytes) is the data hash, and this is what we need to retrieve data. When you get this hash, you can retrieve data directly by following what we demonstrate in Step 4.

## Running a mirror DAS

To avoid exposing the REST interface of your main DAS to the public in order to prevent spamming attacks (as explained in [Security considerations](#security-considerations)), you can choose to run a mirror DAS to complement your setup. The mirror DAS will handle all public REST requests, while reading information from the main DAS via its (now private) REST interface.

In general, mirror DA servers serve two main purposes:

1. Prevent the main DAS from having to serve requests for data, allowing it to focus only on storing the data received.
2. Provide resiliency to the network in the case of a DAS going down.

Find information about how to setup a mirror DAS in [How to deploy a mirror DAS](/run-arbitrum-node/data-availability-committees/03-deploy-mirror-das.mdx).

## Security considerations

Keep in mind the following information when running the DAS.

A DAS should strive not to miss any batch of information sent by the sequencer. Although it can use a REST aggregator to fetch missing information from other DA servers, it should aim to synchronize all received information directly. To facilitate this, avoid placing any load balancing layer before the DAS, enabling it to handle all incoming traffic.

Taking that into account, there's a risk of Denial of Service attacks on those servers if the endpoint for the RPC interface is publicly known. To mitigate this risk, ensure the RPC endpoint's URL is not easily discoverable. It should be known only to the sequencer. Share this information with the chain owner through a private channel to maintain security.

Finally, as explained in the previous section, if you're also running a mirror DAS, there's no need to publicly expose the REST interface of your main DAS. Your mirrors can synchronize over your private network using the REST interface from your main DAS and other public mirrors.

## External signer support

By default the batch poster uses the same ECDSA key to sign `das_store` requests as it uses to sign the batch transactions sent to the sequencer inbox contract. Many installations use an external signer for securing the batch poster's key. While using an external signer is suported for signing batch transactions, it is not currently supported for signing the requests sent to the DA Committee. Currently, if you want to use an external signer for the batch transactions together with AnyTrust, you must generate a separate key for signing the requests sent to the DA Committee. If a wallet file is used the account must be named "l1-batch-poster".

The batch poster would need to have the configuration for the external signer

```
--node.batch-poster.data-poster.external-signer...
```

and the configuration for the key which is only used for signing DA Committee requests.

```
--node.batch-poster.parent-chain-wallet...
```

The Committee servers would need to additionally specify the public key to accept signed messages from.

```
--data-availability.extra-signature-checking-public-key
```

## Other considerations

- When using [nginx](https://www.nginx.com/) in the networking stack, a DAS might fail receiving batches that are over a certain size. If this happens, the DAS won't be able to sign any more certificates and the batch poster will receive an error `413 Request Entity Too Large`. To prevent this behavior, the parameter `client_max_body_size` from nginx configuration should be configured with a higher value than the default 1M. It's recommended to set it to at least 50M.

## What to do next?

Once the DAS is deployed and tested, you'll have to communicate the following information to the chain owner, so they can update the chain parameters and configure the sequencer:

- Public key
- The https URL for the RPC endpoint which includes some random string (e.g., das.your-chain.io/rpc/randomstring123), communicated through a secure channel
- The https URL for the REST endpoint (e.g., das.your-chain.io/rest)

import DASOptionalParameters from '../../node-running/how-tos/data-availability-committee/partials/_das-optional-parameters.mdx';
import DASMetrics from '../../node-running/how-tos/data-availability-committee/partials/_das-metrics.mdx';

## Optional parameters

<DASOptionalParameters />

## Metrics

<DASMetrics />

---

## .mdx (data-availability-committees/03-deploy-mirror-das.mdx)
---
title: 'How to deploy a mirror Data Availability Server (DAS)'
description: This how-to will help you deploy a mirror Data Availability Server (DAS)
author: jose-franco
sidebar_position: 3
content_type: how-to
---

:::caution Running a regular DAS vs running a mirror DAS

The main use-case for running a mirror DAS is to complement your setup as a Data Availability Committee (DAC) member. That means that you should run your main DAS first, and then configure the mirror DAS. Refer to [How to deploy a DAS](/run-arbitrum-node/data-availability-committees/02-deploy-das.mdx) if needed.

:::

<p>
  <a data-quicklook-from="arbitrum-anytrust-protocol">AnyTrust</a> chains rely on an external Data
  Availability Committee (DAC) to store data and provide it on-demand instead of using its{' '}
  <a data-quicklook-from="parent-chain">parent chain</a> as the Data Availability (DA) layer. The
  members of the DAC run a Data Availability Server (DAS) to handle these operations.
</p>

In this how-to, you'll learn how to configure a mirror DAS that serves `GET` requests for stored batches of information through a REST HTTP interface. For a refresher on DACs, refer to the [Introduction](/run-arbitrum-node/data-availability-committees/01-get-started.mdx).

This how-to assumes that you're familiar with:

- How a regular DAS works and what configuration options are available. Refer to [How to deploy a DAS](/run-arbitrum-node/data-availability-committees/02-deploy-das.mdx) for a refresher.
- [Kubernetes](https://kubernetes.io/). The examples in this guide use Kubernetes to containerize your DAS.

## What is a mirror DAS?

To avoid exposing the REST interface of your main DAS to the public in order to prevent spamming attacks (as explained in [How to deploy a DAS](/run-arbitrum-node/data-availability-committees/02-deploy-das.mdx#security-considerations)), you can choose to run a mirror DAS to complement your setup. The mirror DAS will handle all public REST requests, while reading information from the main DAS via its (now private) REST interface.

In general, mirror DA servers serve two main purposes:

1. Prevent the main DAS from having to serve requests for data, allowing it to focus only on storing the data received.
2. Provide resiliency to the network in the case of a DAS going down.

## Configuration options

A mirror DAS will use the same tool and, thus, the same configuration options as your main DAS. You can find an explanation of those options in [How to deploy a DAS](/run-arbitrum-node/data-availability-committees/02-deploy-das.mdx#configuration-options).

## How to deploy a mirror DAS

### Step 0: Prerequisites

Gather the following information:

- The latest Nitro docker image: `@@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@`
- An RPC endpoint for the <a data-quicklook-from="parent-chain">parent chain</a>. It is recommended to use a [third-party provider RPC](/build-decentralized-apps/reference/01-node-providers.mdx#third-party-rpc-providers) or [run your own node](/run-arbitrum-node/02-run-full-node.mdx) to prevent being rate limited.
- The SequencerInbox contract address in the parent chain.
- URL of the list of REST endpoints of other DA servers to configure the REST aggregator.

<!--
In terms of hardware requirements, these are the minimum specs that your infrastructure should comply with to run a stable DAS:

- RAM:
- CPU:

Depending on the storage backend you use for your DAS, you'll need to configure the appropriate infrastructure: for example, an S3 bucket if you choose S3, or a local volume if you choose any of the local backend options. When using a local volume, it is recommended to use …
-->

### Step 1: Set up a persistent volume

First, we'll set up a volume to store the DAS database. In k8s, we can use a configuration like this:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: das-mirror
spec:
  accessModes:
    - ReadWriteOnce
  resources:
  requests:
    storage: 200Gi
  storageClassName: gp2
```

### Step 2: Deploy the mirror DAS

To run the mirror DAS, we'll use the `daserver` tool and we'll configure the following parameters:

| Parameter                                                                   | Description                                                                                                                                                                                 |
| --------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| --data-availability.parent-chain-node-url                                   | RPC endpoint of a parent chain node                                                                                                                                                         |
| --data-availability.sequencer-inbox-address                                 | Address of the SequencerInbox in the parent chain                                                                                                                                           |
| --enable-rest                                                               | Enables the REST server listening on --rest-addr and --rest-port                                                                                                                            |
| --rest-addr                                                                 | REST server listening interface (default "localhost")                                                                                                                                       |
| --rest-port                                                                 | (Optional) REST server listening port (default 9877)                                                                                                                                        |
| --log-level                                                                 | Log level: 1 - ERROR, 2 - WARN, 3 - INFO, 4 - DEBUG, 5 - TRACE (default 3)                                                                                                                  |
| --data-availability.rest-aggregator.enable                                  | Enables retrieval of sequencer batch data from a list of remote REST endpoints                                                                                                              |
| --data-availability.rest-aggregator.online-url-list                         | A URL to a list of URLs of REST DAS endpoints that is checked at startup. This option is additive with the URLs option                                                                      |
| --data-availability.rest-aggregator.urls                                    | List of URLs including 'http://' or 'https://' prefixes and port numbers to REST DAS endpoints. This option is additive with the online-url-list option                                     |
| --data-availability.rest-aggregator.sync-to-storage.check-already-exists    | When using a REST aggregator, checks if the data already exists in this DAS's storage. Must be disabled for fast sync with an IPFS backend (default true)                                   |
| --data-availability.rest-aggregator.sync-to-storage.eager                   | When using a REST aggregator, eagerly syncs batch data to this DAS's storage from the REST endpoints, using the parent chain as the index of batch data hashes; otherwise only syncs lazily |
| --data-availability.rest-aggregator.sync-to-storage.eager-lower-bound-block | When using a REST aggregator that's eagerly syncing, starts indexing forward from this block from the parent chain. Only used if there is no sync state.                                    |
| --data-availability.rest-aggregator.sync-to-storage.retention-period        | When using a REST aggregator, period to retain the synced data (defaults to forever)                                                                                                        |
| --data-availability.rest-aggregator.sync-to-storage.state-dir               | When using a REST aggregator, directory to store the sync state in, i.e., the block number currently synced up to, so that it doesn't sync from scratch each time                           |

To enable caching, you can use the following parameters:

| Parameter                                | Description                                                                        |
| ---------------------------------------- | ---------------------------------------------------------------------------------- |
| --data-availability.local-cache.enable   | Enables local in-memory caching of sequencer batch data                            |
| --data-availability.local-cache.capacity | Maximum number of entries (up to 64KB each) to store in the cache. (default 20000) |

Finally, for the storage backends you wish to configure, use the following parameters. Toggle between the different options to see all available parameters.

::::warning Local Badger database deprecated

The local badger DB storage option (set with `local-db-storage`) has been deprecated and should be replaced with the local files storage option (set with `local-file-storage`).

A migration tool has been included in Nitro to migrate all data from the local badger db to local files. You can activate it by using the parameter `--data-availability.migrate-local-db-to-file-storage`.

::::

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import S3Parameters from '../../node-running/how-tos/data-availability-committee/partials/parameters/_s3-parameters.mdx';
import LocalFilesParameters from '../../node-running/how-tos/data-availability-committee/partials/parameters/_local-files-parameters.mdx';
import GoogleCloudStorageParameters from '../../node-running/how-tos/data-availability-committee/partials/parameters/_gcs-parameters.mdx';
import LocalBadgerDBParameters from '../../node-running/how-tos/data-availability-committee/partials/parameters/_local-badger-db-parameters.mdx';

<div className="dynamic-content-tabs">
  <Tabs className="tabgroup" defaultValue={null}>
    <TabItem value="s3-bucket" label="AWS S3 bucket">
      <S3Parameters />
    </TabItem>
    <TabItem value="local-files" label="Local files">
      <LocalFilesParameters />
    </TabItem>
    <TabItem value="gcs-bucket" label="(Experimental) Google Cloud Storage">
      <GoogleCloudStorageParameters />
    </TabItem>
    <TabItem value="badger-db" label="(Deprecated) Local Badger database">
      <LocalBadgerDBParameters />
    </TabItem>
  </Tabs>
</div>

Here's an example `daserver` command for a mirror DAS that:

- Enables local cache
- Enables AWS S3 bucket storage that doesn't discard data after expiring ([archive](#archive-da-servers))
- Enables local file storage that, by default, doesn't discard data after expiring ([archive](#archive-da-servers))
- Uses a local main DAS as part of the REST aggregator

```shell
daserver
    --data-availability.parent-chain-node-url "<YOUR PARENT CHAIN RPC ENDPOINT>"
    --data-availability.sequencer-inbox-address "<ADDRESS OF SEQUENCERINBOX ON PARENT CHAIN>"
    --enable-rest
    --rest-addr '0.0.0.0'
    --log-level 3
    --data-availability.local-cache.enable
    --data-availability.rest-aggregator.enable
    --data-availability.rest-aggregator.urls "http://your-main-das.svc.cluster.local:9877"
    --data-availability.rest-aggregator.online-url-list "<URL TO LIST OF REST ENDPOINTS>"
    --data-availability.rest-aggregator.sync-to-storage.eager
    --data-availability.rest-aggregator.sync-to-storage.eager-lower-bound-block "BLOCK NUMBER"
    --data-availability.rest-aggregator.sync-to-storage.state-dir /home/user/data/syncState
    --data-availability.s3-storage.enable
    --data-availability.s3-storage.access-key "<YOUR ACCESS KEY>"
    --data-availability.s3-storage.bucket "<YOUR BUCKET>"
    --data-availability.s3-storage.region "<YOUR REGION>"
    --data-availability.s3-storage.secret-key "<YOUR SECRET KEY>"
    --data-availability.s3-storage.object-prefix "<YOUR OBJECT KEY PREFIX>/"
    --data-availability.s3-storage.discard-after-timeout false
    --data-availability.local-file-storage.enable
    --data-availability.local-file-storage.data-dir /home/user/data/das-data
```

And here's an example of how to use a k8s deployment to run that command:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
    name: das-mirror
spec:
    replicas: 1
    selector:
    matchLabels:
        app: das-mirror
    strategy:
    rollingUpdate:
        maxSurge: 0
        maxUnavailable: 50%
    type: RollingUpdate
    template:
    metadata:
        labels:
        app: das-mirror
    spec:
        containers:
        - command:
        - bash
        - -c
        - |
            mkdir -p /home/user/data/badgerdb
            mkdir -p /home/user/data/syncState
            /usr/local/bin/daserver --data-availability.parent-chain-node-url "<YOUR PARENT CHAIN RPC ENDPOINT>" --data-availability.sequencer-inbox-address "<ADDRESS OF SEQUENCERINBOX ON PARENT CHAIN>" --enable-rest --rest-addr '0.0.0.0' --log-level 3 --data-availability.local-cache.enable --data-availability.rest-aggregator.enable --data-availability.rest-aggregator.urls "http://your-main-das.svc.cluster.local:9877" --data-availability.rest-aggregator.online-url-list "<URL TO LIST OF REST ENDPOINTS>" --data-availability.rest-aggregator.sync-to-storage.eager  --data-availability.rest-aggregator.sync-to-storage.eager-lower-bound-block "BLOCK NUMBER" --data-availability.rest-aggregator.sync-to-storage.state-dir /home/user/data/syncState --data-availability.s3-storage.enable --data-availability.s3-storage.access-key "<YOUR ACCESS KEY>" --data-availability.s3-storage.bucket "<YOUR BUCKET>" --data-availability.s3-storage.region "<YOUR REGION>" --data-availability.s3-storage.secret-key "<YOUR SECRET KEY>" --data-availability.s3-storage.object-prefix "<YOUR OBJECT KEY PREFIX>/" --data-availability.local-file-storage.enable --data-availability.local-file-storage.data-dir /home/user/data/das-data
        image: @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@
        imagePullPolicy: Always
        resources:
            limits:
            cpu: "4"
            memory: 10Gi
            requests:
            cpu: "4"
            memory: 10Gi
        ports:
        - containerPort: 9877
            hostPort: 9877
            protocol: TCP
        volumeMounts:
        - mountPath: /home/user/data/
            name: data
        readinessProbe:
            failureThreshold: 3
            httpGet:
            path: /health/
            port: 9877
            scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
        volumes:
        - name: data
        persistentVolumeClaim:
            claimName: das-mirror
```

## Archive DA servers

Archive DA servers are servers that don't discard any data after expiring. Each DAC should have at the very least one archive DAS to ensure all historical data is available.

To activate the "archive mode" in your DAS, set the parameter `discard-after-timeout` to `false` in your storage method. For example:

```shell
--data-availability.local-db-storage.discard-after-timeout=false
```

:::note

The `s3-storage` **doesn't support** expiration of data, instead that can be enabled from user side by setting `Expiration` as a rule in Lifecycle Configuration of the corresponding S3 bucket. More information available at [AWS Expiring objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-expire-general-considerations.html).

:::

:::note

The `local-file-storage` doesn't discard data after expiring by default, but expiration can be enabled with `enable-expiry`.

:::

Archive servers should make use of the `--data-availability.rest-aggregator.sync-to-storage` options described above to pull in any data that they don't have.

## Helm charts

A helm chart is available at [ArtifactHUB](https://artifacthub.io/packages/helm/offchainlabshelm/das). It supports running a mirror DAS by providing the parameters for your server. Find more information in the [OCL community Helm charts repository](https://github.com/OffchainLabs/community-helm-charts/tree/main/charts/das).

## Testing the DAS

Once the DAS is running, we can test if everything is working correctly using the following methods.

### Test 1: REST health check

The REST interface enabled in the mirror DAS has a health check on the path `/health` which will return `200` if the underlying storage is working, otherwise `503`.

Example:

```shell
curl -I <YOUR REST ENDPOINT>/health
```

## Security considerations

Keep in mind the following information when running the mirror DAS.

For a mirror DAS, using a load balancer is recommended to manage incoming traffic effectively. Additionally, as the REST interface is cacheable, consider deploying a Content Delivery Network (CDN) or caching proxy in front of your REST endpoint. The URL for the REST interface will be publicly known; ensure that it is sufficiently distinct from the RPC endpoint to prevent the latter from being easily discovered.

## What to do next?

Once the DAS is deployed and tested, you'll have to communicate the following information to the chain owner, so they can update the chain parameters and configure the sequencer:

- The https URL for the REST endpoint (e.g., `das.your-chain.io/rest`)

import DASOptionalParameters from '../../node-running/how-tos/data-availability-committee/partials/_das-optional-parameters.mdx';
import DASMetrics from '../../node-running/how-tos/data-availability-committee/partials/_das-metrics.mdx';

## Optional parameters

<DASOptionalParameters />

## Metrics

<DASMetrics />

---

## .mdx (data-availability-committees/04-configure-dac.mdx)
---
title: 'How to configure the Data Availability Committee (DAC) in your chain'
description: This how-to will help you configure the DAC in your chain.
target_audience: 'Developers and node operators deploying and maintaining AnyTrust-based chains.'
author: TheGreatSoshiant, jose-franco
sme: TheGreatSoshiant, jose-franco
sidebar_position: 4
content_type: how-to
---

<p>
  <a data-quicklook-from="arbitrum-anytrust-protocol">AnyTrust</a> chains rely on an external Data
  Availability Committee (DAC) to store data and provide it on-demand instead of using its{' '}
  <a data-quicklook-from="parent-chain">parent chain</a> as the Data Availability (DA) layer. The
  members of the DAC run a Data Availability Server (DAS) to handle these operations. Once the DA
  servers are running, the chain needs to be configured with their information to effectively store
  and retrieve data from them.
</p>

In this how-to, you'll learn how to configure the DAC in your chain. Refer to the [Introduction](/run-arbitrum-node/data-availability-committees/01-get-started.mdx) for the full process of running DA servers and configuring the chain.

This how-to assumes that you're familiar with:

- The DAC's role in the AnyTrust protocol. Refer to [Inside AnyTrust](/how-arbitrum-works/08-anytrust-protocol.mdx) for a refresher.
- [Kubernetes](https://kubernetes.io/). The examples in this guide use Kubernetes to containerize your DAS.
- [How to deploy a Data Availability Server (DAS)](/run-arbitrum-node/data-availability-committees/02-deploy-das.mdx). This is needed to understand where the data we'll be handling in this guide comes from.
- The [Foundry toolkit](https://github.com/foundry-rs/foundry)

## Step 0: Prerequisites

Before starting to generate the keyset and configuring the nodes and chain, you'll need to gather the following information from all the DA servers run by the DAC members:

- Public BLS Key
- URL of the RPC endpoint
- URL(s) of the REST endpoint(s)

You should also make sure that at least one DAS is running as an [archive DAS](/run-arbitrum-node/data-availability-committees/02-deploy-das.mdx#archive-da-servers), otherwise the information will not be available after the expiry time.

## Step 1: Generate the keyset and keyset hash with all the information from the servers

### What is a keyset?

The AnyTrust protocol assumes that for the `n` members of the DAC, a minimum of `h` members maintain integrity. So `h` is then the minimum number of trusted committee members on an AnyTrust chain. In scenarios where `k = (n + 1) - h` members of the DAC pledge to grant access to a specific piece of information, these `k` members must sign and attest they have stored the data to be considered successful.

To perform this signing operation, each DAC member must generate their own set of BLS public and private keys. They should do this independently and ensure these keys are random and only used by them. You can find more information about how to generate a BLS pair of keys in [Generating BLS Keys](/run-arbitrum-node/data-availability-committees/deploy-das#step-1-generate-the-bls-keypair).

An AnyTrust chain needs to know all DAC members' public keys to validate the integrity of the data being batched and posted. A _keyset_ is a list of all DAC members' RPC endpoint and BLS public key. Additionally, it also contains information about how many signatures are needed to approve a <a data-quicklook-from="data-availability-certificate">Data Availability Certificate (DACert)</a>, via a special `assumed-honest` parameter (i.e., the `h` parameter we mentioned above). This design lets the chain owner modify the DAC membership over time, and DAC members change their keys if needed. See [Inside AnyTrust](/how-arbitrum-works/08-anytrust-protocol.mdx) for more information.

We use this keyset, and its hash to configure the `SequencerInbox` contract with the valid keyset, and also the batch poster (to request storing information) and full nodes (to request information already stored).

### How to generate a keyset and a keyset hash

Nitro comes with a special tool to generate both the keyset and the keyset hash. To use it, you need to first structure the keyset information in a JSON object with the following structure:

```json
{
    "keyset": {
      "assumed-honest": h,
      "backends": [
        {
          "url": "https://rpc-endpoint-of-member-1/",
          "pubkey":"PUBLIC_KEY_OF_MEMBER_1"
        },
        {
          "url": "https://rpc-endpoint-of-member-2/",
          "pubkey":"PUBLIC_KEY_OF_MEMBER_2"
        },

        ...

        {
          "url": "https://rpc-endpoint-of-member-n/",
          "pubkey":"PUBLIC_KEY_OF_MEMBER_N"
        }
      ]
    }
}
```

The JSON fields represent the following:

- `assumed_honest` is the amount of members that we assume are honest from the `n` members of the DAC. This is the `h` variable we mentioned in the previous section.
- `backends` contain information about each member of the DAC:
  - `url` contains the RPC endpoint of the DAS run by that member
  - `pubkey` contains the base64-encoded BLS public key used in the DAS run by that member

Once you have the JSON structure, save it into a file, for example, `keyset-info.json`.

Finally, we'll use Nitro's `datool dumpkeyset` utility inside Docker to generate the keyset and keyset hash.

```shell
docker run -v $(pwd):/data/keyset --entrypoint datool @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@ dumpkeyset --conf.file /data/keyset/keyset-info.json
```

This command will output two results: `Keyset` and `KeysetHash`. Save them to use in the next steps.

### Example with mocked-up data

Here's an example that uses mocked-up data:

The JSON file is:

```json
{
  "keyset": {
    "assumed-honest": 2,
    "backends": [
      {
        "url": "http://example",
        "pubkey": "YAbcteVnZLty5qRebeswHKhdjEMVwdou+imSfyrI+yVXHOMdLWA3Nf4DGW9tVry/mhmZqJp01TaYIsREXWdsFe1S5QCNqnddyag5yZ/5Y6GZRqx0BXmHTaxPY5kHrhvGnwxmlJVbUk1xjKRFgxxTdTk3c0AfM3JaeWYTed3avV//KGGdwHC+/Z7XPWmeXCNsGhY75YuoEAK2EwcJvAZK9de6lHEwtyBWvxcmOADxo6siacalEO+OdBL9VtHvG5FqEwbjsdnILAmTcb2YYVgqyq2joW6d/uXQ685hCWWYqC8RLQqTXoyrXEjYLjEEsMe6eRV9rRoBmj5/atB3uOYwixFv7A9YI5YiRjw2MfoB4rQnJAkhW4AJQiwWcV2+3lkJBg=="
      },
      {
        "url": "http://example",
        "pubkey": "YAg1+ZXyR48kiS0FDaoon4trnBsYW80oUy+I1hDCZCotxvNQl0AjbTPD4tkTaqsX+BnIxnEpO7ondxd2Lo0cH3usnhfdKNKTmpWbs45QD5wRw4zrvEJuLeqXxAF1plXRdACubHX/SeiEx5RpJJ5wlTJYhUtk+oRFxYWtRdxtxpdVAcavfP9wdCAsaH+Ke/GjrBkmiXVfIyJ1tMhCGxpWaem5BMKaKSzflht4OnwLTOc2kA3k2MY8X4WmXLRK80vvhArO+Eq3X0TEyRN2ELaBB6/zu9zBkRnHqSfBFbe5v7J9hcUA7nfRPsWpejrmv1HTtwpVAuhBbee1646f7uN2QRyjXIp/P1l8dgZXjPlqRxXOWjXPSOOcCh+qLe4i105oGQ=="
      }
    ]
  }
}
```

And when running the command we obtain:

```shell
$ docker run -v $(pwd):/data/keyset --entrypoint datool @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@ dumpkeyset --conf.file /data/keyset/keyset-info.json
Keyset: 0x0000000000000002000000000000000201216006dcb5e56764bb72e6a45e6deb301ca85d8c4315c1da2efa29927f2ac8fb25571ce31d2d603735fe03196f6d56bcbf9a1999a89a74d5369822c4445d676c15ed52e5008daa775dc9a839c99ff963a19946ac740579874dac4f639907ae1bc69f0c6694955b524d718ca445831c5375393773401f33725a79661379dddabd5fff28619dc070befd9ed73d699e5c236c1a163be58ba81002b6130709bc064af5d7ba947130b72056bf17263800f1a3ab2269c6a510ef8e7412fd56d1ef1b916a1306e3b1d9c82c099371bd9861582acaada3a16e9dfee5d0ebce61096598a82f112d0a935e8cab5c48d82e3104b0c7ba79157dad1a019a3e7f6ad077b8e6308b116fec0f58239622463c3631fa01e2b4272409215b8009422c16715dbede5909060121600835f995f2478f24892d050daa289f8b6b9c1b185bcd28532f88d610c2642a2dc6f3509740236d33c3e2d9136aab17f819c8c671293bba277717762e8d1c1f7bac9e17dd28d2939a959bb38e500f9c11c38cebbc426e2dea97c40175a655d17400ae6c75ff49e884c79469249e70953258854b64fa8445c585ad45dc6dc6975501c6af7cff7074202c687f8a7bf1a3ac192689755f232275b4c8421b1a5669e9b904c29a292cdf961b783a7c0b4ce736900de4d8c63c5f85a65cb44af34bef840acef84ab75f44c4c9137610b68107aff3bbdcc19119c7a927c115b7b9bfb27d85c500ee77d13ec5a97a3ae6bf51d3b70a5502e8416de7b5eb8e9feee376411ca35c8a7f3f597c7606578cf96a4715ce5a35cf48e39c0a1faa2dee22d74e6819
KeysetHash: 0xfdca3e4e2de25f0a56d0ced68fd1cc64f91b20cde67c964c55105477c02f49be
```

## Step 2: Update the `SequencerInbox` contract

Once we have the keyset and its hash, we can configure the `SequencerInbox` contract so it accepts DACerts signed by the DAC members.

The `SequencerInbox` can be configured with the new keyset by invoking the [setValidKeyset](https://github.com/OffchainLabs/nitro-contracts/blob/dcc51066b26b84cb157cbeba2f9f492ab33f9093/src/bridge/SequencerInbox.sol#L743) method. Note that only the chain owner can call this method.

Here's an example of how to use Foundry to configure the `SequencerInbox` with the keyset generated in the previous step:

```shell
cast send --rpc-url $PARENT_CHAIN_RPC --private-key $CHAIN_OWNER_PRIVATE_KEY $SEQUENCERINBOX_ADDRESS "setValidKeyset(bytes)" 0x0000000000000002000000000000000201216006dcb5e56764bb72e6a45e6deb301ca85d8c4315c1da2efa29927f2ac8fb25571ce31d2d603735fe03196f6d56bcbf9a1999a89a74d5369822c4445d676c15ed52e5008daa775dc9a839c99ff963a19946ac740579874dac4f639907ae1bc69f0c6694955b524d718ca445831c5375393773401f33725a79661379dddabd5fff28619dc070befd9ed73d699e5c236c1a163be58ba81002b6130709bc064af5d7ba947130b72056bf17263800f1a3ab2269c6a510ef8e7412fd56d1ef1b916a1306e3b1d9c82c099371bd9861582acaada3a16e9dfee5d0ebce61096598a82f112d0a935e8cab5c48d82e3104b0c7ba79157dad1a019a3e7f6ad077b8e6308b116fec0f58239622463c3631fa01e2b4272409215b8009422c16715dbede5909060121600835f995f2478f24892d050daa289f8b6b9c1b185bcd28532f88d610c2642a2dc6f3509740236d33c3e2d9136aab17f819c8c671293bba277717762e8d1c1f7bac9e17dd28d2939a959bb38e500f9c11c38cebbc426e2dea97c40175a655d17400ae6c75ff49e884c79469249e70953258854b64fa8445c585ad45dc6dc6975501c6af7cff7074202c687f8a7bf1a3ac192689755f232275b4c8421b1a5669e9b904c29a292cdf961b783a7c0b4ce736900de4d8c63c5f85a65cb44af34bef840acef84ab75f44c4c9137610b68107aff3bbdcc19119c7a927c115b7b9bfb27d85c500ee77d13ec5a97a3ae6bf51d3b70a5502e8416de7b5eb8e9feee376411ca35c8a7f3f597c7606578cf96a4715ce5a35cf48e39c0a1faa2dee22d74e6819
```

## Step 3: Craft the new configuration for the batch poster

To configure the batch poster, we'll use the JSON structure we created in Step 1. This will allow the batch poster to send RPC requests to all the DA servers (to store the information of the transactions being included in the next batch), craft the DACert, and store it in the `SequencerInbox`.

The configuration to enable the DAC in the batch poster looks like this:

```json
{
  ...

  "data-availability": {
    "enable": true,
    "rpc-aggregator": {
      "enable": true,
      "assumed-honest": h,
      "backends": [
        {
          "url": "https://rpc-endpoint-of-member-1/",
          "pubkey":"PUBLIC_KEY_OF_MEMBER_1"
        },
        {
          "url": "https://rpc-endpoint-of-member-2/",
          "pubkey":"PUBLIC_KEY_OF_MEMBER_2"
        },

        ...

        {
          "url": "https://rpc-endpoint-of-member-n/",
          "pubkey":"PUBLIC_KEY_OF_MEMBER_N"
        }
      ]
    }
  },

  ...
}
```

The following parameters are used:

- `data-availability.enable`: tells the batch poster to handle information stored in a DAC
- `data-availability.rpc-aggregator`: includes information of the RPC endpoints of all the DA servers run by DAC members.
  - `enable`: tells the batch poster that the RPC aggregator will be used
  - `assumed` and `backends`: include information from the DA servers (following the same format as specified in Step 1)

Once the configuration is in place, you can restart your batch poster so it begins communicating with the DA servers to store transaction data, while storing the DACert in the `SequencerInbox`.

## Step 4: Craft the new configuration for your chain's nodes

Finally, we also need to configure all other nodes so they can communicate with the DAC. To do that, we'll also use the JSON structure we created in Step 1.

The configuration to enable the DAC in a full node looks like this:

```json
{
  ...

  "data-availability": {
    "enable": true,
    "rest-aggregator": {
      "enable": true,
      "urls": [
          "https://rest-endpoint-of-member-1/",
          "https://rest-endpoint-of-member-2/",
          ...
          "https://rest-endpoint-of-member-n/",
      ],
      "online-url-list": "https://url-of-list-of-rest-endpoints"
    }
  },

  ...
}
```

The following parameters are used:

- `data-availability.enable`: tells the node to query information from the DAC
- `data-availability.rest-aggregator`: includes information on the REST endpoints of all the DA servers run by DAC members.
  - `enable`: tells the node that the REST aggregator will be used
  - `urls` or `online-url-list`: usually only one of these is used, although both parameters can be used and the information will be aggregated together. `urls` is a list of all REST endpoints of the DA servers, and `online-url-list` is a URL to a list of URLs of the REST endpoints of the DA servers.

Once the configuration is in place, you can restart your node so it begins communicating with the DA servers to retrieve transaction data.

---

## .mdx (more-types/01-run-archive-node.mdx)
---
title: 'How to run an archive node'
description: Learn how to run an Arbitrum archive node on your local machine
author: jason-wanxt
sidebar_position: 3
content_type: how-to
---

An Arbitrum **archive node** is a full node that maintains an archive of historical chain states. This how-to walks you through the process of configuring an archive node on your local machine so that you can query both pre-Nitro and post-Nitro state data.

:::caution

**Most users won't need to configure an archive node**. This node type is great for a small number of use cases––for example if you need to process historical data.

:::

### Before we begin

Before the Nitro upgrade, Arbitrum One ran on the Classic stack for about one year (before block height 22207817). Although the Nitro chain uses the latest snapshot of the Classic chain's state as its genesis state, **the Nitro stack can serve all but six RPC requests for pre-Nitro blocks. Full details are outlined [here](/run-arbitrum-node/more-types/03-run-classic-node.mdx#do-you-need-to-run-a-classic-node).**.

Running an Arbitrum One **full node** in **archive mode** lets you access both pre-Nitro and post-Nitro blocks, but it requires you to run **both Classic and Nitro nodes** together. You may not need to do this, depending on your use case:

| Use case                                                                        | Required node type(s)                                     | Docs                                                                                                 |
| ------------------------------------------------------------------------------- | --------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- |
| Access the **Arbitrum network** without running your own node                   | Fully managed by third-parties, exposed via RPC endpoints | [RPC endpoints and providers](/build-decentralized-apps/reference/01-node-providers.mdx)             |
| Run an **archive node** for **Arbitrum Sepolia (testnet)** or **Arbitrum Nova** | Full node (Nitro)                                         | [How to run a full node (Nitro)](/run-arbitrum-node/02-run-full-node.mdx)                            |
| Send **post-Nitro** archive requests                                            | Full node (Nitro)                                         | [How to run a full node (Nitro)](/run-arbitrum-node/02-run-full-node.mdx)                            |
| Send **pre-Nitro** archive requests                                             | Full node (Classic)                                       | [How to run a full node (Classic, pre-Nitro)](/run-arbitrum-node/more-types/03-run-classic-node.mdx) |
| Send **post-Nitro** _and_ **pre-Nitro** archive requests                        | Full node (Nitro) _and_ full node (Classic)               | That's what this how-to is for; you're in the right place.                                           |

### System requirements

:::caution
As of May 2024, archive node snapshots for Arbitrum One, Arbitrum Nova, and Arbitrum Sepolia are no longer being updated on https://snapshot-explorer.arbitrum.io/ due to accelerated database and state growth. Teams who use these publicly available archive snapshots will need to wait longer than usual for their nodes to sync up.

The Offchain Labs team is actively exploring and working on solutions to address this and will provide an update as soon as possible. In the meantime, the Offchain Labs team continues to recommend that teams periodically create their own snapshots by stopping one of their archive nodes and backing up their database.
:::

:::caution

The minimum storage requirements will change as the Nitro chains grow (growing rates are specified below). We recommend exceeding the minimum requirements as much as you can to minimize risk and maintenance overhead.

:::

1. **RAM:** 16GB+ for Nitro and 32GB+ for Classic
2. **CPU:** 4+ core CPU
3. **Storage (last updated on April 2024):**
   - Arbitrum One: 9.7TB SSD, currently growing at a rate of about 850GB per month
   - Arbitrum Nova: 4.3TB SSD, currently growing at a rate of about 1.8TB GB per month
4. **Docker images:** We'll specify these in the below commands; you don't need to download them manually.
   - Latest Docker image for **Arbitrum One Nitro**: <code>@@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@</code>
   - Latest Docker image for **Arbitrum One Classic**: <code>@@latestClassicNodeImage=offchainlabs/arb-node:v1.4.5-e97c1a4@@</code>
5. **Database snapshots:**
   - Nitro database snapshot
     - Use the parameter `--init.url=` on the first startup to initialize the Nitro database (you can find a list of snapshots [here](https://snapshot-explorer.arbitrum.io/)). Example: <code>--init.url="@@arbOneNitroArchiveSnapshot=https://snapshot.arbitrum.foundation/arb1/nitro-archive.tar@@"</code>
   - Arbitrum One Classic database snapshot
     - Download the latest Arbitrum One Classic database snapshot at [@@arbOneClassicArchiveSnapshot=https://snapshot.arbitrum.foundation/arb1/classic-archive.tar@@](@@arbOneClassicArchiveSnapshot=https://snapshot.arbitrum.foundation/arb1/classic-archive.tar@@) and place it in the mounted point directory
     - Note that other chains don't have Classic blocks and thus don't require an initial genesis database.
   - Snapshot Explorer
     - You can find more snapshots on our [snapshot explorer](https://snapshot-explorer.arbitrum.io/)

### Review and configure ports

<!-- todo: explain why these ports are important and what action needs to be taken - do rules need to be configured? -->
<!-- todo: format into a table  -->

- RPC: `8547`
- Sequencer Feed: `9642`
- WebSocket: `8548`

### Review and configure parameters

| Arbitrum Nitro                                             | Arbitrum Classic                            | Description                                                                                                                                                                                       |
| ---------------------------------------------------------- | ------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `--parent-chain.connection.url=<Layer 1 Ethereum RPC URL>` | `--l1.url=<Layer 1 Ethereum RPC URL>`       | Provide an standard L1 node RPC endpoint that you run yourself or from a third-party node provider (see [RPC endpoints and providers](/build-decentralized-apps/reference/01-node-providers.mdx)) |
| `--chain.id=<L2 chain ID>`                                 | `--l2.chain-id=<L2 Chain ID>`               | See [RPC endpoints and providers](/build-decentralized-apps/reference/01-node-providers.mdx) for a list of Arbitrum chains and the respective child chain IDs                                     |
| `--execution.caching.archive`                              | `--node.caching.archive`                    | Required for running an **Arbitrum One Nitro** archival node and retains past block state                                                                                                         |
| -                                                          | `--node.cache.allow-slow-lookup`            | Required for running an **Arbitrum One Classic** archival node. When this option is present, it will load old blocks from disk if not in memory cache.                                            |
| -                                                          | `--core.checkpoint-gas-frequency=156250000` | Required for running an **Arbitrum One Classic** archival node.                                                                                                                                   |
|                                                            |

### Run the Docker image(s)

<!-- This is the procedure part, so we can focus on steps to take and move "conceptual information" into a dedicated concept doc -->
<!-- Arbitrum One has been upgraded to Nitro, the latest Arbitrum tech stack; "Arbitrum Classic" is our term for the old, pre-Nitro tech stack. The Nitro node databases have the raw data of all blocks, including pre-Nitro blocks. However, Nitro nodes cannot execute anything on pre-Nitro blocks. Arbitrum Nova started as a Nitro chain, so it has no classic blocks. -->

When running a Docker image, an external volume should be mounted to persist the database across restarts. The mount point should be `/home/user/.arbitrum/mainnet`.

<!-- todo: does the user need to do anything to configure the mount point, or is this handled by the below docker commands? -->

To run both Arbitrum Nitro and/or Arbitrum Classic in archive mode, follow one or more of the below examples:

- **Arbitrum One Nitro archive node**:
  ```shell
  docker run --rm -it -v /some/local/dir/arbitrum:/home/user/.arbitrum -p 0.0.0.0:8547:8547 -p 0.0.0.0:8548:8548 @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@ --parent-chain.connection.url https://l1-node:8545 --chain.id=42161 --http.api=net,web3,eth --http.corsdomain=* --http.addr=0.0.0.0 --http.vhosts=* --execution.caching.archive
  ```
- **Arbitrum One Classic archive node**:
  ```shell
  docker run --rm -it -v /some/local/dir/arbitrum-mainnet/:/home/user/.arbitrum/mainnet -p 0.0.0.0:8547:8547 -p 0.0.0.0:8548:8548 @@latestClassicNodeImage=offchainlabs/arb-node:v1.4.5-e97c1a4@@ --l1.url=https://l1-node:8545/ --node.chain-id=42161 --l2.disable-upstream --node.cache.allow-slow-lookup --core.checkpoint-gas-frequency=156250000 --core.lazy-load-core-machine
  ```
- **Arbitrum One Nitro archive node with forwarding classic execution support**:
  ```shell
  docker run --rm -it -v /some/local/dir/arbitrum:/home/user/.arbitrum -p 0.0.0.0:8547:8547 -p 0.0.0.0:8548:8548 @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@ --parent-chain.connection.url https://l1-node:8545 --chain.id=42161 --execution.rpc.classic-redirect=<classic node RPC> --http.api=net,web3,eth --http.corsdomain=* --http.addr=0.0.0.0 --http.vhosts=* --execution.caching.archive
  ```

Note that the above commands both map to port `8547` on their hosts. To run both on the same host, you should edit those mapping to different ports and specify your Classic node RPC URL as `<classic node RPC>` in your Nitro start command. To verify the connection health of your node(s), see [Docker network between containers - Docker Networking Example](https://www.middlewareinventory.com/blog/docker-network-example/).

#### A note on permissions

The Docker image is configured to run as non-root `UID 1000`. If you're running in Linux and you're getting permission errors when trying to run the Docker image, run this command to allow all users to update the persistent folders, replacing `arbitrum-mainnet` as needed:

```shell
mkdir /some/local/dir/arbitrum-mainnet
chmod -fR 777 /some/local/dir/arbitrum-mainnet
```

### Optional parameters

Both Nitro and Classic have multiple other parameters that can be used to configure your node. For a full comprehensive list of the available parameters, use the flag `--help`.

<!-- todo: FAQ -->

### Troubleshooting

If you run into any issues, visit the [node-running troubleshooting guide](/run-arbitrum-node/06-troubleshooting.mdx).

---

## .mdx (more-types/02-run-validator-node.mdx)
---
title: 'How to run a validator'
description: Learn how to run an Arbitrum validator node
author: jose-franco
sidebar_position: 4
content_type: how-to
---

Validators are nodes that choose to participate in the rollup protocol to advance the state of the chain securely. Since the activation of <a data-quicklook-from="bold">BoLD</a>, chains can now choose to make validation permissionless. You can learn more in the [BoLD introduction](/how-arbitrum-works/bold/gentle-introduction.mdx).

This page describes the different strategies a validator may follow and provides instructions on how to run a validator for an Arbitrum chain.

This how-to assumes that you're familiar with the following:

- How to run a full node (see instructions [here](/run-arbitrum-node/02-run-full-node.mdx))
- [How the Rollup protocol works](/how-arbitrum-works/05-validation-and-proving/02-rollup-protocol.mdx)
- [How BoLD works](/how-arbitrum-works/bold/bold-technical-deep-dive.mdx#how-bold-uses-ethereum), if you're running a validator for a chain that has BoLD activated

## Validation strategies

Validators can be configured to follow a specific validation strategy. Here we describe what strategies are available in Nitro:

| Strategy           | Description                                                                                                                                                                                                                                                                                                                                                     | Gas usage                                                                                              |
| ------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ |
| **`Defensive`**    | This validator will follow the chain and if it's local state disagrees with an onchain assertion, this validator will post a bond and create a challenge to defend the chain                                                                                                                                                                                    | Only acts if a bad assertion is found                                                                  |
| **`StakeLatest`**  | This validator will initially bond on the latest correct assertion found, and then move the bond whenever new correct assertions are created. It will also challenge any bad assertions that it finds (this strategy is only available in pre-BoLD chains)                                                                                                      | Gas used every time a new assertion is created                                                         |
| **`ResolveNodes`** | This validator will stay bonded on the latest assertion found, resolve any unconfirmed assertions, and it will challenge any bad assertions that it finds                                                                                                                                                                                                       | Gas used every time a new assertion is created and to resolve unconfirmed assertions                   |
| **`MakeNodes`**    | This validator continuously creates new assertions, resolves any unconfirmed assertions, and challenges bad assertions found. Note that if there is more than one `MakeNodes` validator running, they might all try to create a new assertion simultaneously. In that case, only one will be successful, while the others will have their transactions reverted | Gas used to create new assertions, move the bond to the latest one, and resolve unconfirmed assertions |

### The watchtower strategy

One more validation strategy is available for all types of nodes: `watchtower`. This strategy is enabled by default in all nodes (full and archive), and it doesn't require a wallet, as it never takes any action onchain.

A node in `watchtower` mode will immediately log an error if an onchain assertion deviates from the locally computed chain state.

```shell
found incorrect assertion in watchtower mode
```

To verify that the watchtower mode is enabled, this line should appear in the logs:

```shell
INFO [09-28|18:43:49.367] running as validator                     txSender=nil actingAsWallet=nil whitelisted=false strategy=Watchtower
```

Additionally, the following logs indicate whether all components are working correctly:

- The log line `validation succeeded` shows that the node is validating chain blocks successfully
- The log line `found correct assertion` shows that the node is finding assertions on the parent chain successfully

Watchtower mode adds a small amount of execution and memory overhead to your node. You can deactivate this mode using the parameter `--node.staker.enable=false`.

## How to run a validator node

This section explains how to configure your node to act as a validator.

### Step 0: prerequisites

A validator node is a regular full node with validation enabled, so you'll have to know how to configure a full node. You can find instructions [here](/run-arbitrum-node/02-run-full-node.mdx).

Additionally, you'll need a wallet with enough funds to perform actions onchain and enough tokens to bond. Keep in mind that:

- The token used to perform actions onchain is the native token of the parent chain (usually `ETH`)
- For chains with BoLD activated, the token used to bond depends on the chain configuration. For Arbitrum One and Arbitrum Nova, the staking token is `WETH`
- For chains that don't have BoLD activated, the token used to bond is the native token of the parent chain (usually `ETH`)

### Step 1: configure and run your validator

On top of the configuration of a regular full node, you'll need to configure the following parameters for it to act as a validator:

| Parameter                                       | Value                                                                 | Description                                                                                                                                                           |
| ----------------------------------------------- | --------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `--node.staker.enable`                          | `true`                                                                | Enables validation                                                                                                                                                    |
| `--node.staker.strategy`                        | `Watchtower`, `Defensive`, `StakeLatest`, `ResolveNodes`, `MakeNodes` | Strategy that your node will use                                                                                                                                      |
| `--node.staker.parent-chain-wallet.private-key` | 0xPrivateKey                                                          | Private key of the wallet used to perform the operations onchain. Use either `private-key` or `password` (below)                                                      |
| `--node.staker.parent-chain-wallet.password`    | Password                                                              | Password of a wallet generated with nitro (see instructions [here](#use-nitro-to-create-a-wallet-for-your-validator)). Use either `private-key` (above) or `password` |
| `--node.bold.enable`                            | true                                                                  | Enables validation with BoLD (not needed if BoLD is not activated, only needed before nitro v3.6.0)                                                                   |

Here's an example of how to run a defensive validator for Arbitrum One:

```shell
docker run --rm -it  -v /some/local/dir/arbitrum:/home/user/.arbitrum @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@ --parent-chain.connection.url=https://l1-mainnet-node:8545 --chain.id=42161 --node.staker.enable --node.staker.strategy=Defensive --node.staker.parent-chain-wallet.password="SOME SECURE PASSWORD" --node.staker.strategy=Defensive
```

### Step 2: verify that your node is running as a validator

To verify that your node is acting as a validator, you can look for the following log line:

```shell
INFO [09-28|18:43:49.367] running as validator                     txSender=0x... actingAsWallet=0x... whitelisted=true strategy=Defensive
```

Note that `strategy` should be the configured strategy. `txSender` and `actingAsWallet` should both be present and not `nil`.

Furthermore, the following logs will indicate that all components are working as intended:

- The log line `validation succeeded` shows that the node is validating chain blocks successfully
- The log line `found correct assertion` shows that the node is finding assertions on the parent chain successfully

## Run a validator for an Arbitrum chain

Validation for Arbitrum chains (Orbit) works the same way as for DAO-governed Arbitrum chains. However, as specified in [How to run a node](/run-arbitrum-node/02-run-full-node.mdx#required-parameters), you need to include the information of the chain when configuring your node by using `--chain.info-json`.

```shell
--chain.info-json=<Orbit chain's info>
```

Additionally, keep in mind that some chains might not have BoLD activated yet, so BoLD-specific parameters will not be needed.

## Advanced features

### Use Nitro to create a wallet for your validator

::::warning Clear passwords in the command line

This section shows how to manage a validator wallet using a password. Like any command that requires passing a password or private key, you should take extra precautions to secure your credentials. Failure to protect your password may compromise your validator wallet.

::::

Nitro includes a tool to create a validator wallet for a specific chain automatically. You can access it by using the option `--node.staker.parent-chain-wallet.only-create-key` and setting a password for the wallet with `--node.staker.parent-chain-wallet.password`.

Here is an example of how to create a validator wallet for Arbitrum One and exit:

```shell
docker run --rm -it  -v /some/local/dir/arbitrum:/home/user/.arbitrum @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@ --parent-chain.connection.url=https://l1-mainnet-node:8545 --chain.id=42161 --node.staker.enable --node.staker.parent-chain-wallet.only-create-key --node.staker.parent-chain-wallet.password="SOME SECURE PASSWORD"
```

The wallet file will be created under the mounted directory inside the `<chain-name>/wallet/` directory (for example, `arb1/wallet/` for Arbitrum One, or `nova/wallet/` for Arbitrum Nova). Be sure to backup the wallet file, as it will be the only way to withdraw the bond when desired.

Once the wallet is created, you can instruct your validator to use it by adding the option `--node.staker.parent-chain-wallet.password="SOME SECURE PASSWORD"` when running your node.

::::tip Use environment variables

If you prefer not to include your password in the command line, you can use environment variables or a secure secrets management solution to supply the password at runtime.

::::

### How to add new validators to the allowlist (Arbitrum chains)

On permissioned validation setups, the set of validators that can act on a given chain is limited to the ones added to the allowlist of validators in the Rollup contract.

Follow these instructions to add a new validator address to the allowlist. Remember that you need to be able to perform admin actions to the chain to complete this operation.

1. Find your `upgradeExecutor` contract address
2. Call the `executeCall` method of the `upgradeExecutor` contract:
   - set the `target` address to your Rollup contract's address
   - set the `targetCalldata` to `0xa3ffb772{Your new allowlist validator address}` (`0xa3ffb772` is the signature of `setValidator(address[],bool[])`). For example, if you want to add the address `0x1234567890123456789012345678901234567890`, your `targetCalldata` should be `0xa3ffb7721234567890123456789012345678901234567890`.
3. Call your Rollup contract's `isValidator(address)` and check the result

After performing this operation, the new validator will be able to run a validator node to participate in the chain.

---

## .mdx (more-types/03-run-classic-node.mdx)
---
title: 'How to run a Classic node'
description: Learn how to run an classic node on your local machine.
sidebar_position: 2
content_type: how-to
---

### Do you need to run a Classic node?

Arbitrum One has been upgraded to Nitro, the latest Arbitrum tech stack. "Arbitrum Classic" is our term for the old, pre-Nitro tech stack. The Nitro node databases have the raw data of all blocks, including pre-Nitro blocks. However, Nitro nodes cannot execute anything on pre-Nitro blocks. You need an Arbitrum Classic archive node to execute data on pre-Nitro blocks.

When querying archive blocks, the following commands can only be handled by Arbitrum Classic nodes:

- `eth_call`
- `eth_estimateGas`
- `eth_getBalance`
- `eth_getCode`
- `eth_getTransactionCount`
- `eth_getStorageAt`

🔉 Note that Arbitrum Nova and Arbitrum Sepolia started as a Nitro chain, so they don't have classic blocks.

### Required artifacts

- Latest Docker Image: <code>@@latestClassicNodeImage=offchainlabs/arb-node:v1.4.5-e97c1a4@@</code>
- Latest classic snapshot for Arbitrum One: [@@arbOneClassicArchiveSnapshot=https://snapshot.arbitrum.foundation/arb1/classic-archive.tar@@](@@arbOneClassicArchiveSnapshot=https://snapshot.arbitrum.foundation/arb1/classic-archive.tar@@)

### Required parameters

- `--l1.url=<Layer 1 Ethereum RPC URL>`
  - Must provide standard Ethereum node RPC endpoint.
- `--node.chain-id=<L2 Chain ID>`
  - Must use `42161` for Arbitrum One

### Important ports

- RPC: `8547`
- WebSocket: `8548`

### Putting it all together

- When running docker image, an external volume should be mounted to persist the database across restarts. The mount point should be `/home/user/.arbitrum/mainnet`.
- Here is an example of how to run a classic archive node for Arbitrum One (only needed for archive requests on pre-Nitro blocks, so you'll probably want to enable the archive mode in your nitro node as well):

```shell
docker run --rm -it  -v /some/local/dir/arbitrum-mainnet/:/home/user/.arbitrum/mainnet -p 0.0.0.0:8547:8547 -p 0.0.0.0:8548:8548 @@latestClassicNodeImage=offchainlabs/arb-node:v1.4.5-e97c1a4@@ --l1.url=https://l1-node:8545 --node.chain-id=42161 --l2.disable-upstream
```

### Note on permissions

- The Docker image is configured to run as non-root UID 1000. This means if you are running in Linux and you are getting permission errors when trying to run the docker image, run this command to allow all users to update the persistent folders.

```shell
mkdir /some/local/dir/arbitrum-mainnet
chmod -fR 777 /some/local/dir/arbitrum-mainnet
```

### Optional parameters

We show here a list of the parameters that are most commonly used when running a Classic node. You can also use the flag `--help` for a full comprehensive list of the available parameters.

- `--core.cache.timed-expire`
  - Defaults to `20m`, or 20 minutes. Age of oldest blocks to hold in cache so that disk lookups are not required
- `--node.rpc.max-call-gas`
  - Maximum amount of gas that a node will use in call, default is `5000000`
- `--core.checkpoint-gas-frequency`
  - Defaults to `1000000000`. Amount of gas between saving checkpoints to disk. When making archive queries node has to load closest previous checkpoint and then execute up to the requested block. The farther apart the checkpoints, the longer potential execution required. However, saving checkpoints more often slows down the node in general.
- `--node.cache.allow-slow-lookup`
  - When this option is present, will load old blocks from disk if not in memory cache
  - If archive support is desired, recommend using `--node.cache.allow-slow-lookup --core.checkpoint-gas-frequency=156250000`
- `--node.rpc.tracing.enable`
  - Note that you also need to have a database populated with an archive node if you want to trace previous transactions
  - This option enables the ability to call a tracing API which is inspired by the parity tracing API with some differences
    - Example: `curl http://arbnode -X POST -H "Content-Type: application/json" -d '{"jsonrpc":"2.0","method":"arbtrace_call","params":[{"to": "0x6b175474e89094c44da98b954eedeac495271d0f","data": "0x70a082310000000000000000000000006E0d01A76C3Cf4288372a29124A26D4353EE51BE"},["trace"], "latest"],"id":67}'`
  - The `trace_*` methods are renamed to `arbtrace_*`, except `trace_rawTransaction` is not supported
  - Only `trace` type is supported. `vmTrace` and `stateDiff` types are not supported
  - The self-destruct opcode is not included in the trace. To get the list of self-destructed contracts, you can provide the `deletedContracts` parameter to the method

### Feed relay

- Arbitrum classic does not communicate with Nitro sequencer, so the classic relay is no longer used.

### Why classic nodes serve RPC methods so slowly?

When you call RPC methods on Arbitrum Classic nodes, the request time may take a long time, because classic nodes must perform multiple sequential reads from the database to reconstruct blockchain state for state-accessing operations. Nodes need to load execution cursors from disk, rebuild the machine state from checkpoints, and execute transactions to reach the target state—all of which involve expensive disk I/O operations.

**Solutions:**

- Use sequential block access: Query consecutive blocks (N, N+1, N+2) instead of random blocks to benefit from caching

- Hardware optimization: Use NVMe SSDs with low latency (NVMe PCIe 4.0 or higher)

---

## .mdx (more-types/04-run-split-validator-node.mdx)
---
title: 'Run a split validator node'
description: How to run a split validator node
author: jason-w123
sidebar_position: 4
content_type: how-to
---

## Running split validators for Arbitrum chains

Split validators separate the validation work to a stateless worker, which provides several key benefits:

- **Resource management**: Easier to scale and manage compute resources independently
- **Fault isolation**: Prevents database corruption if the validation worker crashes (e.g., due to OOM errors)
- **Flexibility**: Allows running multiple validation workers for horizontal scalability

This guide explains how to set up a split validator configuration for Arbitrum chains by running a Nitro node (staker) and a validation node separately.

Before you read this doc, please ensure you have already walked through [run a validator](/run-arbitrum-node/more-types/02-run-validator-node.mdx) docs to understand the basics of a validator node.

### Prerequisites

- Docker or Kubernetes with Helm installed
- Staker private key
- Chain information JSON for your Arbitrum chain

### Docker deployment guide

#### Step 1: Set up the validation node

First, generate a JWT secret for secure communication:

```bash
xxd -l 32 -ps -c 40 /dev/urandom > /tmp/nitro-val.jwt
```

Start the validation node with the JWT secret:

```bash
docker run --rm -it \
  --entrypoint nitro-val \
  -p 0.0.0.0:5200:5200 \
  @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@ \
  --auth.addr 127.0.0.1 \
  --auth.origins 0.0.0.0 \
  --auth.jwtsecret /tmp/nitro-val.jwt \
  --auth.port 5200
  --metrics \
  --metrics-server.addr=0.0.0.0 \
  --metrics-server.port=6070
```

The validation node will listen on port 5200, which will also enable the metrics server on port 6070.

#### Step 2: Set up the staker node

Copy the JWT secret to your mount directory:

```bash
cp /tmp/nitro-val.jwt /some/local/dir/arbitrum
```

Start the staker node with the following command, connecting it to your validation node:

```bash
docker run --rm -it \
  -v /some/local/dir/arbitrum:/home/user/.arbitrum \
  @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@ \
  --parent-chain.connection.url=<parent-chain-endpoint> \
  --node.staker.enable=true \
  --node.staker.strategy=MakeNodes \
  --node.staker.parent-chain-wallet.private-key=<staker_private_key> \
  --chain.info-json=<Your_chain_info> \
  --execution.forwarding-target=<forwarding_target> \
  --node.block-validator.validation-server-configs-list="[{\"jwtsecret\":\"/home/user/.arbitrum/nitro-val.jwt\",\"url\":\"ws://Your_validation_address\"}]"
```

Replace the placeholders with your specific values:

- `<parent-chain-endpoint>`: Your parent chain RPC endpoint
- `<staker_private_key>`: Your staker's private key
- `<Your_chain_info>`: Chain information JSON
- `<forwarding_target>`: Your forwarding node URL (usually is the sequencer endpoint)
- `Your_validation_address`: Address of your validation node (including port)

### Kubernetes deployment with Helm

Arbitrum provides a [community Helm chart](https://github.com/OffchainLabs/community-helm-charts) for Kubernetes deployment.

#### Step 1: Create validation node configuration

Create a file named `validation_values.yaml`:

```yaml
configmap:
  data:
    parent-chain:
      id: 1 # Use appropriate parent chain ID
      connection:
        url: 'https://your-parent-chain-rpc'
    execution:
      forwarding-target: 'https://your-forwarding-node'
    node:
      staker:
        enable: true
        strategy: 'MakeNodes'
        parent-chain-wallet:
          private-key: 'your-staker-private-key'
    chain:
      name: 'Your Chain Name'
      id: 42161 # Your chain ID
      info-json: '[Your chain info JSON]'

jwtSecret:
  enabled: true
  value: 'Your 32 bytes hex jwt'

validator:
  enabled: true
  splitvalidator:
    deployments:
      - name: 'current'
```

#### Step 2: Deploy the validation node

```bash
helm install nitro-validator offchainlabs/nitro --values validation_values.yaml
```

### Monitoring and maintenance

- Monitor your validator and staker logs regularly:

  ```bash
  # For Docker
  docker logs -f <container_id>

  # For Kubernetes
  kubectl logs -f <POD>
  ```

- Check validator and staker status through the Arbitrum dashboard or API

### Additional configurations for validation nodes

To get a full list of parameters for the validation node, you can run the following command:

```bash
docker run --rm -it --entrypoint nitro-val  @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@  --help
```

### Additional configurations for helm charts

For more advanced helm chart configurations, please refer to the Arbitrum community Helm Chart [README](https://github.com/OffchainLabs/community-helm-charts/blob/main/charts/nitro/README.md)

### Monitoring

To check if your validation node is running correctly, you can check the `rpc_duration_validation_validate_success_count` metric in your metrics server.

This metric is a counter of the number of successful validation calls; if it is increasing, it means your validation node is running correctly.

You can also check where this log (`validation node started`) appears:

- If this log appears on the validation node, it means your validation node is running correctly.
- If this log appears on the staker node (validator node), it means your staker node is still validating itself. It indicates that your staker node is not connecting to the validation node correctly; you need to double-check your configuration.

---

## .mdx (nitro/01-build-nitro-locally.mdx)
---
title: 'How to build Nitro locally (Debian, Ubuntu, MacOS)'
description: This how-to provides step-by-step instructions for building Nitro locally using Docker on Debian, Ubuntu, or MacOS.
author: amsanghi
sidebar_position: 7
content_type: how-to
---

Arbitrum Nitro is the software that powers all Arbitrum chains. This how-to shows how you can build a Docker image, or binaries, directly from Nitro's source code. If you want to run a node for one of the Arbitrum chains, however, it is recommended that you use the docker image available on DockerHub, as explained in [How to run a full node](/run-arbitrum-node/02-run-full-node.mdx).

This how-to assumes that you're running one of the following operating systems:

- [Debian 11.7 (arm64)](https://cdimage.debian.org/cdimage/archive/11.7.0/)
- [Ubuntu 22.04 (amd64)](https://old-releases.ubuntu.com/releases/22.04.2/)
- [MacOS Sonoma 14.3](https://developer.apple.com/documentation/macos-release-notes/macos-14_3-release-notes).

## Build a Docker image

### Step 1. Configure [Docker](https://docs.docker.com/engine/install)

#### For [Debian](https://docs.docker.com/engine/install/debian)/[Ubuntu](https://docs.docker.com/engine/install/ubuntu)

```shell
for pkg in docker.io docker-doc docker-compose podman-docker containerd runc; do sudo apt-get remove $pkg; done
# Add Docker's official GPG key:
sudo apt-get update
sudo apt-get install ca-certificates curl gnupg
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg

# Add the repository to Apt sources:
echo \
  "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \
  "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
sudo service docker start
```

:::note

If you are running Ubuntu 22.04, you might get an `Unable to locate package docker-buildx-plugin` error. Try `sudo apt install docker-buildx` instead.

:::

#### For [MacOS](https://docs.docker.com/desktop/install/mac-install/)

Depending on whether your Mac has an Intel processor or Apple silicon, download the corresponding disk image from [Docker](https://docs.docker.com/desktop/install/mac-install/), and move it into your Applications folder.

#### [Optional] Run Docker from a different user

After installing Docker, you might want to be able to run it with your current user instead of root. You can run the following commands to do so.

```shell
sudo groupadd docker
sudo usermod -aG docker $USER
newgrp docker
```

For troubleshooting, check Docker's section in [their documentation](https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user)

### Step 2. Download the Nitro source code

```shell
git clone --branch @@nitroVersionTag=v3.7.2@@ https://github.com/OffchainLabs/nitro.git
cd nitro
git submodule update --init --recursive --force
```

### Step 3. Build the Nitro node Docker image

```shell
docker build . --tag nitro-node
```

That command will build a Docker image called `nitro-node` from the local source.

## Build Nitro's binaries natively

If you want to build the node binaries natively, execute Steps 1-3 of the [Build a Docker image](#build-a-docker-image) section and continue with the steps described here. Notice that even though we are building the binaries outside of Docker, it is still used to help build some WebAssembly components.

### Step 4. Configure prerequisites

#### For Debian/Ubuntu

```shell
apt install git curl build-essential cmake npm golang clang make gotestsum wabt lld-13 python3
npm install --global yarn
ln -s /usr/bin/wasm-ld-13 /usr/local/bin/wasm-ld
```

#### For MacOS

Install [Homebrew](https://brew.sh/) package manager and add it to your `PATH` environment variable:

```shell
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
echo "export PATH=/opt/homebrew/bin:$PATH" >> ~/.zprofile && source ~/.zprofile
```

:::note

Replace `~/.zprofile` with `~/.bash_profile` if you use bash instead of zsh).

:::

Install essentials:

```shell
brew install git curl make cmake npm wabt llvm lld libusb gotestsum
npm install --global yarn
sudo mkdir -p /usr/local/bin
echo "export PATH=/opt/homebrew/opt/llvm/bin:$PATH" >> ~/.zprofile && source ~/.zprofile
```

### Step 5. Configure node [23](https://github.com/nvm-sh/nvm)

#### For Debian/Ubuntu

```shell
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.3/install.sh | bash
source "$HOME/.bashrc"
nvm install 23
nvm use 23
```

#### For MacOS

```shell
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.3/install.sh | bash
export NVM_DIR="$HOME/.nvm"
[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"
nvm install 23
nvm use 23
```

### Step 6. Configure [Rust](https://www.rust-lang.org/tools/install)

#### Note that you may also need to use `rustup toolchain remove nightly...` to remove other Rust nightly toolchains that are installed

```shell
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source "$HOME/.cargo/env"
rustup install 1.83.0
rustup default 1.83.0
rustup install nightly-2024-08-08
rustup target add wasm32-unknown-unknown --toolchain 1.83.0
rustup target add wasm32-wasi --toolchain 1.83.0
rustup target add wasm32-wasip1 --toolchain 1.83.0
rustup target add wasm32-unknown-unknown --toolchain nightly-2024-08-08
rustup target add wasm32-wasi --toolchain nightly-2024-08-08
rustup target add wasm32-wasip1 --toolchain nightly-2024-08-08
rustup component add rust-src --toolchain nightly-2024-08-08
cargo install cbindgen
```

### Step 7. Install Bison [3.8.2](https://savannah.gnu.org/projects/bison/)

#### For Debian/Ubuntu

```shell
sudo apt-get install bison
```

#### For MacOS

```shell
brew install bison
```

### Step 8. Configure Go [1.24](https://github.com/moovweb/gvm)

#### Install and configure Go

```shell
bash < <(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer)
source "$HOME/.gvm/scripts/gvm"
gvm install go1.24
gvm use go1.24 --default
curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(go env GOPATH)/bin v1.54.2
```

:::note

If you use zsh, replace `bash` with `zsh`.

:::

#### Install foundry version 1.0.0

```shell
curl -L https://foundry.paradigm.xyz | bash
foundryup -i 1.0.0
```

### Step 9. Check dependencies

```shell
./scripts/check-build.sh
```

If this script shows any errors, fix them before proceeding to the next step.

### Step 10. Start build

```shell
make
```

### Step 11. Produce binaries

```shell
make build
```

#### Warnings on MacOS

In MacOS with Apple Silicon, warnings like the following might appear but they will not hinder the compilation process.

```shell
ld: warning: object file was built for newer 'macOS' version (14.4) than being linked (14.0)
```

To silence these warnings, export the following environment variables before building Nitro.

```shell
export MACOSX_DEPLOYMENT_TARGET=$(sw_vers -productVersion)
export CGO_LDFLAGS=-Wl,-no_warn_duplicate_libraries
```

### Step 12. Run your node

To run your node using the generated binaries, use the following command from the `nitro` folder, with your desired parameters

```shell
./target/bin/nitro <node parameters>
```

#### WASM module root error (v2.3.4 or later)

Since v2.3.4, the State Transition Function (STF) contains code that is not yet activated on the current mainnet and testnet chains. Because of that, you might receive the following error when connecting your built node to those chains:

```shell
ERROR[05-21|21:59:17.415] unable to find validator machine directory for the on-chain WASM module root err="stat {WASM_MODULE_ROOT}: no such file or directory"
```

Try add flag:

```shell
--validation.wasm.allowed-wasm-module-roots={WASM_MODULE_ROOT}
```

---

## .mdx (nitro/02-migrate-state-and-history-from-classic.mdx)
---
title: 'How to migrate state and history from a classic (pre-Nitro) node to a Nitro node'
description: This how-to provides step-by-step instructions for migrating the state and history from a classic (pre-Nitro) node to a Nitro node
author: jose-franco
sme: jose-franco
target_audience: 'Node runners interested in creating the full state of the Arbitrum One chain on their own (i.e., without an initial database)'
content_type: how-to
sidebar_position: 11
---

When running a Nitro node for the first time on a chain that produced [classic blocks](/build-decentralized-apps/03-public-chains.mdx#classic-deprecated) in the past (like Arbitrum One), you need to initialize its database to, at least, the state of the chain after executing the last classic block. The common, and recommended, way of doing that is to provide a database snapshot using the `--init.url` option (as mentioned in [How to run a full node (Nitro)](/run-arbitrum-node/02-run-full-node.mdx)). In this how-to we show you an alternative way for doing that, migrating the state and history of the chain from a fully synced classic node.

:::info Is this How-to for you?

As mentioned, the recommended way of initializing a Nitro node is by using a pre-initialized database snapshot with the `--init.url` option. This guide is for those that are interested in re-creating the full state of the chain from the genesis block using their own classic node.

Keep in mind that this process only applies to Arbitrum One. Other Arbitrum chains didn't produce classic blocks in the past, they started as Nitro chains.

:::

## Prerequisites

To successfully migrate the state and history of the chain from a classic (pre-Nitro) node to a Nitro node, you'll need:

- A fully synced classic node: you can find instructions on how to run a classic node in [this page](/run-arbitrum-node/more-types/03-run-classic-node.mdx).
- A clean, uninitialized Nitro node: you can find instructions on how to set up a Nitro node in [this page](/run-arbitrum-node/02-run-full-node.mdx).

## Step 1: Enable export options in your classic node

Launch your classic node with the option `--node.rpc.nitroexport.enable=true`. All exported data will be written to directory "nitroexport" under the classic instance directory (e.g., `${HOME}/.arbitrum/mainnet/nitroexport`). Make sure the classic node has read the entire rollup state.

:::caution Caution

Enabling the export options is only recommended for nodes with no public/external interfaces.

:::

:::info Exported file contents are not deterministic

Exporting the state of your own classic node should produce the same state as using files supplied by the Arbitrum Foundation (i.e., the same genesis blockhash). However, multiple exports of the same state will not necessarily create identical intermediate files. For example, state export is done in parallel, so the order of entries in the file is not deterministic.

:::

## Step 2: Export information from your classic node

### Block & transaction history

These are block headers, transactions and receipts executed in the classic node. Nitro node uses the history to be able to answer simple requests, like `eth_getTransactionReceipt`, from the classic history. The last block in the chain is the only one that affects the genesis block: timestamp is copied from the last block, and `parentHash` is taken from the last block's `blockHash`.

- Call the RPC method `arb_exportHistory` with parameter `"latest"` to initiate history export. It will return immediately.
- Calling `arb_exportHistoryStatus` will return the latest block exported, or an error if the export failed.
- Data will be stored in the directory `nitroexport/nitro/l2chaindata/ancient`.

### Rollup state

The rollup state is exported as a series of JSON files. State read from these JSON files will be added to Nitro's genesis block.

- Call the RPC method `arb_exportState` with parameter `latest` to initiate state export. Unless disconnected, this will only return after the state export is done.
- Data will be stored in the directory `nitroexport/state/<block_number>/`.

### Outbox messages (optional)

This data does not impact consensus and is optional. It allows a Nitro node to provide the information required when executing a withdrawal made on the classic rollup.

- Call the RPC method `arb_exportOutbox` with parameter `"0xffffffffffffffff"` to initiate outbox export. It will return immediately.
- Calling `arb_exportOutboxStatus` will return the latest outbox batch exported, or an error if the export failed.
- Data will be stored in the directory `nitroexport/nitro/classic-msg`.

## Step 3: Initialize your Nitro node importing the exported data

- Place the `l2chaindata` and `classic-msg` (if exported) directories in Nitro's instance directory (e.g. `${HOME}/.arbitrum/arb1-nitro/`).
- Launch the Nitro node with the argument `--init.import-file=/path/to/state/index.json`

:::caution Caution

This state import operation requires more resources than a regular run of a Nitro node.

:::

### Other useful Nitro options

| Flag                       | Description                                                                                                                                                                                                                                                                       |
| -------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `--init.accounts-per-sync` | Allows the node to make partial database writes to hard-disk during initialization, allowing memory to be freed. This should be used if memory load is very high. A reasonable initial value to try would be 100000. Systems with constrained memory might require a lower value. |
| `--init.then-quit`         | Causes the node to quit after initialization is done.                                                                                                                                                                                                                             |
| `--init.force`             | For an already-initialized node, forces the node to recalculate Nitro's genesis block. If the genesis blockhash doesn't match what's in the database, the node will panic.                                                                                                        |

## See also

- [How to run a full node (Nitro)](/run-arbitrum-node/02-run-full-node.mdx)
- [How to run a full node (Classic, pre-Nitro)](/run-arbitrum-node/more-types/03-run-classic-node.mdx)

---

## .mdx (nitro/03-nitro-database-snapshots.mdx)
---
title: 'Nitro database snapshots'
description: This page explains how to supply a snapshot to Nitro and how to create a new snapshot
author: gligneul
sme: gligneul
target_audience: 'Node runners interested in starting Nitro with a database snapshot'
content_type: how-to
sidebar_position: 11
---

Nitro stores the chain state and data in a database in the local filesystem. When starting Nitro for the first time, it will initialize an empty database by default and start processing transactions from Genesis. It takes a long time for the node to sync from Genesis, so starting from a database snapshot is advisable instead. Moreover, for the Arbitrum One chain, you must start from a snapshot because Nitro cannot process transactions from the Classic Arbitrum node.

## Supply the snapshot URL to Nitro

There are multiple ways to supply Nitro with the database snapshot. The most straightforward way is to provide the configuration, so Nitro downloads the snapshot by itself. It is also possible to download the database manually and supply it to Nitro.

## Downloading the latest snapshot

Nitro has a CLI configuration for downloading the latest snapshot from a remote server. Set the flag `--init.latest` to either `archive`, `pruned`, or `genesis`, and Nitro will download the preferred snapshot. You may also change the `--init.latest-base` flag to set the base URL when searching for the latest snapshot.

### How it works

When searching for the latest snapshot, Nitro uses the chain name provided in `--chain.name`. Make sure to set it correctly; otherwise, Nitro might be unable to find the snapshot. Nitro will look for a remote file in `<latest-base>/<chain-name>/latest-<kind>.txt`, where `<kind>` is the option supplied to `--init.latest`. This file should contain either the path or the full URL to the snapshot; if it only contains the path, Nitro will use the `<latest-base>` as the base URL.

After finding the latest snapshot URL, Nitro will download the archive and temporarily store it in the directory specified in `--init.download-path`. Nitro looks for a SHA256 checksum on the remote server and verifies the checksum of the snapshot after finishing the download. (It is possible to disable this feature by setting `--init.validate-checksum` to false.)

The snapshot can be a single archive file or a series of parts. Nitro first tries to download the snapshot as a single archive. In this case, Nitro will look for a checksum file in `<archive-url>.sha256`. If the remote server returns not found (404 status code), Nitro will proceed to download the snapshot in parts. When downloading in parts, Nitro will look for a manifest file in `<archive-url>.manifest.txt` containing each part's name and checksum. In this case, Nitro will download each part in the manifest file and concatenate them into a single archive.

Finally, Nitro decompresses and extracts the snapshot archive, placing it in the database directory. Nitro will delete the archive after extracting it, so if you need to set up multiple nodes with the same snapshot, consider downloading it manually, as explained below.

---

## Downloading the snapshot from a URL

Instead of letting Nitro search for the latest snapshot, you can provide a specific URL to download by setting the flag `--init.url` with the snapshot URL. If the URL points to a remote server, it should start with the `https://` protocol definition. Given the URL, Nitro will download the snapshot as described in the "Downloading the Latest Snapshot" section.

Nitro also supports importing files from the local file system. In this case, you should provide the file path to `--init.url` starting with the prefix `file://` followed by the file path. Beware that when running Nitro inside a Docker container, you must mount a volume containing the provided snapshot using the docker flag `-v` (see [Docker documentation](https://docs.docker.com/reference/cli/docker/container/run/#volume)). Otherwise, the Nitro container running inside Docker won’t be able to find the snapshot in your local filesystem.

---

## Downloading the snapshot manually

It is possible to download the snapshot manually and supply the archive instead of having Nitro download it.

The first step is downloading the snapshot. The command below illustrates how to do that on the command line using wget. The `-c` flag tells the wget to continue the download from where it left off, which is helpful because snapshots can be huge files, and the download can fail mid-way. The `-P` flag tells wget to place the snapshot on the temporary dir.

```shell
wget -c -P /tmp "$SNAPSHOT_URL"
```

After downloading the snapshot, make sure to verify whether the checksum matches the one provided by the remote server. To fetch the checksum, you may run the command below.

```shell
wget -q -O - "$SNAPSHOT_URL".sha256
```

Once you know the expected snapshot checksum, run the command below to compute the checksum of the downloaded snapshot. Then, compare both and see if they are the same. If they are not the same, consider redownloading the snapshot. You must provide a valid snapshot to Nitro; otherwise, it won’t work properly.

```shell
sha256sum $PATH_TO_SNAPSHOT
```

Finally, you can provide a path to the downloaded snapshot archive to Nitro using the `--init.url` flag, as described in the "Download the Snapshot from a URL" section.

### Downloading snapshot parts

If the snapshot is divided into parts, you should first download the manifest file in `<snapshot-url>.manifest.txt`. This manifest contains the names and checksums of each part. For instance, the snippet below shows how the manifest file should look. You may use the commands described previously to download each part of the snapshot and verify their checksums.

```shell
a938e029605b81e03cd4b9a916c52d96d74c985ac264e2f298b90495c619af74  archive.tar.part0
9e095ce82e70fa62bb6e7b4421e7f2c04b2cd9e21d2bc62cbbaaeb877408357b  archive.tar.part1
e92172d6eaf770a76c7477e6768f742fc51555a5050de606bd0f837e59c7a61d  archive.tar.part2
d1b6fb9aeeb23903cdbb2a7cca8e6909bff4ee8e51c8a5acac2a142b3e3a5437  archive.tar.part3
f37e4552453202f2044e58b307bab7e466205bd280426abbc84f8646c6430cfa  archive.tar.part4
972c5f513faca6ac4fadd22c70bea97707c6d38e9a646432bc311f0ca10497ed  archive.tar.part5
```

After downloading all the parts and verifying their checksums, you may use the command below to join them into a single archive.

```shell
cat archive.tar.part* > archive.tar
```

---

## Extracting the snapshot manually

It is also possible to extract the snapshot archive and place the files manually. First, you need to download the snapshot archive as described in "Manually Downloading the Snapshot". Then, create the directory where Nitro will look for its database. By default, Nitro stores the database on `$HOME/.arbitrum/$CHAIN/nitro`. Move the archive to this directory and extract it. The commands below exemplify this process for the Arbitrum Sepolia chain.

```shell
export CHAIN=sepolia-rollup
export ARCHIVE_PATH=/tmp/archive.tar.gz
mkdir -p $HOME/.arbitrum/$CHAIN/nitro
cd $HOME/.arbitrum/$CHAIN/nitro
tar zxfv $ARCHIVE_PATH
```

You should see the following subdirectories in this directory after extracting the archive.

```shell
arbitrumdata
l2chaindata
nodes
```

---

## Creating a snapshot

To generate a snapshot for the Nitro database, you first need to stop the process gracefully. You must not generate the snapshot while Nitro runs because the database might be in an intermediary state. Nitro should print logs like the ones described below when stopping.

```shell
^CINFO [08-22|18:10:55.015] shutting down because of sigint
INFO [08-22|18:10:55.016] delayed sequencer: context done          err="context canceled"
INFO [08-22|18:10:55.016] rpc response                             method=eth_getBlockByNumber logId=123 err="context canceled" result=null attempt=0 args="[\"0x405661\", false]"
INFO [08-22|18:10:55.293] Writing cached state to disk             block=39988 hash=8bebf3..939ab2 root=4f7a22..00c334
INFO [08-22|18:10:55.297] Persisted trie from memory database      nodes=643 size=156.31KiB time=3.673459ms gcnodes=329 gcsize=102.61KiB gctime="248.708µs" livenodes=2448 livesize=806.00KiB
INFO [08-22|18:10:55.297] Writing cached state to disk             block=39987 hash=ddcd60..fe0fc3 root=d6973e..7b9265
INFO [08-22|18:10:55.298] Persisted trie from memory database      nodes=34  size=11.19KiB  time="283.875µs" gcnodes=0   gcsize=0.00B     gctime=0s          livenodes=2414 livesize=794.81KiB
INFO [08-22|18:10:55.298] Writing cached state to disk             block=39861 hash=2a9dd3..f00ff0 root=139d5a..d6bf21
INFO [08-22|18:10:55.298] Persisted trie from memory database      nodes=73  size=24.88KiB  time="502.916µs" gcnodes=0   gcsize=0.00B     gctime=0s          livenodes=2341 livesize=769.93KiB
INFO [08-22|18:10:55.299] Writing cached state to disk             block=39861 hash=2a9dd3..f00ff0 root=139d5a..d6bf21
INFO [08-22|18:10:55.299] Persisted trie from memory database      nodes=0   size=0.00B     time="1.417µs"   gcnodes=0   gcsize=0.00B     gctime=0s          livenodes=2341 livesize=769.93KiB
INFO [08-22|18:10:55.299] Writing snapshot state to disk           root=bd18ce..3b0763
INFO [08-22|18:10:55.299] Persisted trie from memory database      nodes=0   size=0.00B     time="1.125µs"   gcnodes=0   gcsize=0.00B     gctime=0s          livenodes=2341 livesize=769.93KiB
INFO [08-22|18:10:55.304] Blockchain stopped
```

After Nitro stops, go to the database directory and generate an archive file for the directories `arbitrumdata`, `l2chaindata`, and `nodes`. By default, the database directory for Nitro is `$HOME/.arbitrum/$CHAIN/nitro`. The commands below exemplify how to generate the snapshot for Nitro.

```shell
export CHAIN=sepolia-rollup
export ARCHIVE_PATH=/tmp/archive.tar.gz
cd $HOME/.arbitrum/$CHAIN/nitro
tar zcfv $ARCHIVE_PATH arbitrumdata l2chaindata nodes
```

This command purposely omits the `wasm` directory from the snapshot archive. The `wasm` contains native-code executables, so it might be a security concern for users downloading the snapshot. If the user downloading the snapshot trusts you, or if you are storing it for your own use, you may include the `wasm` directory in it.

### Optional: divide it into parts

It is possible to divide the snapshot into smaller parts to facilitate its download. This is particularly useful for archive snapshots of heavily used chains, such as Arbitrum One. These kinds of snapshots can reach terabytes, so dividing them into smaller parts is helpful. The snippet below illustrates how to divide the snapshot into parts using the split command. The `-b` argument tells the split to divide the snapshot into 100 GB parts. The `-d` argument tells split to enumerate the parts using a numeric suffix instead of an alphabetic one.

```shell
split -b 100g -d archive.tar.gz archive.tar.gz.part
```

After dividing it into parts, you should generate the manifest file containing the parts' names and checksums. Nitro will use these files to know how many parts there are and to validate their checksum. The command below exemplifies how to do that.

---

## .mdx (nitro/04-how-to-convert-databases-from-leveldb-to-pebble.mdx)
---
title: 'How to convert databases from leveldb to pebble'
sidebar_label: 'Convert node database'
description: 'Learn how convert your node database from leveldb to pebble'
author: joshuacolvin0
sme: joshuacolvin0
user_story: As a node runner, I want to learn how to migrate my node database from leveldb to pebble.
content_type: How-To
---

import { VanillaAdmonition } from '@site/src/components/VanillaAdmonition/';

Switching from LevelDB to Pebble in Ethereum's Geth client provides several advantages, particularly in terms of resilience to data corruption during unexpected shutdowns, which helps maintain the integrity of your Ethereum node's database.

While Pebble provides better resilience and potentially improved performance, deciding to switch should consider your specific needs and the possible effects of a full resync on your operations.

<VanillaAdmonition type="note">

It's important to understand that transitioning to Pebble may necessitate a complete resynchronization of your Ethereum node. This process can take many days/weeks and could lead to downtime. If you are not facing significant issues with LevelDB and do not need Pebble's specific advantages, there may not be a strong reason to make the switch at this time.

</VanillaAdmonition>

## convert-databases.bash script

The script can be found in the [Nitro Docker image](https://github.com/OffchainLabs/nitro/blob/7fdc5681c410bf283029e62bde4a1367b32f647d/Dockerfile#L283).

```shell
Usage: convert-databases.bash [OPTIONS..]

OPTIONS:
--dbconv          dbconv binary path (default: "/usr/local/bin/dbconv")
--src             directory containing source databases (default: "/home/user/.arbitrum/arb1/nitro")
--dst             destination directory
--force           remove destination directory if it exists
--skip-existing   skip convertion of databases which directories already exist in the destination directory
--clean           sets what should be removed in case of error, possible values:
                      "failed" - remove database which conversion failed (default)
                      "none"   - remove nothing, leave unfinished and potentially corrupted databases
                      "all"    - remove whole destination directory

```

Upon successful completion, the script prints out:

```shell
== Conversion status:
   l2chaindata database: converted
   l2chaindata database freezer (ancient): copied
   arbitrumdata database: converted
   wasm database: converted
   classic-msg database: converted

```

### Running the conversion script in Docker

```shell
docker run \
       --detach \
       --rm \
       --name convert_db \
       -v /path/to/src/data/arbitrum:/home/user/.arbitrum \
       -v /path/to/dst/data/arbitrum:/home/user/dst \
       -it \
       --entrypoint /bin/bash \
       nitro-node \
       -c "convert-databases.bash --dst /home/user/dst/arb1/nitro"
```

## dbconv tool

`dbconv` tool can be used to:

- Convert single database - copy all entries from a single source database to the destination database that may use a different database engine (`leveldb` or `pebble`)
- Compact single database - run compaction on the destination database
- Verify database contents - check if all keys (and optionally values) from the source database are present in the destination database

Selected command arguments:

```shell
--dst.data string         destination directory of stored chain state
--dst.db-engine string    backing database implementation to use ('leveldb' or 'pebble') (default "pebble")
--src.data string         source directory of stored chain state
--src.db-engine string    backing database implementation to use ('leveldb' or 'pebble') (default "leveldb")
--convert                 enables conversion step
--compact                 enables compaction step
--verify string           enables verification step ("" = disabled, "keys" = only keys, "full" = keys and values)
```

To see all possible configuration options, run: `dbconv --help`

### example usage

- Converting the `leveldb` database to `pebble` and compacting the resulting database.

```shell
./target/bin/dbconv --src.data /path/to/source/database/ --src.db-engine leveldb --dst.data /path/to/destination/database/ --dst.db-engine "pebble" --convert --compact
```

- Verifying that all source DB entries are in the destination DB (checking only if keys exist)

```shell
./target/bin/dbconv --src.data /path/to/source/database/ --src.db-engine leveldb --dst.data /path/to/destination/database/ --dst.db-engine "pebble" --verify "keys"
```

- Converting the `leveldb` database to `pebble`, compacting the resulting database, and then verifying that all keys from the source database exist in the destination database

```shell
./target/bin/dbconv --src.data /path/to/source/database/ --src.db-engine leveldb --dst.data /path/to/destination/database/ --dst.db-engine "pebble" --convert --compact --verify "keys"
```

---

## .mdx (partials/run-full-node/_dao-chains-example.mdx)
```shell
docker run --rm -it -v /some/local/dir/arbitrum:/home/user/.arbitrum -p 0.0.0.0:8547:8547 -p 0.0.0.0:8548:8548 @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@ --parent-chain.connection.url=<Ethereum RPC URL> --parent-chain.blob-client.beacon-url=<Ethereum beacon chain RPC URL> --chain.id=<Arbitrum chain id> --init.latest=pruned --http.api=net,web3,eth --http.corsdomain=* --http.addr=0.0.0.0 --http.vhosts=*
```

---

## .mdx (partials/run-full-node/_dao-chains-parameters.mdx)
<!--
    Breaking the title ordering here (it should be h3 instead of h4) so these titles don't appear in the right-hand sidebar
-->

#### 1. Parent chain (Ethereum) parameters

The `--parent-chain.connection.url` parameter needs to provide a standard RPC endpoint for an Ethereum node, whether self-hosted or obtained from a node service provider:

```shell
--parent-chain.connection.url=<Ethereum RPC URL>
```

Additionally, use the parameter `--parent-chain.blob-client.beacon-url` to provide a beacon chain RPC endpoint:

```shell
--parent-chain.blob-client.beacon-url=<Ethereum beacon chain RPC URL>
```

:::info Try it out

If you choose to self-host an EVM node, the [Prysm client software](https://www.offchainlabs.com/prysm/docs) is a great choice. It's straightforward, efficient, and effective—ensuring your setup runs smoothly!

:::

You can also consult our [list of Ethereum beacon chain RPC providers](/run-arbitrum-node/04-l1-ethereum-beacon-chain-rpc-providers.mdx). Note that historical blob data is required for these chains to properly sync up if they are new or have been offline for more than 18 days. The beacon chain RPC endpoint you use may also need to provide historical blob data. Please see [Special notes on ArbOS 20: Atlas support for EIP-4844](/run-arbitrum-node/arbos-releases/arbos20.mdx#special-notes-on-arbos-20-atlas-support-for-eip-4844) for more details.

#### 2. Arbitrum chain parameters

Use the parameter `--chain.id` to specify the chain you're running this node for. See [RPC endpoints and providers](/build-decentralized-apps/reference/01-node-providers.mdx) to find the IDs of these chains.

```shell
--chain.id=<Arbitrum chain ID>
```

Alternatively, you can use the parameter `--chain.name` to specify the chain you're running this node for. Use `arb1` for Arbitrum One, `nova` for Arbitrum Nova, or `sepolia-rollup` for Arbitrum Sepolia.

```shell
--chain.name=<Child chain name>
```

---

## .mdx (partials/run-full-node/_optional-orbit-sequencer-compatible-cli-partial.mdx)
<section class='small-table'>

| Flag                                                      | Description                                                                                                                                                                                                                                                                                                   |
| --------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `--execution.rpc.classic-redirect=<RPC>`                  | Redirects archive requests for pre-nitro blocks to this RPC of an Arbitrum Classic node with an archive database, **only for Arbitrum One**.                                                                                                                                                                  |
| `--execution.rpc.classic-redirect=<RPC>`                  | Redirects archive requests for pre-nitro blocks to this RPC from an Arbitrum Classic node with an archive database, **only for Arbitrum One**.                                                                                                                                                                |
| `--http.api`                                              | Which APIs need to be opened over the HTTP-RPC interface. Default: `net,web3,eth,arb`. Add `debug` for tracing.                                                                                                                                                                                               |
| `--http.corsdomain`                                       | Accepts cross origin requests from these comma-separated domains (browser enforced).                                                                                                                                                                                                                          |
| `--http.vhosts`                                           | Accepts requests from these comma-separated virtual hostnames (server enforced). Default: `localhost`. Accepts `*`.                                                                                                                                                                                           |
| `--http.addr`                                             | Address to bind RPC to. May require `0.0.0.0` for Docker networking.                                                                                                                                                                                                                                          |
| `--execution.caching.archive`                             | Will retain past block state. **For archive nodes**.                                                                                                                                                                                                                                                          |
| `--node.feed.input.url=<feed address>`                    | Default: `wss://<chainName>.arbitrum.io/feed`. ⚠️ One feed relay per datacenter is advised. See [feed relay guide](/run-arbitrum-node/run-feed-relay.mdx).                                                                                                                                                    |
| `--execution.rpc.evm-timeout`                             | Default: `5s`. Timeout for `eth_call`. (`0` == no timeout).                                                                                                                                                                                                                                                   |
| `--execution.rpc.gas-cap`                                 | Default: `50000000`. Gas cap for `eth_call`/`estimateGas`. (`0` = no cap).                                                                                                                                                                                                                                    |
| `--execution.rpc.tx-fee-cap`                              | Default: `1`. Transaction fee cap (in `ETH`) for RPC APIs. (`0` = no cap).                                                                                                                                                                                                                                    |
| `--ipc.path`                                              | Filename for IPC socket/pipe within `datadir`. Not supported on macOS. **Note**: The path is within the Docker container.                                                                                                                                                                                     |
| `--init.prune`                                            | Prunes database before starting the node. It can be used for **full** or **validator** nodes.                                                                                                                                                                                                                 |
| `--init.url="<snapshot file>"`                            | **Non-Orbit Nitro nodes only**: URL from which to download the genesis database. Required only for the first startup of an Arbitrum One node. Reference to [snapshots](https://snapshot.arbitrum.foundation/index.html) and [archive node guide](/run-arbitrum-node/more-types/01-run-archive-node.mdx).      |
| `--init.download-path="/path/to/dir"`                     | **Non-Orbit Nitro nodes only**: Temporarily saves the downloaded database snapshot. Defaults to `/tmp/`. Used with `--init.url`.                                                                                                                                                                              |
| `--node.batch-poster.post-4844-blobs`                     | Boolean. Default: `false`. Used to enable or disable the posting of transaction data using Blobs to Ethereum mainnet. If using calldata is more expensive and the parent chain supports `EIP4844` blobs, the batch poster will use blobs when this flag is set to `true`. It can be set to `true` or `false`. |
| `--node.batch-poster.ignore-blob-price`                   | Boolean. Default: `false`. If the parent chain supports `EIP4844` blobs and `ignore-blob-price` is set to `true`, the batch poster will use `EIP4844` blobs even if using calldata is cheaper. It can be set to `true` or `false`.                                                                            |
| `--execution.sequencer.enable`                            | Act as sequencer and post to L1.                                                                                                                                                                                                                                                                              |
| `--execution.sequencer.enable-profiling`                  | Enable CPU profiling and tracing.                                                                                                                                                                                                                                                                             |
| `--execution.sequencer.expected-surplus-hard-threshold`   | If the expected surplus is lower than this value, new incoming transactions will be denied (default "default").                                                                                                                                                                                               |
| `--execution.sequencer.expected-surplus-soft-threshold`   | Warnings are posted if the expected surplus is lower than this value (default "default").                                                                                                                                                                                                                     |
| `--execution.sequencer.forwarder.connection-timeout`      | Total time to wait before canceling connection (default 30s).                                                                                                                                                                                                                                                 |
| `--execution.sequencer.forwarder.idle-connection-timeout` | Time until idle connections are closed (default 1m0s).                                                                                                                                                                                                                                                        |
| `--execution.sequencer.forwarder.max-idle-connections`    | Maximum number of idle connections to keep open (default `100`).                                                                                                                                                                                                                                              |
| `--execution.sequencer.forwarder.redis-url`               | The recommended Redis URL to use as target.                                                                                                                                                                                                                                                                   |
| `--execution.sequencer.forwarder.retry-interval`          | Minimal time between update retries (default 100ms).                                                                                                                                                                                                                                                          |
| `--execution.sequencer.forwarder.update-interval`         | Forwarding target update interval (default 1s).                                                                                                                                                                                                                                                               |
| `--execution.sequencer.max-acceptable-timestamp-delta`    | Maximum acceptable time difference between the local time and the latest L1 block's timestamp (default 1h0m0s).                                                                                                                                                                                               |
| `--execution.sequencer.max-block-speed`                   | Minimum delay between blocks (sets a maximum speed of block production) (default 250ms).                                                                                                                                                                                                                      |
| `--execution.sequencer.max-revert-gas-reject`             | Maximum gas executed in a revert for the sequencer to reject the transaction instead of posting it (anti-DOS).                                                                                                                                                                                                |
| `--execution.sequencer.max-tx-data-size`                  | Maximum transaction size the sequencer will accept (default `95000`).                                                                                                                                                                                                                                         |
| `--execution.sequencer.nonce-cache-size`                  | Size of the transaction sender nonce cache (default `1024`).                                                                                                                                                                                                                                                  |
| `--execution.sequencer.nonce-failure-cache-expiry`        | Maximum time to wait for a predecessor before rejecting a transaction whose nonce is too high (default 1s).                                                                                                                                                                                                   |
| `--execution.sequencer.nonce-failure-cache-size`          | Number of transactions whose nonce is too high to keep in memory while waiting for their predecessor (default `1024`).                                                                                                                                                                                        |
| `--execution.sequencer.queue-size`                        | Size of the pending transaction queue (default `1024`).                                                                                                                                                                                                                                                       |
| `--execution.sequencer.queue-timeout`                     | Maximum time a transaction can wait in a queue (default 12s).                                                                                                                                                                                                                                                 |
| `--execution.sequencer.sender-whitelist`                  | Comma-separated allowlist of authorized senders (if empty, every sender is allowed).                                                                                                                                                                                                                          |
| `--node.delayed-sequencer.finalize-distance`              | Number of blocks in the past L1 block for the transaction to be considered final. This value is ignored when using merge finality. Default: `20`.                                                                                                                                                             |
| `--node.delayed-sequencer.require-full-finality`          | Whether to wait for full finality before sequencing delayed messages.                                                                                                                                                                                                                                         |
| `--node.delayed-sequencer.use-merge-finality`             | Whether to use The Merge's notion of finality before sequencing delayed messages (default to `true`).                                                                                                                                                                                                         |

</section>

---

## .mdx (partials/run-full-node/_optional-parameters.mdx)
<section class='small-table'>

| Flag                                     | Description                                                                                                                                                                                                                                                                                                                                                                                                        |
| ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `--http.api`                             | Offers APIs over the HTTP-RPC interface. Default: `net,web3,eth,arb`. Add `debug` for tracing.                                                                                                                                                                                                                                                                                                                     |
| `--http.corsdomain`                      | Accepts cross-origin requests from these comma-separated domains (browser enforced).                                                                                                                                                                                                                                                                                                                               |
| `--http.vhosts`                          | Accepts requests from these comma-separated virtual hostnames (server enforced). Default: `localhost`. Accepts `*`.                                                                                                                                                                                                                                                                                                |
| `--http.addr`                            | Sets the address to bind RPC to. May require `0.0.0.0` for Docker networking.                                                                                                                                                                                                                                                                                                                                      |
| `--execution.caching.archive`            | Retains past block state. For archive nodes.                                                                                                                                                                                                                                                                                                                                                                       |
| `--node.feed.input.url=<feed address>`   | Sets the sequencer feed address to this URL. Default: `wss://<chainName>.arbitrum.io/feed`. ⚠️ One feed relay per datacenter is advised. See [feed relay guide](/run-arbitrum-node/run-feed-relay.mdx).                                                                                                                                                                                                            |
| `--execution.forwarding-target=<RPC>`    | Sets the sequencer endpoint to forward requests to.                                                                                                                                                                                                                                                                                                                                                                |
| `--execution.rpc.evm-timeout`            | Default: `5s`. Timeout for `eth_call`. (0 == no timeout).                                                                                                                                                                                                                                                                                                                                                          |
| `--execution.rpc.gas-cap`                | Default: `50000000`. Gas cap for `eth_call`/`estimateGas`. (0 = no cap).                                                                                                                                                                                                                                                                                                                                           |
| `--execution.rpc.tx-fee-cap`             | Default: `1`. Transaction fee cap (in ether) for RPC APIs. (0 = no cap).                                                                                                                                                                                                                                                                                                                                           |
| `--execution.tx-lookup-limit`            | Default: `126230400`, ~1 year worth of blocks at 250ms/block. Maximum number of blocks from head whose transaction indices are reserved (e.g. `eth_getTransactionReceipt` and `eth_getTransactionByHash` will only return results for indexed transactions). Set to 0 to index transactions for all blocks. Changing this parameter will reindex all missing transactions without the need of resyncing the chain. |
| `--execution.rpc.classic-redirect=<RPC>` | (Arbitrum One only) Redirects archive requests for pre-nitro blocks to this RPC of an Arbitrum Classic node with archive database.                                                                                                                                                                                                                                                                                 |
| `--ipc.path`                             | Filename for IPC socket/pipe within datadir. 🔉 Not supported on macOS. Note the path is within the Docker container.                                                                                                                                                                                                                                                                                              |
| `--init.prune`                           | Prunes the database before starting the node. Can be "full" or "validator".                                                                                                                                                                                                                                                                                                                                        |
| `--init.url="<snapshot file>"`           | (Required for Arbitrum One) URL to download the genesis database from. Only required for Arbitrum One nodes, when running them for the first time. See [this guide](/run-arbitrum-node/nitro/03-nitro-database-snapshots.mdx) for more information.                                                                                                                                                                |
| `--init.download-path="/path/to/dir"`    | Temporarily saves the downloaded database snapshot. Defaults to `/tmp/`. Used with `--init.url`.                                                                                                                                                                                                                                                                                                                   |
| `--init.latest`                          | Searches for the latest snapshot of the given kind (accepted values: `archive`, `pruned`, `genesis`)                                                                                                                                                                                                                                                                                                               |
| `--init.latest-base`                     | Base url used when searching for the latest snapshot. Default: "https://snapshot.arbitrum.foundation/". If you are running an Arbitrum chain, ask the chain owner for this URL.                                                                                                                                                                                                                                    |
| `--init.then-quit`                       | Allows any `--init.*` parameters to complete, and then the node will automatically quit. It doesn't initiate pruning by itself but works in conjunction with other `--init.*` parameters, making it easier to script tasks like database backups after initialization processes finish.                                                                                                                            |

</section>

---

## .mdx (partials/run-full-node/_orbit-chains-example.mdx)
```shell
docker run --rm -it -v /some/local/dir/arbitrum:/home/user/.arbitrum -p 0.0.0.0:8547:8547 -p 0.0.0.0:8548:8548 @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@ --parent-chain.connection.url=<Parent chain RPC URL>  --chain.info-json=<Orbit chain's info> --chain.name=<Orbit chain name> --node.feed.input.url=<Sequencer feed url> --execution.forwarding-target=<Sequencer node endpoint url> --http.api=net,web3,eth --http.corsdomain=* --http.addr=0.0.0.0 --http.vhosts=*
```

- You can see an example of `--chain.info-json` in the section above.

---

## .mdx (partials/run-full-node/_orbit-chains-parameters.mdx)
<!--
    Breaking the title ordering here (it should be h3 instead of h4) so these titles don't appear in the right-hand sidebar
-->

#### 1. Parent chain parameters

The `--parent-chain.connection.url` parameter needs to provide a standard RPC endpoint for an EVM node, whether self-hosted or obtained from a node service provider:

```shell
--parent-chain.connection.url=<Parent chain RPC URL>
```

:::info Try it out

If you choose to self-host an EVM node, the [Prysm client software](https://www.offchainlabs.com/prysm/docs) is a great choice. It's straightforward, efficient, and effective—ensuring your setup runs smoothly!

:::

Additionally, if the chain is a Layer-2 (L2) chain on top of Ethereum and uses blobs to post calldata, use the parameter `--parent-chain.blob-client.beacon-url` to provide a beacon chain RPC endpoint:

```shell
--parent-chain.blob-client.beacon-url=<Parent chain beacon chain RPC URL>
```

:::info Public Arbitrum RPC endpoints

[Public Arbitrum RPC endpoints](/build-decentralized-apps/reference/01-node-providers.mdx#arbitrum-public-rpc-endpoints) rate-limit connections. To avoid hitting a bottleneck, you can run a local node for the parent chain or rely on third-party RPC providers.

:::

You can find beacon providers in our [list of Ethereum beacon chain RPC providers](/run-arbitrum-node/04-l1-ethereum-beacon-chain-rpc-providers.mdx). Note that historical blob data is required for these chains to properly sync up if they are new or have been offline for more than 18 days. This means that the beacon chain RPC endpoint you use may also need to provide historical blob data. Please see [Special notes on ArbOS 20: Atlas support for EIP-4844](/run-arbitrum-node/arbos-releases/arbos20.mdx#special-notes-on-arbos-20-atlas-support-for-eip-4844) for more details.

#### 2. Child chain parameters

The parameter `--chain.info-json` specifies a JSON string that contains the information about the Arbitrum (Orbit) chain required by the node.

```shell
--chain.info-json=<Orbit chain's info>
```

This information should be provided by the chain owner and will look something like the following:

```shell
--chain.info-json="[{\"chain-id\":94692861356,\"parent-chain-id\":421614,\"chain-name\":\"My Arbitrum L3 Chain\",\"chain-config\":{\"chainId\":94692861356,\"homesteadBlock\":0,\"daoForkBlock\":null,\"daoForkSupport\":true,\"eip150Block\":0,\"eip150Hash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\",\"eip155Block\":0,\"eip158Block\":0,\"byzantiumBlock\":0,\"constantinopleBlock\":0,\"petersburgBlock\":0,\"istanbulBlock\":0,\"muirGlacierBlock\":0,\"berlinBlock\":0,\"londonBlock\":0,\"clique\":{\"period\":0,\"epoch\":0},\"arbitrum\":{\"EnableArbOS\":true,\"AllowDebugPrecompiles\":false,\"DataAvailabilityCommittee\":false,\"InitialArbOSVersion\":10,\"InitialChainOwner\":\"0xAde4000C87923244f0e95b41f0e45aa3C02f1Bb2\",\"GenesisBlockNum\":0}},\"rollup\":{\"bridge\":\"0xde835286442c6446E36992c036EFe261AcD87F6d\",\"inbox\":\"0x0592d3861Ea929B5d108d915c36f64EE69418049\",\"sequencer-inbox\":\"0xf9d77199288f00440Ed0f494Adc0005f362c17b1\",\"rollup\":\"0xF5A42aDA664E7c2dFE9DDa4459B927261BF90E09\",\"validator-utils\":\"0xB11EB62DD2B352886A4530A9106fE427844D515f\",\"validator-wallet-creator\":\"0xEb9885B6c0e117D339F47585cC06a2765AaE2E0b\",\"deployed-at\":1764099}}]"
```

Use the parameter `--chain.name` to specify the chain you're running this node for. The name of the chain should match the name used in the JSON string used in `--chain.info-json`:

```shell
--chain.name=<Orbit chain name>
```

#### 3. Parameters to connect to the sequencer

Use the parameter `--node.feed.input.url` to point at the sequencer feed endpoint, which should be provided by the chain owner.

```shell
--node.feed.input.url=<Sequencer feed url>
```

Use the parameter `--execution.forwarding-target` to point at the sequencer node of the Arbitrum (Orbit) chain, which should also be provided by the chain owner.

```shell
--execution.forwarding-target=<Sequencer node endpoint url>
```

#### 3. Additional parameters for AnyTrust chains

If you're running a node for an <a data-quicklook-from="arbitrum-anytrust-chain">Anytrust chain</a>, you need to specify information about the Data Availability Committee (DAC) in the configuration of your node.

First, enable `data-availability` using the following parameters:

```shell
--node.data-availability.enable
--node.data-availability.rest-aggregator.enable
```

Then, choose one of these methods to specify the DAS REST endpoints that your node will read the information from. These endpoints should also be provided by the chain owner.

    1. Set the DAS REST endpoints directly:

    ```shell
    --node.data-availability.rest-aggregator.urls=<A list of DAS REST endpoints, separated by commas>
    ```

    2. Set a URL that returns a list of the DAS REST endpoints:

    ```shell
    --node.data-availability.rest-aggregator.online-url-list=<A URL that returns a list of the DAS REST endpoints>
    ```

::::tip Setting a DAS (for chain owners)

If you are a chain owner, please refer to the [DAC setup guide](/run-arbitrum-node/data-availability-committees/01-get-started.mdx#if-you-are-a-chain-owner) to set it up.

Additionally, for your batch poster to post data to the DAS, follow [Step 3 of How to configure a DAC](/run-arbitrum-node/data-availability-committees/04-configure-dac.mdx#step-3-craft-the-new-configuration-for-the-batch-poster) to configure your batch poster node.

::::

---

## .mdx (partials/run-full-node/_tx-prechecker-parameters.mdx)
<section class='small-table'>

| Flag                                                   | Description                                                                                                                                                                                                                                                                     |
| ------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `--execution.tx-pre-checker.strictness`                | How strict to be when checking transactions before forwarding them. 0 = accept anything, 10 = should never reject anything that'd succeed, 20 = likely won't reject anything that'd succeed, 30 = full validation which may reject transactions that would succeed (default 20) |
| `--execution.tx-pre-checker.required-state-age`        | How long ago should the storage conditions from `eth_SendRawTransactionConditional` be true, 0 = don't check old state (default 2)                                                                                                                                              |
| `--execution.tx-pre-checker.required-state-max-blocks` | Maximum number of blocks to look back while looking for the `<required-state-age>` seconds old state, 0 = don't limit the search (default 4)                                                                                                                                    |

</section>

---

## .mdx (run-feed-relay.mdx)
---
title: 'How to run a feed relay'
description: Learn how to run an Arbitrum feed relay on your local machine.
sidebar_position: 8
content_type: how-to
---

:::caution

If running a single node, there is no need to run a feed relay. When running more that one node, it is strongly suggested to run a single feed relay per datacenter, which will reduce ingress fees and improve stability.

:::

:::caution

Feed endpoints will soon require compression with a custom dictionary, so if connecting to feed with anything other than a standard node, it is strongly suggested to run a local feed relay which will provide an uncompressed feed by default.

:::

The feed relay is in the same docker image as the Nitro node.

- Here is an example of how to run the feed relay for Arbitrum One:
  ```shell
  docker run --rm -it  -p 0.0.0.0:9642:9642 --entrypoint relay @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@ --node.feed.output.addr=0.0.0.0 --node.feed.input.url=wss://arb1-feed.arbitrum.io/feed --chain.id=42161
  ```
- Here is an example of how to run nitro-node for Arbitrum One with custom relay:
  ```shell
  docker run --rm -it  -v /some/local/dir/arbitrum:/home/user/.arbitrum -p 0.0.0.0:8547:8547 -p 0.0.0.0:8548:8548 @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@ --parent-chain.connection.url=https://l1-mainnet-node:8545 --chain.id=42161 --http.api=net,web3,eth --http.corsdomain=* --http.addr=0.0.0.0 --http.vhosts=* --node.feed.input.url=ws://local-relay-address:9642
  ```

Note that Arbitrum classic does not communicate with Nitro sequencer, so classic relay is no longer used.

## Helm charts (Kubernetes)

If you are using [Kubernetes](https://kubernetes.io/) to run your feed relay, a helm chart is available at [ArtifactHUB](https://artifacthub.io/packages/helm/offchainlabshelm/relay). It supports running a Nitro relay by providing the feed input URL. Find more information in the [OCL community Helm charts repository](https://github.com/OffchainLabs/community-helm-charts/tree/main/charts/relay).

---

## .mdx (sequencer/02-read-sequencer-feed.mdx)
---
title: 'How to read the sequencer feed'
description: Learn how to read the sequencer feed
sidebar_position: 9
content_type: how-to
todos:
  - Follow convention and style guide
  - Communicate "who this is for" and "under which scenarios this is useful".
  - Set expectations - prior knowledge, what to expect
  - Build supporting conceptual content
  - Align on what we want to treat as proper nouns vs common nouns
---

[Running an Arbitrum relay locally as a feed relay](/run-arbitrum-node/run-feed-relay.mdx) lets you subscribe to an uncompressed <a data-quicklook-from="sequencer-feed">sequencer feed</a> for real-time data as the sequencer accepts and orders transactions offchain.

When connected to websocket port `9642` of the local relay, you'll receive a data feed that looks something like this:

```json
{
  "version": 1,
  "messages": [
    {
      "sequenceNumber": 25757171,
      "message": {
        "message": {
          "header": {
            "kind": 3,
            "sender": "0xa4b000000000000000000073657175656e636572",
            "blockNumber": 16238523,
            "timestamp": 1671691403,
            "requestId": null,
            "baseFeeL1": null
          },
          "l2Msg": "BAL40oKksUiElQL5AISg7rsAgxb6o5SZbYNoIF2DTixsqDpD2xII9GJLG4C4ZAhh6N0AAAAAAAAAAAAAAAC7EQiq1R1VYgL3/oXgvD921hYRyAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAArAAaAkebuEnSAUvrWVBGTxA7W+ZMNn5uyLlbOH7Nrs0bYOv6AOxQPqAo2UB0Z7vqlugjn+BUl0drDcWejBfDiPEC6jQA=="
        },
        "delayedMessagesRead": 354560
      },
      "signature": null
    }
  ]
}
```

Breaking this feed down a bit: the top-level data structure is defined by the [`BroadcastMessage struct`](https://github.com/OffchainLabs/nitro/blob/9b1e622102fa2bebfd7dffd327be19f8881f1467/broadcaster/broadcaster.go#L42):

```go
type BroadcastMessage struct {
	Version int `json:"version"`
	// Note: the "Messages" object naming is slightly ambiguous: since there are different types of messages
	Messages                       []*BroadcastFeedMessage         `json:"messages,omitempty"`
	ConfirmedSequenceNumberMessage *ConfirmedSequenceNumberMessage `json:"confirmedSequenceNumberMessage,omitempty"`
}
```

The `messages` field is the [`BroadcastFeedMessage struct`](https://github.com/OffchainLabs/nitro/blob/9b1e622102fa2bebfd7dffd327be19f8881f1467/broadcaster/broadcaster.go#L49):

```go
type BroadcastFeedMessage struct {
	SequenceNumber arbutil.MessageIndex         `json:"sequenceNumber"`
	Message        arbstate.MessageWithMetadata `json:"message"`
	Signature      []byte                       `json:"signature"`
	BlockMetadata  arbostypes.BlockMetadata     `json:"blockMetadata"`
}
```

Each `message` conforms to [`arbstate.MessageWithMetadata`](https://github.com/OffchainLabs/nitro/blob/a05f768d774f60468a58a6a94fcc1be18e4d8fae/arbstate/inbox.go#L42):

```
type MessageWithMetadata struct {
	Message             *arbos.L1IncomingMessage `json:"message"`
	DelayedMessagesRead uint64                   `json:"delayedMessagesRead"`
}
```

Finally, we get the transaction's information in the `message` subfield as an [`L1IncomingMessage`](https://github.com/OffchainLabs/nitro/blob/9b1e622102fa2bebfd7dffd327be19f8881f1467/arbos/incomingmessage.go#L61):

```go
type L1IncomingMessage struct {
	Header *L1IncomingMessageHeader `json:"header"`
	L2msg  []byte                   `json:"l2Msg"`
	// Only used for `L1MessageType_BatchPostingReport`
	BatchGasCost *uint64 `json:"batchGasCost,omitempty" rlp:"optional"`
}
```

You can use the [`ParseL2Transactions`](https://github.com/OffchainLabs/nitro/blob/9b1e622102fa2bebfd7dffd327be19f8881f1467/arbos/incomingmessage.go#L227) function to decode the message.

Using the feed relay, you can also retrieve the `L2 block number` of a message:

- On <a data-quicklook-from="arbitrum-one">Arbitrum One</a>, this can be done by adding the Arbitrum One genesis block number (22207817) to the sequence number of the feed message.
- Note that in the case of <a data-quicklook-from="arbitrum-nova">Arbitrum Nova</a>, the Nitro genesis number is `0`, so it doesn't need to be included when adding to the feed message's sequence number.

:::info

Note that the `messages[0].message.message.header.blockNumber` is `L1 block number` instead of `L2 block number`

:::

---

## .mdx (sequencer/03-run-sequencer-coordination-manager.mdx)
---
title: 'How to run a Sequencer Coordinator Manager (SQM)'
description: Learn how to run a Sequencer Coordinator Manager terminal UI tool on your local machine.
sidebar_position: 10
content_type: how-to
---

import ImageZoom from '@site/src/components/ImageZoom';

The Sequencer Coordinator Manager (SQM) is a command-line tool that allows you to manage the priority list of sequencers, update their positions, add new sequencers to the list, and refresh the lists from the Redis server.
The tool offers keyboard-only support. Any changes you make are stored locally until you choose to save and push them to the Redis server.

- Clone and enter nitro repository:

  ```shell
  git clone --branch @@nitroVersionTag=v3.7.2@@ https://github.com/OffchainLabs/nitro.git
  cd nitro
  ```

- Before starting the Sequencer Coordinator Manager, please read and follow the commands in Step 4 - Step 7 of [Build Nitro's binaries natively](../nitro/01-build-nitro-locally.mdx#build-nitros-binaries-natively) to install the necessary dependencies.

- Here is an example of how to start the Sequencer Coordinator Manager and connect to a local Redis server:
  ```shell
  # In nitro directory
  make target/bin/seq-coordinator-manager
  ./target/bin/seq-coordinator-manager redis://127.0.0.1:6379
  ```

If mouse support is enabled, you can use your mouse to explore the tool. Otherwise, use your keyboard to explore the UI. The `enter` key selects options; `c` switches focus between lists.
When you bring up any form, you can navigate within the form's options using the `Tab` key and use the up/down arrow keys to select options from the dropdown menu.

<ImageZoom
  src="/img/run-node-seq-coordinator-manager.png"
  alt="Sequencer coordinator manager"
  className="img-600px"
/>

:::note
One of the sequencers is marked with a `chosen` indicator. This visual cue helps you identify the current chosen sequencer.
:::

You can click/enter on any sequencer element in the priority list to bring up a form for updating its position. Within this form, you have the options to make changes via a dropdown menu and then either click `Update` to see the change locally, `Cancel` to cancel the operation, or `Remove` to remove the sequencer from the priority list.

When a sequencer is removed, it is automatically added to the `--Not in priority list but online--` list if it is online. Please remember that all changes made using this method are local and need to be saved to the Redis server by pressing `s` from the keyboard shortcuts to make them permanent.

<ImageZoom src="/img/run-node-change-priority.png" alt="Change priority" className="img-600px" />

After selecting a sequencer from the non-priority list, you are given the option to add the selected sequencer to the priority list at any position of your choice. You can specify the position via a dropdown menu that lists all possible positions. Clicking `Update` will then display the updated priority list with the newly added sequencer in the chosen position.

<ImageZoom src="/img/run-node-add-to-priority.png" alt="Add to priority" className="img-600px" />

You can also add a new sequencer to the priority list by pressing `a` from the keyboard shortcuts. This action will bring up a form to enter the sequencer details. After adding the sequencer URL, you can click `Add` to see the changes or `Cancel` to abort the operation.

There is a flag in the sequencer's config, `--node.seq-coordinator.my-url`, which needs to be set to your sequencer's endpoint URL. Ensure that the URL of your sequencer added to the sequencer coordinator manager matches this flag.

<ImageZoom src="/img/run-node-add-new-seq.png" alt="Add new Sequencer" className="img-600px" />

To exit the tool, press `q` from the keyboard shortcuts.

---

## .mdx (sequencer/04-run-sequencer-node.mdx)
---
title: 'How to run a normal sequencer node for an Orbit chain'
description: Learn how to run an normal Arbitrum orbit sequencer node on your local machine
author: Jason Wan
sme: Jason Wan
content_type: how-to
---

:::caution

The following instructions are meant for Arbitrum Orbit chains only. This article only applies to test environments. If you need support spinning up a production Orbit chain, we recommend contacting a [provider](/launch-arbitrum-chain/06-third-party-integrations/02-third-party-providers.md#rollup-as-a-service-raas-providers).

We also provide a guide for running a high-availability sequencer node for an Orbit chain. You can find it [here](./05-high-availability-sequencer-docs.mdx).
:::

This how-to provides step-by-step instructions for running a sequencer node on your local machine.

## Minimum hardware configuration

The following are the minimum hardware configurations required to set up a <a data-quicklook-from="arbitrum-nitro">Nitro</a> full node (not archival):

| Resource     | Recommended                                   |
| :----------- | :-------------------------------------------- |
| RAM          | 16 GB                                         |
| CPU          | 4 core CPU (for AWS, a `t3 xLarge` instance)  |
| Storage type | NVMe SSD drives are recommended               |
| Storage size | Depends on the chain and its traffic overtime |

Please note that:

- These minimum requirements for RAM and CPU are recommended for nodes that process a small amount of RPC requests. For nodes that require processing multiple simultaneous requests, both RAM and the number of CPU cores will need to scale with the amount of traffic served.
- Single core performance is important. If the node is falling behind and a single core is 100% busy, it is recommended to update to a faster processor
- The minimum storage requirements will change over time as the chain grows. Using more than the minimum requirements to run a robust full node is recommended.

## Recommended Nitro version

:::caution

Even though there are alpha and beta versions of the Arbitrum Nitro software, only use release versions when running your node. Running alpha or beta versions is unsupported and might lead to unexpected behaviors.

:::

Latest Docker image: <code>@@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@</code>

## Required parameters

### 1. Sequencer node parameters

The following parameters are required to run a sequencer node:

#### 1. Enable sequencer

Enable the sequencer mode:

```shell
--node.sequencer=true
```

#### 2. Make the node act as a sequencer and post to L1

Enable the sequencer execution:

```shell
--execution.sequencer.enable=true
--execution.sequencer.max-tx-data-size=85000
```

#### 3. Enable delayed sequencer

Enable your node to read and include transactions from the parent chain delayed inbox.

```shell
--node.delayed-sequencer.enable=true
--node.delayed-sequencer.use-merge-finality=false
--node.delayed-sequencer.finalize-distance=1
```

#### 4. Enable batch poster

Enable your node to send batches to the parent chain:

```shell
--node.batch-poster.enable=true
--node.batch-poster.max-size=90000
--node.batch-poster.parent-chain-wallet.private-key=<Your Parent Chain Wallet Private Key>
```

#### 4. Disable transaction forwarding

Disable your sequencer's forwarding transactions, as the node will queue the transaction directly:

```shell
--execution.forwarding-target=""
```

#### 5. Enable feed-out queued transactions

Enable your node to feed out transactions so full node can receive queued transactions:

```shell
--node.feed.output.enable=true
--node.feed.output.addr=0.0.0.0
--node.feed.output.port=<Sequencer feed port>
```

#### 5. Connect the node to data availability servers

:::note

This step is only required in Anytrust mode.

:::

Enable your node to send batches to DAS and get DACerts from them.

```shell
--node.data-availability.enable=true
--node.data-availability.sequencer-inbox-address=<Sequencer Inbox Address>
--node.data-availability.parent-chain-node-url=<Parent Chain Node URL>
--node.data-availability.rest-aggregator.enable=true
--node.data-availability.rest-aggregator.urls=<A list of DAS REST endpoints, can be only one URL>
--node.data-availability.rpc-aggregator.enable=true
--node.data-availability.rpc-aggregator.assumed-honest=1
--node.data-availability.rpc-aggregator.backends=<A list of RPC backends>
```

### 2. Putting it all together

- When running a Docker image, an external volume should be mounted to persist the database across restarts. The mount point inside the Docker image should be `/home/user/.arbitrum`

- Example:

  ```shell wordWrap=true
  docker run --rm -it -v /some/local/dir/arbitrum:/home/user/.arbitrum -p 0.0.0.0:8547:8547 -p 0.0.0.0:8548:8548 @@latestNitroNodeImage=offchainlabs/nitro-node:v3.7.2-42be4fe@@ --node.sequencer=true --node.delayed-sequencer.enable=true --node.delayed-sequencer.use-merge-finality=false --node.delayed-sequencer.finalize-distance=1 --node.batch-poster.enable=true --node.batch-poster.max-size=90000 --node.batch-poster.parent-chain-wallet.private-key=<Your Parent Chain Wallet Private Key> --node.staker.enable=true --node.staker.strategy=MakeNodes --node.staker.parent-chain-wallet.private-key=<Your Parent Chain Wallet Private Key> --node.data-availability.enable=true --node.data-availability.sequencer-inbox-address=<Sequencer Inbox Address> --node.data-availability.parent-chain-node-url=<Parent Chain Node URL> --node.data-availability.rest-aggregator.enable=true --node.data-availability.rest-aggregator.urls=<A list of DAS REST endpoints, can be only one URL> --node.data-availability.rpc-aggregator.enable=true --node.data-availability.rpc-aggregator.assumed-honest=1 --node.data-availability.rpc-aggregator.backends=<A list of RPC backends> --execution.sequencer.enable=true --execution.sequencer.max-tx-data-size=85000
  ```

  - Ensure that `/some/local/dir/arbitrum` already exists; otherwise, the directory might be created with `root` as owner, and the Docker container won't be able to write to it.

- Json Example:
  ```json
  {
    "node": {
      "sequencer": true,
      "delayed-sequencer": {
        "enable": true,
        "use-merge-finality": false,
        "finalize-distance": 1
      },
      "batch-poster": {
        "max-size": 90000,
        "enable": true,
        "parent-chain-wallet": {
          "private-key": "<batch post key>"
        }
      },
      "feed": {
        "output": {
          "enable": true,
          "addr": "0.0.0.0",
          "port": "<Sequencer feed port>"
        }
      },
      "data-availability": {
        "enable": true,
        "sequencer-inbox-address": "<Sequencer inbox address>",
        "parent-chain-node-url": "https://sepolia-rollup.arbitrum.io/rpc",
        "rest-aggregator": {
          "enable": true,
          "urls": ["http://das-server:9877"]
        },
        "rpc-aggregator": {
          "enable": true,
          "assumed-honest": 1,
          "backends": "[{\"url\":\"http://das-server:9876\",\"pubkey\":\"YAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\",\"signermask\":1}]"
        }
      }
    },
    "execution": {
      "forwarding-target": "",
      "sequencer": {
        "enable": true,
        "max-tx-data-size": 85000
      }
    }
  }
  ```

### Note on permissions

- The Docker image is configured to run as non-root `UID 1000`. If you are running Linux or macOS and you are getting permission errors when trying to run the Docker image, run this command to allow all users to update the persistent folders:

```shell
mkdir /data/arbitrum
chmod -fR 777 /data/arbitrum
```

## Optional parameters

Here's a list of the parameters that are most commonly used when running your Orbit sequencer node. You can also use the flag `--help` for a comprehensive list of available parameters.

import OptionalOrbitSequencerCompatibleCLIFlagsPartial from '../partials/run-full-node/_optional-orbit-sequencer-compatible-cli-partial.mdx';

<OptionalOrbitSequencerCompatibleCLIFlagsPartial />

---

## .mdx (sequencer/05-high-availability-sequencer-docs.mdx)
---
title: 'How to set up a high-availability sequencer'
description: 'Learn how to set up a high-availabilty sequencer for your Arbitrum chain.'
author: Jason Wan
sme: Jason Wan
---

import ImageZoom from '@site/src/components/ImageZoom';

:::note

This documentation is for production sequencer deployments. If you want to set up a sequencer for testing or on a testnet, please refer to [How to run a testnet sequencer node](./04-run-sequencer-node.mdx).

:::

## Introduction

The sequencer is a critical component of your Arbitrum chain and is responsible for queuing transactions submitted to the network. It serves as the transaction ordering engine, accepting transactions forwarded from full nodes, queuing them, and returning feed messages to those full nodes. It subsequently sends queued transactions to batch posters for data posting.

If your sequencer goes offline, the network cannot process new transactions arriving at the child chain's RPC nodes, impacting the user experience. This guide provides detailed instructions for setting up a high availability (HA) sequencer architecture to minimize downtime and ensure your Arbitrum chain remains operational even if individual components fail.

## Prerequisites

Before you begin, ensure you have:

- Experience with Kubernetes and container orchestration
- Access to a Kubernetes cluster with multiple availability zones
- Understanding of Redis and cloud infrastructure
- A properly configured parent chain node or RPC node endpoint
- Sequencer keys and permissions
- Sufficient storage and compute resources

## High-availability sequencer architecture

A high-availability sequencer deployment consists of seven key components:

1. **CDN/Load Balancer** - Manages traffic routing and bot detection
2. **Nitro Fullnodes** - Process read requests and forward write transactions
3. **External Relays** - Handle public feed traffic
4. **Sequencer Relays** - Combine feeds from all sequencers
5. **Sequencers** - Multiple redundant transaction queueing machines
6. **Redis** - Coordinates active sequencer selection and sequencer-related information sharing
7. **Batch Poster** - Posts transaction batches to the parent chain

### Architecture diagrams

The architecture varies slightly depending on whether your full nodes use Redis to identify the active sequencer or forward transactions to a predefined endpoint.

#### Send to active enabled

In this configuration, full nodes query Redis to determine the active sequencer and forward transactions directly to it:

<ImageZoom
  src="/img/ha-sequencer-sendto-active.png"
  alt="send to active enable"
  className="img-900px"
/>

#### Send to active disabled

In this configuration, full nodes forward transactions to a predefined endpoint without checking which sequencer is active:

<ImageZoom
  src="/img/ha-sequencer-send-to-nonactive.png"
  alt="send to active disable"
  className="img-900px"
/>

## Using helm charts for deployment

We recommend using the [Offchain Labs community Helm charts](https://github.com/OffchainLabs/community-helm-charts) to deploy your high-availability sequencer setup. These charts provide pre-configured templates for all the necessary components and make it easier to maintain your deployment. Base configuration values are provided in the examples below. However, you should adjust them to fit your needs and use values files for production deployments.

## Detailed component setup

### 1. Load balancing (CDN)

We strongly recommend using a CDN for managing traffic and security concerns:

- **Recommendation**: Use Cloudflare or a similar CDN service
- **Configuration**:
  - Direct RPC traffic to the Nitro full node fleet
  - Direct feed traffic to public-facing relays
  - Implement rate limiting and bot detection as needed
- **Benefits**: Distributes load, improves security, and enhances availability

### 2. Relays setup

The requirement is for two types of relays in the architecture:

#### Sequencer relays

- Deployment should have multiple replicas
- Configure to listen to feed outputs from all sequencers
- All other components connect to these relays instead of directly to the sequencers
- Minimize direct load on sequencer nodes

Deploy sequencer relays using the relay helm chart:

```shell
helm install sequencer-relay offchainlabs/relay \
  --set replicaCount=2 \
  --set configmap.data.chain.id=<your-chain-id> \
  --set configmap.data.node.feed.input.url=ws://sequencer-nitro-0.sequencer-nitro:9642,ws://sequencer-nitro-1.sequencer-nitro:9642,ws://sequencer-nitro-2.sequencer-nitro:9642
```

Key configuration parameters:

- `replicaCount`: Number of relay replicas to deploy (recommend at least two for high availability)
- `configmap.data.chain.id`: Your chain ID
- `configmap.data.node.feed.input.url`: Comma-separated list of WebSocket URLs for all sequencer feed outputs. This relies on the `perReplicaHeadlessService.enabled=true` parameter in the sequencer deployment to create individual services for each sequencer replica.

#### External relays

- Connect to Sequencer Relays (not directly to sequencers)
- Handle all public feed requests
- Provide an additional layer of isolation for production sequencers
- Since these are public facing, ensure they scale appropriately based on your traffic needs:

```shell
helm install external-relay offchainlabs/relay \
  --set replicaCount=2 \
  --set configmap.data.chain.id=<your-chain-id> \
  --set configmap.data.node.feed.input.url=ws://sequencer-relay:9642
```

You can check [run a feed relay](/run-arbitrum-node/run-feed-relay.mdx) to see how to set up a relay node.

### 3. Nitro full node setup

```console
helm install fullnode offchainlabs/nitro \
  --set replicaCount=2 \
  --set configmap.data.parent-chain.id=<parent-chain-id> \
  --set configmap.data.parent-chain.connection.url=<parent-node-url> \
  --set configmap.data.chain.id=<child-chain-id> \
  --set configmap.data.execution.forwarding-target=http://sequencer-nitro:8547
```

#### Send to active configuration (optional)

To enable Redis-based active sequencer discovery:

- Monitor Redis to identify the active Sequencer
- Enable with: `-execution.forwarder.redis-url=redis://<redis-url>:6379`
- Ensure connectivity to individual sequencer services and Redis
- Test failover scenarios before production deployment

```shell
helm install fullnode offchainlabs/nitro \
  --set replicaCount=2 \
  --set configmap.data.parent-chain.id=<parent-chain-id> \
  --set configmap.data.parent-chain.connection.url=<parent-node-url> \
  --set configmap.data.chain.id=<child-chain-id> \
  --set configmap.data.execution.forwarder.redis-url=redis://<redis-url>:6379
```

#### Mutating-only endpoint (optional)

For high availability, it is recommended to route mutating transactions to a fleet of `precheckers`, which will forward them to the sequencer. This configuration can be done by setting up a separate endpoint for mutating transactions and only allowing calls such as `eth_sendRawTransaction` to be routed to the `precheckers`, which then route to the sequencer. This insulates the sequencer from unnecessary load and enables it to focus on transaction ordering. However, , this configuration requires custom load-balancing logic and is out of the scope of this guide.

### 4. Redis setup

Set up a highly available Redis cluster for sequencer coordination:

#### Deployment options

- Use a managed service like AWS ElastiCache (recommended)
- Deploy within Kubernetes using a StatefulSet with PersistentVolumeClaims

#### Requirements

- Minimum of three replicas across different availability zones (recommended)
- Secured access (only accessible within the Kubernetes cluster)
- Backups enabled

- **Configuration**:
  - Use a Redis cluster or Redis Sentinel for high-availability
  - Secure the endpoint with proper network policies
  - Monitor Redis health as part of your overall monitoring strategy

### 5. Sequencer setup

Deploy multiple sequencer replicas with availability zone spread using the nitro helm chart (availability zone spread is not demonstrated in the example below):

```shell
helm install sequencer offchainlabs/nitro \
  --set replicaCount=3 \
  --set configmap.data.parent-chain.id=<parent-chain-id> \
  --set configmap.data.parent-chain.connection.url=<parent-node-url> \
  --set configmap.data.chain.id=<child-chain-id> \
  --set configmap.data.node.sequencer.enable=true \
  --set configmap.data.node.delayed-sequencer.enable=true \
  --set configmap.data.node.seq-coordinator.enable=true \
  --set configmap.data.node.seq-coordinator.redis-url=<redis-url> \
  --set configmap.data.node.feed.output.enable=true \
  --set configmap.data.node.feed.output.port=9642 \
  --set configmap.data.execution.sequencer.enable=true \
  --set perReplicaHeadlessService.enabled=true
```

#### Critical sequencer coordinator parameters

The sequencer coordinator is the key component for high availability. These parameters are essential:

| Parameter                        | Description                  | Recommended Value    |
| -------------------------------- | ---------------------------- | -------------------- |
| `node.seq-coordinator.enable`    | Enable sequencer coordinator | `true`               |
| `node.seq-coordinator.redis-url` | Redis URL for coordination   | Your Redis URL       |
| `node.seq-coordinator.my-url`    | URL for this sequencer       | Unique per sequencer |

#### Setting the sequencer's self URL

A critical configuration for the sequencer coordinator is setting a unique URL for each sequencer instance. This configuration can be adjusted using Kubernetes environment variables:

```yaml
extraEnv:
  - name: POD_NAME
    valueFrom:
      fieldRef:
        fieldPath: metadata.name
  - name: NITRO_NODE_SEQ__COORDINATOR_MY__URL
    value: 'http://$(POD_NAME).<NAMESPACE>.svc.cluster.local:8547/rpc'
```

This configuration:

1. Gets the pod name from Kubernetes metadata
2. Uses it to create a unique URL for each sequencer instance
3. Sets this URL as the `node.seq-coordinator.my-url` parameter

Adjust the domain name (`<NAMESPACE>.svc.cluster.local`) to match your Kubernetes cluster's DNS configuration. This configuration requires Nitro to be configured to read environment variables beginning with `NITRO_`.

#### Individual sequencer services

You can enable the automatic creation of headless services for each sequencer replica by setting the `perReplicaHeadlessService.enabled=true` parameter in the Helm chart (as shown in the installation command above). This configuration will create individual services named `<release-name>-nitro-<index>` that allow direct access to each sequencer replica.

These individual services are critical for a proper high-availability setup because they allow components like sequencer relays to connect directly to specific sequencer instances. This direct connection is essential for:

- **Proper failover**: When the active sequencer changes, other components can address the new active sequencer directly
- **Feed aggregation**: Sequencer relays need to collect feeds from all sequencer instances to ensure no messages get lost during transitions

### 6. Sequencer coordinator manager

To manage active sequencer selection, use the built-in sequencer coordinator UI:

- Follow detailed instructions at: [Running a Sequencer Coordinator Manager](https://docs.arbitrum.io/node-running/how-tos/running-a-sequencer-coordinator-manager)
- Use this interface to switch between sequencer replicas when needed manually
- Configure permissions and access controls appropriately

### 7. `Batchposter` setup

Deploy the batch poster using the Nitro helm chart:

```shell
helm install batchposter offchainlabs/nitro \
  --set configmap.data.parent-chain.id=<parent-chain-id> \
  --set configmap.data.parent-chain.connection.url=<parent-node-url> \
  --set configmap.data.chain.id=<child-chain-id> \
  --set configmap.data.execution.forwarding-target=null \
  --set configmap.data.node.seq-coordinator.enable=true \
  --set configmap.data.node.seq-coordinator.redis-url=<redis-url> \
  --set configmap.data.node.batch-poster.enable=true \
  --set "configmap.data.node.batch-poster.parent-chain-wallet.private-key=<your-private-key>"
```

:::warning Important

Do not add the batch poster to the sequencer priority list in the Sequencer Coordinator Manager (SQM) to prevent it from becoming the active sequencer unintentionally.

:::

## Monitoring and maintenance

### Health checks

Implement comprehensive health checks for all components:

- **Sequencer health**: Monitor the sequencer's logs and metrics
- **Redis connectivity**: Ensure all components can access Redis
- **Feed availability**: Verify feed connectivity between components
- **Transaction processing**: Monitor end-to-end transaction flow

### Troubleshooting

If you run into any issues, visit the [node-running troubleshooting guide](/run-arbitrum-node/06-troubleshooting.mdx).

## References

- [How to Run a Fullnode](https://docs.arbitrum.io/node-running/how-tos/running-a-node)
- [Running a Sequencer Coordinator Manager](https://docs.arbitrum.io/node-running/how-tos/running-a-sequencer-coordinator-manager)
- [Kubernetes Documentation](https://kubernetes.io/docs/)
- [Offchain Labs Community Helm Charts](https://github.com/OffchainLabs/community-helm-charts)

---

# how-arbitrum-works Folder

## .mdx (01-a-gentle-introduction.mdx)
---
title: 'A gentle introduction'
description: 'Learn the fundamentals of Nitro, Arbitrum stack.'
author: dzgoldman
sme: dzgoldman
user_story: As a current or prospective Arbitrum user, I need learn more about Nitros design.
content_type: get-started
---

import ImageWithCaption from '@site/src/components/ImageCaptions/';

:::info

This page gives a high-level overview. It explains <a data-quicklook-from="arbitrum">Arbitrum</a>, its purpose, and a brief "how it works." As you progress through the articles in this section, the content will get more technical and dive deeper into each component(s) and function(s) of the Arbitrum technology stack.

:::

Arbitrum is a technology suite designed to improve Ethereum. You can use Arbitrum chains for the same things you do on Ethereum, like using Web3 apps and deploying smart contracts. The difference is that your transactions will be cheaper and faster. Our main product, Arbitrum Rollup, is an Optimistic Rollup protocol that offers the same security as Ethereum.

### But why?

Ethereum is awesome, but it has its limitations. The Ethereum <a data-quicklook-from="blockchain">Blockchain</a> can handle only about 20 to 40 transactions per second (TPS) for all users. Once it reaches this limit, users must compete for space to have their transactions included, increasing fees.

Arbitrum Rollup solves this issue! Here's how it works: an <a data-quicklook-from="arbitrum-rollup-chain">Arbitrum Rollup Chain</a> acts as a submodule within Ethereum. Unlike regular Ethereum transactions, we don't need Ethereum nodes to handle every Arbitrum transaction. Instead, Ethereum takes an ["innocent until proven guilty"](https://insights.deribit.com/market-research/making-sense-of-rollups-part-2-dispute-resolution-on-arbitrum-and-optimism/) stance with Arbitrum. Initially, the <a data-quicklook-from="parent-chain">Parent chain</a> assumes that actions on Arbitrum follow the rules. If there is a rule violation (like someone claiming, "Now I have all of your money"), that claim is challengeable on the parent chain. In this case, we can prove fraud, disregard the invalid claim, and penalize the guilty party. This ability to investigate and confirm fraud on the parent chain is Arbitrum's main feature and explains why it benefits from Ethereum's security.

### How do these fraud proofs work?

People who help manage the <a data-quicklook-from="arbitrum-chain">Arbitrum chain</a> on the parent chain are called validators. They make claims about the chain's state, dispute others' claims, and more. Most Arbitrum users are unlikely to run a <a data-quicklook-from="validator">validator</a>, just as most Ethereum users do not operate their parent chain staking node. However, anyone can become a validator; you don't need special permission ([once the allowlist gets lifted](https://docs.arbitrum.foundation/state-of-progressive-decentralization)). You only need to run the [open source validator software](https://github.com/OffchainLabs/nitro) and bond `ETH` if required.

The chain stays secure if there is at least one honest validator. It only takes one trustworthy fraud prover to catch many bad actors. These features make the system "<a data-quicklook-from="trustless">Trustless</a>." Users do not depend on any specific group to keep their funds safe.

### Who does this fraud proofing?

This step is where the "Rollup" part comes in. Arbitrum Rollup chains handle user <a data-quicklook-from="transaction">transaction</a> data by posting it directly on Ethereum. This setup means that as long as Ethereum is secure, anyone can see what happens on Arbitrum. They can also spot and prove any fraud that occurs.

Validators are the nodes that help move the Arbitrum <a data-quicklook-from="chain-state">Chain state</a> forward on the parent chain. They make claims about the chain's state and can dispute claims made by others. Most Arbitrum users are unlikely to want to run a validator, just as most Ethereum users typically don't run their parent chain staking nodes. However, anyone can become a validator. Once the allowlist gets removed, users only need to run the open-source validator software and bond `ETH` if they need to take action.

The network stays secure if there is at least one honest validator, which means that just one trustworthy fraud prover can catch any number of bad actors. This setup makes the system "trustless"; users do not have to rely on any specific person to keep their funds safe.

### The game

It's not as complicated as it seems. If two validators disagree, only one is telling the truth. In the event of a dispute, the two validators engage in an interactive game, where they respond to each other in a call-and-response format. This process allows them to narrow their disagreement to a single computational step—something straightforward, like multiplying two numbers. This step executes on the parent chain, which shows which party is honest. For a more detailed explanation, see here.

Users only experience delays when they withdraw funds from Arbitrum back to Ethereum. When withdrawing directly from Arbitrum to Ethereum, users usually wait one week to receive their funds on the parent chain. However, users can skip this waiting period if they use a fast <a data-quicklook-from="bridge">bridge</a> application, often for a small fee. Other activities do not have this delay, like depositing funds from Ethereum to Arbitrum or using a decentralized application (<a data-quicklook-from="dapp">dApp</a>) on the Arbitrum chain.

### How is it cheaper?

Arbitrum helps lower user transaction costs by reducing the strain on the parent chain. The primary way it does this is by processing transactions in batches. A <a data-quicklook-from="batch">batch</a> can contain several hundred <a data-quicklook-from="child-chain">child chain</a> transactions and gets submitted as one parent chain transaction. This batching makes interacting with the parent chain cheaper since you save on overhead costs compared to submitting each transaction individually.

Additionally, Arbitrum posts transaction data on the parent chain in a compressed format. It only decompresses this data within the child chain environment, reducing the amount of information that needs storing on the parent chain.

### How does it all work together?

At the most basic level, an Arbitrum chain works like this:

<ImageWithCaption
  caption="Original napkin sketch drawn by Arbitrum co-founder Ed Felten"
  src="https://lh4.googleusercontent.com/qwf_aYyB1AfX9s-_PQysOmPNtWB164_qA6isj3NhkDnmcro6J75f6MC2_AjlN60lpSkSw6DtZwNfrt13F3E_G8jdvjeWHX8EophDA2oUM0mEpPVeTlMbsjUCMmztEM0WvDpyWZ6R"
/>

Users and contracts put messages into the inbox. The chain reads the messages one at a time and processes each one. This processing updates the state of the chain and produces some outputs.

If you want an Arbitrum chain to process a transaction for you, you need to put that transaction into the chain's inbox. Then, the chain will see your transaction, execute it, and produce some outputs: a transaction receipt and any withdrawals that your transaction initiated.

Execution is deterministic, meaning that the contents of its inbox uniquely determine the chain's behavior. Because of this, the result of your transaction is knowable as soon as it gets put in the inbox. Any Arbitrum node will be able to tell you the result. (And you can run an Arbitrum node yourself if you want.)

All of the technical details in this document connect to this diagram. To get from this diagram to a complete description of Arbitrum, we'll need to answer questions like these:

- Who keeps track of the inbox, Chain state, and outputs?
- How does Arbitrum make sure that the chain state and outputs are correct?
- How can Ethereum users and contracts interact with Arbitrum?
- How does Arbitrum support Ethereum-compatible contracts and transactions?
- How are ETH and tokens transferred into and out of Arbitrum chains, and how are they managed while on the chain?
- How can I run my own Arbitrum node or validator?

### Nitro's Design: The Four Big Ideas

The essence of Nitro and its key innovations lie in four big ideas. We'll list them here with a quick summary of each. We will unpack them in more detail in later sections.

**Big Idea: Sequencing, Followed by Deterministic Execution**: Nitro processes transactions with a two-phase strategy. First, the transactions get organized into a single-ordered sequence, and Nitro commits to that sequence. Then, the transactions get processed in that sequence by a deterministic <a data-quicklook-from="state-transition-function">State Transition Function</a>.

**Big Idea: <a data-quicklook-from="geth">Geth</a> at the Core**: Nitro supports Ethereum's data structures, formats, and virtual machine by compiling in the core code of the popular go-ethereum ("Geth") Ethereum node software. Using Geth as a library in this way ensures a very high degree of compatibility with Ethereum.

**Big Idea: Separate Execution from Proving**: Nitro takes the same source code and compiles it twice, once to native code for execution in a Nitro node, optimized for speed, and again to <a data-quicklook-from="wasm">WASM</a> for use in proving, optimized for portability and security.

**Big Idea: Optimistic Rollup with Interactive Fraud Proofs**: Nitro settles transactions to the parent Ethereum chain using an Optimistic Rollup protocol, including the interactive fraud proofs pioneered by Arbitrum.
Now that we have covered the foundational concepts, the big picture, and the four big ideas of <a data-quicklook-from="arbitrum-nitro">Arbitrum Nitro</a>, we will begin a journey following a transaction through the Arbitrum protocol. In the next section, the transaction lifecycle begins.

---

## .mdx (02-transaction-lifecycle.mdx)
---
title: Sequencing, Followed by Deterministic Execution
description: 'Learn the fundamentals of the Arbitrum Transaction Lifecycle, sequencing, and deterministic execution.'
author: pete-vielhaber
sme: Mehdi
user_story: As a current or prospective Arbitrum user, I need to learn more about the transaction lifecycle.
content_type: get-started
---

import ImageZoom from '@site/src/components/ImageZoom';

This section explores the various methods users can employ to submit transactions for inclusion on the <a data-quicklook-from="arbitrum">Arbitrum</a> chain. We discuss the different pathways available—sending transactions to the <a data-quicklook-from="sequencer">Sequencer</a> or bypassing it by submitting transactions through the <a data-quicklook-from="delayed-inbox">Delayed Inbox</a> contract on the <a data-quicklook-from="parent-chain">parent chain</a>. By outlining these options, we aim to clarify how users can interact with the network, detail the processes involved in each method, and identify the modules responsible for handling these transactions. This overview will enhance your understanding of the initial steps in Arbitrum ecosystem's <a data-quicklook-from="transaction">Transaction</a> lifecycle and prepare you for a detailed exploration of transaction inclusion mechanisms in the subsequent sections.

The first subsection, [Submitting Transactions to the Sequencer](#submitting-transactions-to-the-sequencer), presents four different methods users can utilize to send their transactions to the sequencer: via Public RPC, Third-Party RPC, Arbitrum Nodes, and the Sequencer Endpoint. Transactions sent through the first three pathways will route through our Load Balancer before reaching the sequencer. In contrast, the Sequencer Endpoint allows transactions to bypass the Load Balancer and be sent directly to the sequencer.

The second subsection, [Bypassing the Sequencer](#bypassing-the-sequencer), describes an alternative method where users can include their transactions on the <a data-quicklook-from="arbitrum-chain">Arbitrum chain</a> without relying on the sequencer. By sending transactions directly to the Delayed Inbox contract on the parent chain, users gain additional flexibility, ensuring that their transactions can be processed even if the sequencer is unavailable or if they prefer not to use it.

This diagram illustrates the various pathways for submitting transactions to the Arbitrum chain. It highlights the options for sending transactions through the sequencer or bypassing it and using the Delayed Inbox contract on the parent chain.

<ImageZoom
  src="/img/haw-transaction-lifecycle.svg"
  alt="Transaction lifecycle diagram showing various pathways for submitting transactions"
  className="img-600px"
/>

## Submitting transactions to the Sequencer

This section outlines the different methods for users to submit transactions to the sequencer on the Arbitrum chain. There are four primary ways to do this: Public RPC, Third-Party RPCs, Arbitrum Nodes, and the Sequencer Endpoint. We will explore these methods in detail, explaining when to choose one over the other and how to use each to effectively submit transactions to the Arbitrum sequencer.

### 1. Public RPC

Arbitrum provides public RPCs for its main chains: <a data-quicklook-from="arbitrum-one">Arbitrum One</a>, <a data-quicklook-from="arbitrum-nova">Arbitrum Nova</a>, and Arbitrum Sepolia. Due to their rate-limited nature, these RPC endpoints are suitable for less resource-intensive operations. Public RPCs can be an accessible option for general use cases and light interactions with the network.

For more details on the specific RPC endpoints for each chain, please see [this section](https://docs.arbitrum.io/build-decentralized-apps/reference/node-providers#arbitrum-public-rpc-endpoints) of the documentation.

### 2. Third-Party RPC

Users also have the option to interact with Arbitrum's public chains using third-party node providers. These providers are often the same popular ones used for Ethereum, making them reliable choices for resource-intensive operations. We recommend using these third-party providers when performance and scalability are critical.

You can find a list of supported third-party providers [here](https://docs.arbitrum.io/build-decentralized-apps/reference/node-providers#third-party-rpc-providers).

### 3. Arbitrum Nodes

Another approach for sending transactions to the sequencer is through self-hosted Arbitrum nodes. Running a node gives you direct control over your transactions, which go to the sequencer via the <a data-quicklook-from="sequencer-feed">Sequencer Feed</a>.

Please see the [Arbitrum Node](https://docs.arbitrum.io/run-arbitrum-node/overview) documentation to learn more about setting up and running a node.

### 4. Sequencer Endpoint

The Sequencer Endpoint is the most direct method for users looking to minimize delays in their transactions reaching the sequencer. Unlike standard RPC URLs, the Sequencer Endpoint supports only `eth_sendRawTransaction` and `eth_sendRawTransactionConditional` calls, bypassing the load balancer entirely. This endpoint makes it an optimal choice for users who require the quickest transaction processing time.

The diagram below shows different ways to submit transactions to the Sequencer:

<ImageZoom
  src="/img/haw-submit-tx-to-sequencer.svg"
  alt="Submit transaction to the Sequencer"
  className="img-600px"
/>

## Bypassing the Sequencer

This section delves into an alternative method for submitting transactions to the Arbitrum chain, bypassing the Sequencer. This page focuses on how users can send their transactions directly to the Delayed Inbox contract on the parent chain rather than through the Sequencer. This method offers two distinct paths a transaction can take, with each route interacting with the network differently to achieve transaction inclusion. This approach provides users with greater flexibility and ensures that transactions can still be processed if the Sequencer is unavailable or if users choose not to depend on it. This section highlights these alternative submission mechanisms and underscores the robustness and decentralization features inherent in the Arbitrum network.

In **Diagram 3**, we demonstrate how users can submit their transactions using the Delayed Inbox contract to bypass the Sequencer. As illustrated in the diagram, there are two possible paths for transaction handling. When a transaction is submitted to the Delayed Inbox, the Sequencer may automatically pick it up, include it as an ordered transaction, and send it to the Sequencer feed. However, if the Sequencer does not process the transaction within 24 hours, users have the reliable option to call the `forceInclude` function on the `SequencerInbox` contract. This action ensures that the Sequencer to picks up the transaction and includes it in the ordered transaction list, providing users with a sense of security about their transactions.

<ImageZoom
  src="/img/haw-bypassing-the-sequencer.svg"
  alt="Bypassing the Sequencer"
  className="img-600px"
/>

To send a transaction to the Delayed Inbox instead of submitting it to the sequencer, users can construct their transaction and then call the [`sendL2Message`](https://github.com/OffchainLabs/nitro-contracts/blob/fbbcef09c95f69decabaced3da683f987902f3e2/src/bridge/AbsInbox.sol#L150) function, passing the data of the serialized signed transaction as an argument. This function allows users to send a generic child chain messages to the chain, suitable for any message that does not require parent chain validation.

If the sequencer is not back online within 24 hours or decides to censor the transaction, users can invoke the [`forceInclusion`](https://github.com/OffchainLabs/nitro-contracts/blob/fbbcef09c95f69decabaced3da683f987902f3e2/src/bridge/SequencerInbox.sol#L284) function on the `SequencerInbox` contract. This action ensures their transaction is included on the chain, bypassing the sequencer's role.

Additionally, the Arbitrum SDK provides the [`InboxTools`](https://github.com/OffchainLabs/arbitrum-sdk/blob/792a7ee3ccf09842653bc49b771671706894cbb4/src/lib/inbox/inbox.ts#L64C14-L64C24) class, which simplifies the process of submitting transactions to the Delayed Inbox. Users can utilize the [`sendChildSignedTx`](https://github.com/OffchainLabs/arbitrum-sdk/blob/792a7ee3ccf09842653bc49b771671706894cbb4/src/lib/inbox/inbox.ts#L401C16-L401C33) method to send the transaction and the [`forceInclude`](https://github.com/OffchainLabs/arbitrum-sdk/blob/792a7ee3ccf09842653bc49b771671706894cbb4/src/lib/inbox/inbox.ts#L367) method to ensure its inclusion. The SDK also offers helper methods like [`signChildTx`](https://github.com/OffchainLabs/arbitrum-sdk/blob/792a7ee3ccf09842653bc49b771671706894cbb4/src/lib/inbox/inbox.ts#L429) to assist with signing the transaction during the creation of the serialized signed transaction hex string, streamlining the entire process.

---

## .mdx (03-sequencer.mdx)
---
title: The Sequencer and Censorship Resistance
description: 'Learn the fundamentals of the Arbitrum Sequencer.'
author: pete-vielhaber
sme: Mehdi
user_story: As a current or prospective Arbitrum user, I need to learn more about the Sequencer.
content_type: get-started
---

import ImageZoom from '@site/src/components/ImageZoom';

The <a data-quicklook-from="sequencer">Sequencer</a> is a pivotal component of the <a data-quicklook-from="arbitrum">Arbitrum</a> network and is responsible for efficiently ordering and processing transactions. It plays a crucial role in providing users with fast <a data-quicklook-from="transaction">transaction</a> confirmations while maintaining the security and integrity of the <a data-quicklook-from="blockchain">Blockchain</a>. In Arbitrum, the Sequencer orders incoming transactions and manages the batching, compression, and posting of transaction data to <a data-quicklook-from="parent-chain">parent chain</a>, optimizing costs and performance.

<ImageZoom
  src="/img/haw-sequencer-operations.svg"
  alt="Sequencer operations"
  className="img-600px"
/>

In this section, we will explore the operation of the Sequencer in detail. The topics covered include:

- [Sequencing and Broadcasting (Sequencer Feed)](#sequencing-and-broadcasting): An overview of the real-time transaction feed provided by the Sequencer, which allows nodes to receive instant updates on the transaction sequence.
- [Batch-Posting](#batch-posting): How the Sequencer groups transactions into batches, compresses them to reduce data size and sends them to the Sequencer Inbox Contract on the parent chain. This section also delves into the parent chain pricing model and how it affects transaction costs.
  - [Batching](#batching-and-compression)
  - [Compression](#compression)
  - [Submitting to the Sequencer Inbox Contract](#submitting-to-the-sequencer-inbox-contract)
- [Finality](#finality): Understanding how transaction finality is achieved in Arbitrum through both soft and hard finality mechanisms, ensuring that transactions are confirmed securely and reliably. (not as a sequencer task)
- [Censorship Timeout](#censorship-timeout): A brief introduction to a special feature that aims to limit the negative effects of prolonged sequencer censorship and/or unexpected sequencer outages.

By examining these aspects, you will understand the Sequencer's role within the Arbitrum ecosystem, including how it enhances transaction throughput, reduces latency, and maintains a fair and decentralized network.

## Sequencing and Broadcasting

The **<a data-quicklook-from="sequencer-feed">Sequencer Feed</a>** is a critical component of the Arbitrum network's Nitro architecture. It enables real-time dissemination of transaction data as they are accepted and ordered by the Sequencer. It allows users and nodes to receive immediate updates on transaction sequencing, facilitating rapid transaction confirmations and enhancing the network's overall responsiveness.

### How the Sequencer Publishes the Sequence

The Sequencer communicates the transaction sequence through two primary channels:

1. **Real-Time Sequencer Feed**: A live broadcast that publishes transactions instantly as they are sequenced. Nodes and clients subscribed to this feed receive immediate notifications, allowing them to process transactions without delay.
2. **Batches Posted on the Parent Chain**: At regular intervals, the Sequencer aggregates transactions and posts them to the parent chain for finality. (Refer to the <a data-quicklook-from="batch">Batch</a>-Posting section for detailed information on this process.)

<ImageZoom src="/img/haw-sequencer-feed.svg" alt="Sequencer feed" className="img-600px" />

### Real-Time Sequencer Feed

The real-time feed represents the Sequencer's commitment to process transactions in a specific order. By subscribing to this feed, nodes and clients can:

- **Receive Immediate Notifications**: Obtain instant information about newly sequenced transactions and their ordering.
- **Process Transactions Promptly**: Utilize the sequenced transactions to update the state locally, enabling rapid application responses and user interactions.
- **Benefit from Soft Finality**: Gain provisional assurance about transaction acceptance and ordering before the parent chain reaches finality.

This mechanism is particularly valuable for applications requiring low latency and high throughput, such as decentralized exchanges or real-time gaming platforms.

### Soft Finality and Trust Model

"Soft finality" refers to the preliminary <a data-quicklook-from="confirmation">confirmation</a> of transactions based on the Sequencer's real-time feed. Key aspects include:

- **Dependence on Sequencer Integrity**: The feed's accuracy and reliability depend on the Sequencer operating honestly and without significant downtime.
- **Immediate User Feedback**: Users can act on transaction confirmations swiftly, improving the user experience.
- **Eventual Consistency with the parent chain**: While the real-time feed provides quick updates, ultimate security, and finality are established once transactions are posted to and finalized on the parent chain. (See the **Finality** section for an in-depth discussion.)

Understanding this trust model is essential. While we expect the Sequencer to behave correctly, users and developers should know that soft finality depends on this assumption. In scenarios where absolute certainty is required, parties may wait for transactions to achieve finality on the parent chain.

### Role of the Sequencer Feed in the Network

The Sequencer Feed serves several vital functions within the Arbitrum ecosystem:

- **State Synchronization**: Nodes use the feed to stay synchronized with the latest state of the network, ensuring consistency across the decentralized platform.
- **Application Development**: Developers can build applications that respond instantly to network events, enabling features like live updates, instant notifications, and real-time analytics.
- **Ecosystem Transparency**: The feed promotes transparency and trust within the community by providing visibility into transaction sequencing and network activity.

### Considerations and Limitations

While the Sequencer Feed offers significant advantages, consider the following:

- **Reliance on Sequencer Availability**: The effectiveness of the real-time feed depends on the Sequencer's uptime and responsiveness. Network issues or Sequencer downtime can delay transaction visibility.
- **Provisional Nature of Soft Finality**: Until transactions reach finality on the parent chain, there is a small risk that the provisional ordering provided by the feed could change in exceptional circumstances.
- **Security Implications**: For high-stakes transactions where security is paramount (e.g., centralized exchange deposits and withdrawals), users may prefer to wait for the parent chain confirmation despite the longer latency.

Developers and users should design their applications and interactions with these factors in mind, choosing the appropriate balance between speed and certainty based on their requirements.

### **Delayed messages on the Sequencer feed**

As illustrated in the diagram, the Sequencer feed not only sends <a data-quicklook-from="child-chain">child chain</a> transactions posted directly to the Sequencer but also incorporates parent chain-–submitted child chain transactions. These include child chain messages submitted on the parent chain and retryable transactions. The Sequencer agent monitors the finalized messages submitted to the parent chain's <a data-quicklook-from="delayed-inbox">Delayed Inbox</a> Contract. Once finalized, it processes them as incoming messages to the feed, ensuring they are added as ordered transactions.

It is important to note that the Nitro node can be configured to add Delayed Inbox transactions immediately after their submission to the parent chain, even before finalization. However, this approach introduces a risk of the child chain reorganization if the transaction fails to finalize on the parent chain. To mitigate this risk, on <a data-quicklook-from="arbitrum-one">Arbitrum One</a> and Nova, the Sequencer only includes these transactions in the feed once they are finalized on the Ethereum chain.

You can also explore how the feed sends incoming messages via WebSocket and learn how to extract message data from the feed on this page: [Read Sequencer Feed](/run-arbitrum-node/sequencer/02-read-sequencer-feed.mdx).

## Batch-Posting

Batch-Posting is a fundamental process in the operation of the Sequencer within the Arbitrum network. It involves collecting multiple child chain transactions, organizing them into batches, compressing the data to reduce size, and sending these batches to the Sequencer Inbox Contract on parent chain. This mechanism is crucial for ensuring that transactions are securely recorded on the parent chain blockchain while optimizing for cost and performance.

In this section, we will explore the Batch-Posting process in detail, covering the following topics:

- **Batching**: How the Sequencer groups incoming transactions into batches for efficient processing and posting.
- **Compression**: The methods used to compress transaction data, minimizing the amount of data that needs to be posted on parent chain and thereby reducing costs.
- **Sending to Sequencer Inbox contract**: The procedure for submitting compressed batches to the Sequencer Inbox contract on parent chain, ensuring secure and reliable recording of transactions.

Understanding Batch-Posting is essential for grasping how Arbitrum achieves scalability and cost-efficiency without compromising security. By delving into these subtopics, you'll gain insight into the Sequencer's role in optimizing transaction throughput and minimizing fees, as well as the innovative solutions implemented to address the challenges of parent chain data pricing.

## Batching and compression

The Sequencer in Arbitrum is critical in collecting and organizing child chain transactions before posting them to the parent chain. The batching process is designed to optimize for both cost efficiency and timely transaction inclusion.

<ImageZoom src="/img/haw-batching.svg" alt="Batching" className="img-600px" />

**Transaction Collection and Ordering:**

- **Continuous Reception:** The Sequencer continuously receives transactions submitted by users.
- **Ordering:** Transactions are ordered based on the sequence in which they are received, maintaining a deterministic transaction order.
- **Buffering:** Received transactions are temporarily stored in a buffer awaiting batch formation.

**Batch Formation Criteria:**

- **Size Thresholds:** Batch formation occurs when accumulated transactions reach a predefined size limit. This limit ensures that the fixed costs of posting data to the parent chain are amortized over more transaction, improving cost efficiency.
- **Time Constraints:** The Sequencer also monitors the time elapsed since the last posted batch to prevent undue delays. Upon reaching the maximum time threshold, the Sequencer will create a batch with the transactions collected so far, even if the batch doesn't meet the size threshold.

**Batch Creation Process:**

- **Aggregation:** Once the batch formation criteria (the size or time threshold) are satisfied, the Sequencer aggregates the buffered transactions into a single batch.
- **Metadata Inclusion:** The batch includes the necessary metadata of all transactions.
- **Preparation for Compression:** Batch preparation for the compression stage begins, where techniques will minimize the data size before posting to parent chain.

This batching mechanism allows the Sequencer to efficiently manage transactions by balancing the need for cost-effective parent chain posting with the requirement for prompt transaction processing. By strategically grouping transactions into batches based on size and time criteria, the Sequencer reduces per-transaction costs and enhances the overall scalability of the Arbitrum network.

### Compression

The Sequencer employs compression when forming transaction batches to optimize the data and cost of batches posted to the parent chain. Arbitrum uses the Brotli compression algorithm due to its high compression ratio and efficiency, crucial for reducing parent chain posting costs.

<ImageZoom src="/img/haw-compression.svg" alt="Compression" className="img-600px" />

### Compression level in the Brotli algorithm

Brotli’s compression algorithm includes a parameter: **compression level**, which ranges from **0 to 11**. This parameter allows you to balance two key factors:

- **Compression Efficiency**: Higher levels result in greater size reduction.
- **Computational Cost**: Higher levels require more processing power and time.

As the compression level increases, you achieve better compression ratios at the expense of longer compression times.

### Dynamic compression level setting

The compression level on Arbitrum is dynamically adjusted based on the current backlog of batches waiting to be posted to parent chain by Sequencer. In scenarios where multiple batches are queued in the buffer, the **compression level** can be dynamically adjusted to improve throughput. When the buffer becomes overloaded with overdue batches, the compression level decreases.

This tradeoff prioritizes speed over compression efficiency, enabling faster processing and transmitting pending batches. Doing so, clears the buffer more quickly, ensuring smoother overall system performance.

Now that transactions are batched and compressed, they can be passed to batch-poster to be sent to the parent chain.

## Submitting to the Sequencer Inbox Contract

After batching and compressing transactions, the Sequencer posts these batches to the parent chain to ensure security and finality. This process involves the **Batch Poster**, an Externally Owned Account (EOA) controlled by the Sequencer. The Batch Poster is responsible for submitting the compressed transaction batches to the **Sequencer Inbox Contract** on parent chain.

There are two primary methods the Sequencer uses to send batches to the parent chain, depending on whether the chain supports EIP-4844 (Proto-Danksharding) and the current network conditions:

<ImageZoom
  src="/img/haw-submit-to-sequencer-inbox.svg"
  alt="Submit to Sequencer inbox"
  className="img-600px"
/>

### 1. Using Blobs with `addSequencerL2BatchFromBlobs`

- **Default Approach**: When the parent chain supports EIP-4844, the Sequencer utilizes blob transactions to post batches efficiently.
- **Method**: The Batch Poster calls the `addSequencerL2BatchFromBlobs` function of the Sequencer Inbox Contract.
- **Process**:
  - Batch data gets included as blobs—large binary data structures optimized for scalability.
  - The transaction includes metadata about the batch but does not include the batch data itself in the calldata.
- **Benefits**:
  - **Cost Efficiency**: Blobs allow cheaper data inclusion than calldata, reducing gas costs.
  - **Scalability**: Leveraging blobs enhances the network's ability to handle large volumes of transactions.

### 2. Using Calldata with `addSequencerL2Batch`

- **Alternative Approach**: If the **Blob Base Fee** is significantly high or the blob space is constrained during batch posting, the Sequencer may opt to use calldata.
- **Method**: The Batch Poster calls the `addSequencerL2Batch` function of the Sequencer Inbox contract.
- **Process**:
  - The compressed batch transactions are included directly in the transaction's calldata.
- **Considerations**:
  - **Cost Evaluation**: The Sequencer dynamically assesses whether using calldata is more cost-effective than blobs based on current gas prices and blob fees.
  - **Compatibility**: If the parent chain does not support EIP-4844, this method is the default and only option for batch posting.

:::note
The Sequencer continuously monitors network conditions to choose the most economical method for batch posting, ensuring optimal operation under varying circumstances.
:::

### Authority and Finality

- **Exclusive Access**: Only the Sequencer can call these methods on the Sequencer Inbox Contract. This exclusivity ensures that no other party can directly include messages.
- **Soft-Confirmation Receipts**: The Sequencer's unique ability to immediately process and include transactions allows it to provide users with instant, "soft-confirmation" receipts,
- **Parent chain Finality**: Once batches post, the transactions achieve parent-chain-level finality, secured by Parent chain’s consensus mechanism.

By efficiently sending compressed transaction batches to the Sequencer Inbox contract using the most cost-effective method available, the Sequencer ensures transactions are securely recorded on parent chain. This process maintains the integrity and reliability of the network, providing users with fast and secure transaction processing.

## Finality

Finality in blockchain systems refers to the point at which a transaction becomes irreversible and permanently included in the blockchain's ledger. In the context of Arbitrum's Nitro architecture, understanding finality is crucial for developers and users to make informed decisions about transaction confirmations, security guarantees, and application design.

Arbitrum offers two levels of finality:

1. **Soft Finality**: Provided by the Sequencer's real-time feed, offering immediate but provisional transaction confirmations.
2. **Hard Finality**: Occurs when transactions are included in batches posted to and finalized on the parent chain, providing strong security assurances.

This section explores the concepts of soft and hard finality, their implications, trust considerations, and guidance for utilizing them effectively within the Arbitrum network.

### Soft Finality

Soft finality refers to the preliminary confirmation of transactions based on the Sequencer's real-time feed. Key characteristics include:

- **Immediate Confirmation**: Transactions are confirmed almost instantly as they are accepted and ordered by the Sequencer.
- **Provisional Assurance**: The confirmations are provisional and rely on the Sequencer's integrity and availability.
- **High Performance**: Enables applications to offer rapid responses and real-time interactions, enhancing user experience.

**Advantages of Soft Finality**:

- **Low Latency**: Users receive immediate feedback on transaction status.
- **Optimized for Speed**: Ideal for applications where responsiveness is critical.
- **Improved User Experience**: Reduces waiting times and uncertainty.

**Limitations of Soft Finality**:

- **Trust Dependency**: Relies on the Sequencer's honesty and ability to maintain uptime..
- **Potential for Reordering**: In rare cases, if the Sequencer acts maliciously or encounters issues, the provisional ordering could change.
- **Not Suitable for High-Value Transactions**: For transactions requiring strong security guarantees, soft finality may not suffice.

### Hard Finality

Hard finality occurs when batched transactions get posted to the parent chain. Key characteristics include:

- **Strong Security Guarantees**: When included in blocks on the parent chain, transactions inherit the parent chain's security assurances.
- **Irreversibility**: Once finalized, transactions are immutable and cannot be altered or reversed.
- **Data Availability**: All transaction data is recorded onchain, ensuring transparency and verifiability.

**Advantages of Hard Finality**:

- **Maximum Security**: Protected by the robustness of the parent chain's consensus mechanism.
- **Trust Minimization**: This does not require trust in the Sequencer; security comes from the underlying blockchain.
- **Suitable for High-Value Transactions**: Ideal for scenarios where security and immutability are paramount.

**Limitations of Hard Finality**:

- **Higher Latency**: Achieving hard finality takes longer due to the time required for the parent chain to process and finalize batches.
- **Cost Considerations**: Posting batches to the parent chain incurs fees, which may affect transaction costs.

### Trust Considerations

Understanding the trust assumptions associated with each level of finality is essential:

- **Soft Finality Trust Model**:
  - **Reliance on the Sequencer**: Users must trust that the Sequencer operates honestly, sequences transactions correctly, and remains available.
  - **Risk of Misbehavior**: If the Sequencer acts maliciously, it could reorder or censor certain transactions before they achieve hard finality.
- **Hard Finality Trust Model**:
  - **Reliance on the Parent Chain**: Security is based on the consensus and integrity of the parent chain.
  - **Reduced Trust in Sequencer**: Even if the Sequencer misbehaves, transactions included in posted batches are secured once finalized on the parent chain.

### Application Implications

Developers and users should consider the appropriate level of finality based on their specific use cases:

- **When to Rely on Soft Finality**:
  - **Low-Risk Transactions**: For transactions where the potential impact of reordering or delays is minimal.
  - **User Experience Priority**: Applications where responsiveness and immediacy enhance user engagement, such as gaming or social platforms.
  - **Frequent Transactions**: Scenarios involving a high volume of small transactions where waiting for hard finality is impractical.
- **When to Require Hard Finality**:
  - **High-Value Transactions**: Financial transfers, large trades, or any transaction where security is critical.
  - **Regulatory Compliance**: Situations requiring strict adherence to security standards and auditable records.
  - **Centralized Exchanges (CeXs)**: For deposit and withdrawal operations where certainty of transaction finality is mandatory.

## Censorship Timeout

As mentioned in the original [Arbitrum BoLD Forum Post](https://forum.arbitrum.foundation/t/aip-bold-permissionless-validation-for-arbitrum/23232/70?), the initial release of [Arbitrum BoLD](../how-arbitrum-works/bold/gentle-introduction.mdx) will come with a feature called ”Censorship Timeout” (originally called “Delay Buffer”). For Arbitrum One and Arbitrum Nova, it is proposed that this feature be enabled by default alongside BoLD’s upgrade.

Censorship Timeout aims to limit the negative effects of:

- Prolonged sequencer censorship, and/or,
- Unexpected sequencer outages.

### How the Censorship Timeout feature works

To explain how this feature improves the security of chains settling to Arbitrum One and Arbitrum Nova, consider the scenario where an L3’s parent chain sequencer (the L2 sequencer) is censoring or offline. In such a case, every assertion and/or sub-challenge move would need to wait 24 hours before bypassing the L2 sequencer (using the SequencerInbox’s forceInclusion method described here). In this scenario, challenge resolution would be delayed by a time `t` where `t = (24 hours) * number of moves for a challenge`. To illustrate with sample numbers, if a challenge takes 50 sequential moves to resolve, then the delay would be 50 days.

The Censorship Timeout feature mitigates this by decreasing the force inclusion threshold when unexpected delays in delayed message inclusion occur due to one (or all) of the above mentioned cases of censorship or sequencer outage, enabling entities to make moves without the 24 hour delay-per-move.

The force inclusion window is actually the lesser of either the `delayBuffer` and `delayBlocks`, where `delayBlocks` is a constant, currently set to 24 hours, and `delayBuffer` grows and shrinks between 30 minutes and 48 hours.

The `delayBuffer` value "grows and shrinks" depending on how long the Sequencer is offline or censoring transactions. As a way to measure this behavior, the `delayBuffer` is decremented by the difference between a delayed message's delay beyond the `threshold` and how long it has been delayed (i.e., when some delayed messages are delayed by more than the `threshold`, the difference between the messages' delay and the `threshold` is removed from the buffer). As an example, if the `threshold` is 30 minutes and a message was delayed by 32 minutes, then the `delayBuffer` is decremented by two minutes. The `threshold` is set to 30 minutes on Arbitrum One and 1 hour on Arbitrum Nova.

The `delayBuffer` replenishes at a linear rate when the sequencer is operating correctly at a nominal rate of one minute for every 20 minutes of time where no messages are delayed beyond the `threshold`.

Below are the initial, proposed parameter values for the Censorship Timeout feature for Arbitrum One and Arbitrum Nova:

- `delay buffer` = 14400 parent chain (Ethereum) blocks (2 days)
- `threshold` = 150 L1 Ethereum blocks (30 minutes) for Arbitrum One and 300 parent chain (Ethereum) blocks (one hour) for Arbitrum Nova
- `replenish rate` = 5% (meaning one day is replenished every 20 days or roughly a 95% uptime)

We believe that the Censorship Timeout feature provides stronger guarantees of censorship resistance for Arbitrum chains - especially those that settle to Arbitrum One or Arbitrum Nova. As always, chain owners can decide whether to utilize this feature for their chain and can also change the default parameters as they see fit for their use case.

### Decentralized Fair Sequencing

Arbitrum’s long-term vision includes transitioning from a centralized Sequencer to a decentralized, fair sequencing model. In this framework, a committee of servers (or validators) collectively determines transaction ordering, ensuring fairness, reducing the influence of any single party, and making it more resistant to manipulation. By requiring a supermajority consensus, this approach distributes sequencing power among multiple honest participants, mitigates the risks of front-running or censorship, and aligns with the broader blockchain principles of enhanced security, transparency, and decentralization.

---

## .mdx (04-state-transition-function/01-stf-gentle-intro.mdx)
---
title: 'A gentle introduction'
description: 'Learn the fundamentals of Nitro, Arbitrum stack.'
author: petevielhaber
sme: Mehdi
user_story: As a current or prospective Arbitrum user, I need learn more about Nitros design.
content_type: get-started
---

A <a data-quicklook-from="state-transition-function">State Transition Function</a> (STF) is a core mechanism in <a data-quicklook-from="blockchain">blockchain</a> systems that defines how the system evolves from one state to another when transactions are processed. At its essence, the STF takes the current state of the blockchain–a comprehensive snapshot of account balances, <a data-quicklook-from="smart-contract">smart contract</a> data, and other ledger information–and an input (such as a <a data-quicklook-from="transaction">transaction</a> or a block of transactions) and deterministically computes the new state. This deterministic property is crucial because it ensures that all nodes in a decentralized network arrive at the same result when applying transactions, thereby maintaining consensus across the system.

With <a data-quicklook-from="arbitrum">Arbitrum</a>, the STF plays an even more pivotal role. Offchain transactions are executed in batches according to this function, with the STF periodically submitting a concise summary of changes to the <a data-quicklook-from="parent-chain">parent chain</a>. This approach leverages offchain computation to boost throughput and reduce gas costs while anchoring the process to Ethereum's robust security model. To safeguard against incorrect or malicious offchain execution, Arbitrum employs a <a data-quicklook-from="challenge">challenge</a> mechanism known as fraud proofs. If a dispute arises, the STF can be recomputed step-by-step onchain, enabling the network to verify the validity of the offchain computations and ensure that the errors or fraudulent behavior are detected and rectified.

Beyond these foundational aspects, the STF on the <a data-quicklook-from="arbitrum-nitro">Arbitrum Nitro</a> Stack closely mirrors Ethereum's STF in its overall structure but incorporates several key modifications to accommodate the unique requirements of the <a data-quicklook-from="arbitrum-chain">Arbitrum chain</a>. Essentially, the function receives a sequence of ordered transactions as its input, applies a defined set of rules to these inputs, and produces a new final state that reflects all the updates from the <a data-quicklook-from="batch">batch</a> of transactions.

With the introduction of <a data-quicklook-from="stylus">Stylus</a>, Arbitrum expands its execution model beyond the Ethereum Virtual Machine (EVM) by adding WebAssembly (<a data-quicklook-from="wasm">WASM</a>)–based smart contract execution. This enhancement allows high-performance smart contracts written in Rust, C, and C++ to run alongside traditional EVM contracts. The integration of Stylus introduces several modifications to the STF, including:

### Stylus-specific transaction processing

A modified version of <a data-quicklook-from="geth">Geth</a> that recognizes and processes Stylus transactions, ensuring proper inclusion in state transitions.

### Execution in a WASM runtime

Stylus transactions execute in <a data-quicklook-from="arbos">ArbOS</a>'s WASM runtime instead of the EVM, enabling faster execution and more efficient computation.

### Stylus gas accounting and pricing

Unlike standard EVM transactions, Stylus transactions introduce new gas pricing models that account for factors such as opcode pricing, host I/O operations, and <a data-quicklook-from="ink">Ink</a> usage costs.

### Interoperability with the EVM

Stylus contracts can interact seamlessly with Solidity contracts, enabling hybrid applications that leverage EVM and WASM execution environments.

These Stylus-related changes aim to maintain compatibility with Ethereum's execution model while introducing a more efficient, flexible, and scalable alternative for smart contract development.

In the following sections, we will explain what these inputs are and how Arbitrum nodes receive them. We will also discuss the rules implemented in the Arbitrum Nitro Stack's STF, highlighting the differences between how state transitions occur in Ethereum, Arbitrum, and Stylus-based execution environments. Stylus-specific execution tasks handled within ArbOS will be covered separately, focusing on host I/O operations, caching, and WASM memory management.

---

## .mdx (04-state-transition-function/02-stf-inputs.mdx)
---
title: 'Inputs to the State Transition Function'
description: 'Learn the fundamentals of Nitro, Arbitrum stack.'
author: petevielhaber
sme: Mehdi
user_story: As a current or prospective Arbitrum user, I need learn more about Nitros design.
content_type: get-started
---

<a data-quicklook-from="arbitrum">Arbitrum</a> nodes receive <a data-quicklook-from="transaction">
  transaction
</a> inputs through a dual-channel approach that enhances reliability and consistency.

First, nodes subscribe
to the <a data-quicklook-from="sequencer">Sequencer</a> feed, receiving the upcoming ordered transactions
published in real time. To learn more, refer to the [real-time sequencer feed documentation](/how-arbitrum-works/03-sequencer.mdx#real-time-sequencer-feed).

In addition to the <a data-quicklook-from="sequencer-feed">Sequencer Feed</a>, Arbitrum nodes subscribe to the `SequencerBatchDelivered` event on the <a data-quicklook-from="parent-chain">parent chain</a>. This event occurs whenever a <a data-quicklook-from="batch">batch</a> of transactions gets delivered to the parent chain via the batch poster. Upon receiving this event, nodes verify that the transactions recorded on the parent chain match those previously provided by the Sequencer feed. If discrepancies arise, nodes re-organize to adopt the transactions confirmed on the parent chain, using the parent chain as the definitive source of truth.

As discussed in the [considerations and limitations section](/how-arbitrum-works/03-sequencer.mdx#considerations-and-limitations), these methods may be more appropriate for different applications depending on their specific requirements.

Arbitrum nodes reliably receive an ordered set of transactions through these mechanisms, which serve as inputs for the <a data-quicklook-from="state-transition-function">State Transition Function</a> (STF). The function then processes these transactions and outputs the final state, ensuring consistency and security across the network.

## Message types

Arbitrum supports submitting a variety of message types to its chains. These messages fall into two broad categories:

- Messages submitted directly to the Sequencer as <a data-quicklook-from="child-chain">child chain</a> messages
- Messages submitted to the parent chain

For more details on message types beyond standard child chain transactions, please refer to the [parent-to-child chain messaging documentation](/how-arbitrum-works/10-l1-to-l2-messaging.mdx).

In the system, we refer to these messages as `L1IncomingMessage` [source code reference](https://github.com/OffchainLabs/nitro/blob/4ac7e9268e9885a025e0060c9ec30f9612f9e651/arbos/incomingmessage.go#L54).

When submitted to the Sequencer feed, messages are tagged with a unique identifier, ensuring Arbitrum nodes correctly differentiate and process them. Below is the list of message types, their associated constant values, and descriptions:

```solidity
uint8 constant L2_MSG = 3;
uint8 constant L1MessageType_L2FundedByL1 = 7;
uint8 constant L1MessageType_submitRetryableTx = 9;
uint8 constant L1MessageType_ethDeposit = 12;
uint8 constant L1MessageType_batchPostingReport = 13;
uint8 constant L2MessageType_unsignedEOATx = 0;
uint8 constant L2MessageType_unsignedContractTx = 1;

uint8 constant ROLLUP_PROTOCOL_EVENT_TYPE = 8;
uint8 constant INITIALIZATION_MSG_TYPE = 11;
```

- `L2_MSG` (3): Child chain messages submitted directly to the Sequencer.

- `L1MessageType_L2FundedByL1` (7): Child chain messages that go to the parent chain's <a data-quicklook-from="delayed-inbox">Delayed Inbox</a>, with funding provided on the parent chain itself.

- `L1MessageType_submitRetryableTx` (9): Submitting parent chain messages to the child chain via retryable tickets.

- `L1MessageType_ethDeposit` (12): Child chain messages that handle deposits of native tokens (`ETH`) into the child chain.

- `L1MessageType_batchPostingReport` (13): Used by the Sequencer to update the pricing model based on payment(s) by the batch poster.

- `L2MessageType_unsignedEOATx` (0): Child chain messages submitted by externally owned accounts (EOAs) to the parent chain that does not include a signature.

- `L2MessageType_unsignedContractTx` (1): Child chain messages submitted on the parent chain that does not include a signature.

- `ROLLUP_PROTOCOL_EVENT_TYPE` (8): <a data-quicklook-from="arbitrum-classic">Arbitrum Classic</a> used it for messages sent to <a data-quicklook-from="bridge">bridge</a>, Nitro does not use it.

- `INITIALIZATION_MSG_TYPE` (11): The first message added to a new Rollup inbox. Its presence indicates proper initialization of the Rollup.

These tagged identifiers enable Arbitrum nodes to parse incoming messages and direct their payloads appropriately, ensuring smooth and reliable network operation.

---

## .mdx (04-state-transition-function/03-ethereum-vs-arbitrum.mdx)
---
title: 'State Transition Function: Ethereum vs. Arbitrum'
description: 'Learn the fundamentals of Nitro, Arbitrum stack.'
author: petevielhaber
sme: Mehdi
user_story: As a current or prospective Arbitrum user, I need learn more about Nitros design.
content_type: get-started
---

import ImageZoom from '@site/src/components/ImageZoom';

This section explains the architecture and implementation of the <a data-quicklook-from="state-transition-function">State Transition Function</a> (STF) in Ethereum and the <a data-quicklook-from="arbitrum">Arbitrum</a> Nitro stack, highlighting their similarities and key differences.

## STF in Ethereum

In Ethereum, the STF receives transactions as inputs, processes them via the EVM, and produces the final state as output.

The Ethereum state is a vast data structure represented by a modified Merkle Patricia Trie. This structure holds all accounts, linking them via hashes and reducing the entire state to a single root hash stored on the <a data-quicklook-from="blockchain">blockchain</a>.

The Ethereum Virtual Machine (EVM) operates similarly to a mathematical function: given an input, it produces a deterministic output. Ethereum's STF encapsulates this behavior:

$$
Y(S, T) = S'
$$

Here, `S` represents the current state, `T` denotes the <a data-quicklook-from="transaction">transaction</a>, and `S'` is the new state resulting from the execution of `T`.

The EVM operates as a stack machine with a maximum depth of 1024 items. Each item is a 256-bit word, chosen for compatibility with 256-bit cryptography (e.g., Keccak-256 hashes and secp256k1 signatures).

During execution, the EVM uses transient _memory_ (a word-addresses byte array) that only persists for the duration of a transaction. In contrast, each contract maintains a persistent Merkle Patricia _storage_ trie–a word-addressable word array–that forms part of the global state.

<a data-quicklook-from="smart-contract">smart contract</a> bytecode compiles into a series of EVM opcodes
that perform standard stack operations (such as `XOR`, `AND`, `ADD`, `SUB`) and blockchain-specific operations
(such as `ADDRESS`, `BALANCE`, `BLOCKHASH`).

<a data-quicklook-from="geth">Geth</a> (go-Ethereum) is one of the primary <a data-quicklook-from="client">
  client
</a> implementations of Ethereum, serving as the practical embodiment of both the STF and the EVM execution
engine. It processes transactions by executing the smart contract's bytecode and updating the global
state, ensuring that every state change is deterministic and secure.

In essence, Geth converts transaction inputs into precise computational steps within the EVM, maintaining the intricate data structures that underpin Ethereum's blockchain. Its robust design not only powers the core operations of Ethereum but also provides the foundation for advanced modificaitons in platforms like the <a data-quicklook-from="arbitrum-nitro">Arbitrum Nitro</a> stack.

## STF on Arbitrum

The Arbitrum Nitro stack implements a modified version of Ethereum's STF. While it retains the core principles of Ethereum, several Arbitrum-specific features and processes distinguish it from Ethereum's implementation. Key differences include:

### Gas accounting

In Arbitrum, executing a transaction incurs two costs, one for executing on the <a data-quicklook-from="child-chain">child chain</a> and another for submitting transaction batches to the <a data-quicklook-from="parent-chain">parent chain</a>. This dual fee structure requires a different approach to gas accounting.

### Base fee mechanism

Gas prices and base fees on Arbitrum change differently from Ethereum's. Arbitrum maintains two base fees: one to track the cost of execution on the child chain and another to estimate the cost of posting batches to the parent chain. The mechanism that changes the child chain's base fee differs from what is currently on Ethereum.

### Cross-chain <a data-quicklook-from="bridge">bridge</a> functionalities

Arbitrum supports cross-chain messaging and regular transactions, facilitating seamless interactions between the child and parent chains.

### Block time and block number

The <a data-quicklook-from="arbitrum-chain">Arbitrum chain</a> uses two block numbers: the child chain's block number, and the block number of the first non-Arbitrum ancestor chain. Additionally, block times are determined by the child chain's sequencer clock, as described in the [Block numbers and time documentation](/build-decentralized-apps/arbitrum-vs-ethereum/02-block-numbers-and-time.mdx).

### Precompiles

Specific precompiled contracts on the Arbitrum chain will process Arbitrum-specific features. Moreover, individual Arbitrum chains may implement their own custom precompiles.

### Custom transaction types

Nitro Geth introduces several child chain-specific transaction types that utilize the non-standard [EIP-2718: Typed Transaction Envelope](https://eips.ethereum.org/EIPS/eip-2718). These types represent different transactions on the Arbitrum chain, such as retryable transactions.

### <a data-quicklook-from="stylus">Stylus</a> in the STF

With the introduction of Stylus, Arbitrum Nitro now supports executing smart contracts written in Rust, C, and C++, offering a high-performance alternative to the EVM. Unlike standard Ethereum transactions, Stylus contracts execute in a <a data-quicklook-from="wasm">WASM</a>-based runtime within <a data-quicklook-from="arbos">ArbOS</a>, leveraging efficient host I/O operations and optimized caching mechanisms.

## Implementation overview

The Arbitrum STF is implemented using a modified version of Geth and integrated with ArbOS. This combination allows Arbitrum to support additional functionalities while maintaining compatibility with Ethereum's core execution model. The following diagram provides an abstract overview:

<ImageZoom src="/img/haw-geth-sandwich.svg" alt="Geth sandwich" className="img-600px" />

The organization of the Nitro node software has three main layers:

1. **Base layer (Geth Core)**: This layer comprises the core components of Geth that emulate EVM contract execution and maintain Ethereum's state data structures. Nitro integrates this code as a library with minor modifications to insert additional hooks.

2. **Middle layer (ArbOS)**: ArbOS is custom software that extends child chain functionalities. It decompresses and parses <a data-quicklook-from="sequencer">Sequencer</a> data batches, accounts for parent chain gas costs (collecting fees for reimbursement), and supports cross-chain bridge operations (e.g., Ether and token deposits and withdrawals). More details about ArbOS are in the [next section](#arbos-overview).

3. **Top layer (Node software)**: Primarily derived from Geth, this layer handles network connections, incoming RPC requests, and other high-level functionalities required to operate an Ethereum-compatible blockchain node.

Because both the top and bottom layers are Geth-based, this architecture is often referred to as a "geth sandwich"–with Geth as the "bread" and ArbOS as the "filling."

The State Transition Function (STF) itself consists of the modified Geth core (base layer) combined with a portion of ArbOS (middle layer). In practice, the STF is a designated function within the source code (including all the code it calls), which:

- Takes the bytes of a transaction received in the inbox as input.

- Operates on a modifiable copy of the Ethereum state tree.

- May modify the state during execution.

- Emits a new block header (formatted in Ethereum's block header structure) that appends to the Nitro chain.

## Modified Geth on Nitro

[Nitro's](https://github.com/OffchainLabs/nitro) design builds upon the robust foundation of Geth with targeted modifications that enable it to support child chain-specific functionalities. These modifications include:

### Custom hooks and interface adaptions

Nitro integrates a series of custom hooks within Geth's transaction processing flow. These hooks allow Nitro to implement dual gas accounting, manage parent chain calldata fees, and handle custom transaction types–such as retryable transactions–without deviating from Ethereum's core logic.

### Strategic re-appropriations of core types

By adapting key data structures and interfaces within Geth, Nitro ensures that state transitions and transaction executions can seamlessly accommodate child chain optimizations. These adaptations maintain compatibility with Ethereum's existing architecture while enabling enhanced functionality.

### Minimal yet essential adjustments

The modifications made to Geth are deliberately minimal to preserve its stability and security properties. This "modified Geth" layer, in tandem with ArbOS, provides the necessary enhancements for Nitro's STF without compromising the deterministic behavior of the EVM.

### Stylus-specific features

Geth is modified to route Stylus transactions, handle custom gas accounting, and track Stylus-specific state changes, ensuring seamless execution within Arbitrum Nitro.

Detailed technical explanations of these modifications are in the following sections, where we dive deeper into how Nitro's custom hooks, interface implementations, and Geth adaptations come together to support an advanced, high-performance STF.

## ArbOS overview

ArbOS is the child chain EVM hypervisor that provides the execution environment for the Arbitrum chain. Acting as a trusted "system glue" component within the STF, ArbOS is responsible for:

### Managing network resources

It allocates and tracks resources necessary for executing transcations on the child chain.

### Block production

ArbOS processes incoming sequencer data batches to produce child chain blocks, ensuring the state is updated correctly.

### Cross-chain messaging

It facilitates communication between the parent and child chains, supporting functionalities like Ether and token deposits and withdrawals.

### Enhanced EVM execution

ArbOS operates its instrumented instance of Geth to execute smart contracts, incorporating additional logic specific to the child chain environment.

### Stylus-specific tasks in ArbOS

ArbOS manages host I/O calls, memory operations, and execution context for Stylus transactions, ensuring efficient and deterministic processing with the WASM runtime.

By offloading tasks that would execute at a high cost on the parent chain, ArbOS enables these operations to be performed quickly and cost-effectively on the child chain. This design reduces computational and storage costs and offers significant flexibility, allowing the child chain code to evolve or be customized more easily than a parent chain-enforced architecture.

While Ethereum's STF forms the basis for secure and deterministic state updates, Arbitrum's Nitro stack builds on this foundation with key modifications–ranging from dual gas accounting to cross-chain messaging–to optimize performance and flexibility. These innovations are realized through minimal yet strategic modifications to Geth, integrated seamlessly with ArbOS, forming the "geth sandwich."

With the introduction of Stylus, Arbitrum extends its execution model beyond the EVM, enabling high-performance WASM-based smart contracts. This integration introduces additional modifications to Geth, ensuring compatibility with Stylus transactions while preserving Ethereum-like execution guarantees. These changes include handling Stylus-specific transaction types and ensuring smooth interaction between the EVM and WASM environments.

In the following section, we'll dive deep into these modifications, exploring how Nitro leverages 'Geth at the core' and the custom enhancements provided by ArbOS to deliver an advanced, high-performance STF. Stylus-specific tasks within ArbOS are covered separately to highlight its role in managing execution, host I/O, and memory operations.

---

## .mdx (04-state-transition-function/04-modified-geth-on-arbitrum.mdx)
---
title: 'Geth at the core: modified Geth on Arbitrum Nitro'
description: 'Learn the fundamentals of Nitro, Arbitrum stack.'
author: petevielhaber
sme: Mehdi
user_story: As a current or prospective Arbitrum user, I need learn more about Nitros design.
content_type: get-started
---

Nitro makes minimal modifications to <a data-quicklook-from="geth">Geth</a> in hopes of not violating its assumptions. This section will explore the relationship between Geth and <a data-quicklook-from="arbos">ArbOS</a>, which consists of a series of hooks, interface implementations, and strategic re-appropriations of Geth's basic types.

We store ArbOS's state at an address inside a Geth `statedb`. In doing so, ArbOS inherits the `statedb`'s statefulness and lifetime properties. For example, a <a data-quicklook-from="transaction">Transaction</a>'s direct state changes to ArbOS would get discarded upon a revert.

`0xA4B05FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF` is a fictional account representing ArbOS.

:::info

Any links on this page may reference old releases of Nitro or our fork of Geth. While we try to keep this up to date, and most of this should be stable, please check against the latest releases for [Nitro](https://github.com/OffchainLabs/nitro/releases) and [Geth](https://github.com/OffchainLabs/go-ethereum/releases) for the most recent changes.

:::

## Hooks

<a data-quicklook-from="arbitrum">Arbitrum</a> uses various hooks to modify Geth's behavior when processing
transactions. Each provides an opportunity for ArbOS to update its state and make decisions about the
transaction during its lifetime. Transactions are applied using Geth's [`ApplyTransaction`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/state_processor.go#L152)
function.

Below is `ApplyTransaction`'s callgraph, with additional info on where the various Arbitrum-specific hooks are. Click on any to go to their section. By default, these hooks do nothing to leave Geth's default behavior unchanged, but for chains configured with [`EnableArbOS`](#enablearbos) set to true, [`ReadyEVMForL2`](#readyevmforl2) installs the alternative <a data-quicklook-from="child-chain">child chain</a> hooks.

- `core.ApplyTransaction` -> `core.applyTransaction` -> `core.ApplyMessage`
  - `core.NewStateTransition`
    - `ReadyEVMForL2`
  - `core.TransitionDb`
    - [`StartTxHook`](#starttxhook)
    - `core.transitionDbImpl`
      - if `IsArbitrum()` remove tip
      - [`GasChargingHook`](#gascharginghook)
      - `evm.Call`
        - `core.vm.EVMInterpreter.Run`
          - [`PushCaller`](#pushcaller)
          - `PopCaller`
      - `core.StateTransition.refundGas`
        - [`ForceRefundGas`](#forcerefundgas)
        - [`NonrefundableGas`](#nonrefundablegas)
      - [`EndTxHook`](#endtxhook)
  - added return parameter: `transactionResult`

What follows is an overview of each hook in chronological order.

### [`ReadyEVMForL2`](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbstate/geth-hook.go#L47)

A call to `ReadyEVMForL2` installs the other transaction-specific hooks into each Geth `EVM` right before it performs a state transition. Without this call, the state transition will instead use the default `DefaultTxProcessor` and get the same results as vanilla Geth. A `TxProcessor` object carries these hooks and the associated Arbitrum-specific state during the transaction's lifetime.

### [`StartTxHook`](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/tx_processor.go#L100)

Geth calls the `StartTxHook` before a transaction executes, which allows ArbOS to handle two Arbitrum-specific transaction types.

If the transaction is `ArbitrumDepositTx`, ArbOS adds balance to the destination account. This approach is safe because the <a data-quicklook-from="parent-chain">parent chain</a> <a data-quicklook-from="bridge">bridge</a> submits such a transaction only after collecting the same amount of funds on the parent chain.

If the transaction is an `ArbitrumSubmitRetryableTx`, ArbOS creates a retryable based on the transaction's fields. ArbOS schedules a retry of the new retryable if the transaction includes sufficient gas.

The hook returns `true` for both transaction types, signifying that the state transition is complete.

### [`GasChargingHook`](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/tx_processor.go#L354)

This fallible hook ensures the user has enough funds to pay their poster's parent chain calldata costs. If not, the transaction is reverted, and the EVM does not start. In the common case, that the user can pay, the amount paid for calldata is set aside for later reimbursement of the poster. All other fees go to the network account, as they represent the transaction's burden on validators and nodes more generally.

Suppose the user attempts to purchase compute gas over ArbOS's per-block gas limit. In that case, the difference is [set aside](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/tx_processor.go#L407) and [refunded later](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/state_transition.go#L419) via `ForceRefundGas` so only the gas limit is used. Note that the limit observed may not be the same as that seen [at the start of the block](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/block_processor.go#L176) if ArbOS's larger gas pool falls below the [`MaxPerBlockGasLimit`](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/l2pricing/l2pricing.go#L86) while processing the block's previous transactions.

### [`PushCaller`](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/tx_processor.go#L76)

These hooks track the callers within the EVM callstack, pushing and popping as calls are made and completed. This hook provides [`ArbSys`](/build-decentralized-apps/precompiles/02-reference.mdx#arbsys) with info about the callstack, which is used to implement the methods `WasMyCallersAddressAliased` and `MyCallersAddressWithoutAliasing`.

### [`L1BlockHash`](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/tx_processor.go#L617)

In Arbitrum, the `BlockHash` and `Number` operations return data that relies on the underlying parent chain blocks instead of child chain blocks to accommodate the normal use-case of these opcodes, which often assumes Ethereum-like time passes between different blocks. The `L1BlockHash` and `L1BlockNumber` hooks have the required data for these operations.

### [`ForceRefundGas`](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/tx_processor.go#L425)

This hook allows ArbOS to add additional refunds to the user's transaction. The only usage of this hook is to refund any compute gas purchased in excess of ArbOS's per-block gas limit during the `GasChargingHook`.

### [`NonRefundableGas`](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/tx_processor.go#L418)

Because poster costs come at the expense of the parent chain aggregators and not the network–the amounts paid for the parent chain calldata should not be refunded. This hook provides Geth access to the equivalent amount of child chain gas the poster's cost equals, ensuring reimbursement for this amount doesn't occur for network-incentivized behaviors like freeing storage slots.

### [`EndTxHook`](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/tx_processor.go#L429)

The `EndTxHook` calls after the `EVM` has returned a transaction's result, allowing one last opportunity for ArbOS to intervene before the state transition finalization. Final gas amounts are known, enabling ArbOS to credit the network and poster share of the user's gas expenditures and adjust the pools. The hook returns from the [`TxProcessor`](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/tx_processor.go#L38) a final time, discarding its state as the system moves on to the next transaction, where its contents will renew.

## Interfaces and components

### [`APIBackend`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/arbitrum/apibackend.go#L34)

`APIBackend` implements the `ethapi.Backend` interface, which allows a simple integration of the <a data-quicklook-from="arbitrum-chain">Arbitrum chain</a> to the existing Geth API. The `Backend` member answers most calls.

### [`Backend`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/arbitrum/backend.go#L15)

This struct is an Arbitrum equivalent to the [`Ethereum`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/eth/backend.go#L68) struct. It is mostly glue logic, including a pointer to the `ArbInterface` interface.

### [`ArbInterface`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/arbitrum/arbos_interface.go#L10)

This interface is the main interaction between geth-standard APIs and the Arbitrum chain. Geth APIs either check the status by working on the `Blockchain` struct retrieved from the `Blockchain` call or send transactions to Arbitrum using the [`PublishTransactions`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/arbitrum/arbos_interface.go#L11) call.

### [`RecordingKV`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/arbitrum/recordingdb.go#L22)

`RecordingKV` is a read-only key-value store that retrieves values from an internal trie database. All values accessed by a `RecordingKV` get recorded internally. This value records all preimages accessed during block creation, which will be needed to prove the execution of this particular block. A [`RecordingChainContext`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/arbitrum/recordingdb.go#L123) should also be used to record which block headers the block execution reads (another option would always be to assume the last 256 block headers were accessed). The process is simplified using two functions: [`PrepareRecording`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/arbitrum/recordingdb.go#L152) creates a stateDB and chain context objects, running block creation process using these objects records the required preimages, and [`PreimagesFromRecording`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/arbitrum/recordingdb.go#L174) function extracts the preimages recorded.

## Transaction types

Nitro Geth includes a few child chain-specific transaction types. Click any to jump to their section.

| Transaction Type                                          | Represents                                                               | Last Hook Reached             | Source      |
| --------------------------------------------------------- | ------------------------------------------------------------------------ | ----------------------------- | ----------- |
| [`ArbitrumUnsignedTx`](#arbitrumunsignedtx)               | A parent chain to child chain message                                    | [`EndTxHook`](#endtxhook)     | Bridge      |
| [`ArbitrumContractTx`](#arbitrumcontracttx)               | A nonce-less parent chain to child chain message                         | [`EndTxHook`](#endtxhook)     | Bridge      |
| [`ArbitrumDepositTx`](#arbitrumdeposittx)                 | A user deposit                                                           | [`StartTxHook`](#starttxhook) | Bridge      |
| [`ArbitrumSubmitRetryableTx`](#arbitrumsubmitretryabletx) | Creating a retryable                                                     | [`StartTxHook`](#starttxhook) | Bridge      |
| [`ArbitrumRetryTx`](#arbitrumretrytx)                     | A <a data-quicklook-from="retryable-redeem">retryable redeem</a> attempt | [`EndTxHook`](#endtxhook)     | Child chain |
| [`ArbitrumInternalTx`](#arbitruminternaltx)               | ArbOS state update                                                       | [`StartTxHook`](#starttxhook) | ArbOS       |

The following reference documents each type.

### [`ArbitrumUnsignedTx`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/types/arb_types.go#L43)

It provides a mechanism for a user on a parent chain to message a contract on a child chain. This mechanism uses the bridge for authentication rather than requiring the user's signature. Address remapping of the user's address will occur on the child chain to distinguish them from a normal child chain caller.

### [`ArbitrumContractTx`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/types/arb_types.go#L104)

These are like [`ArbitrumUnsignedTx`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/types/arb_types.go#L43)'s but intended for smart contracts. These use the bridge's unique, sequential nonce rather than requiring the caller to specify their own. A parent chain contract may still use an `ArbitrumUnsignedTx`, but doing so may necessitate tracking the nonce in the parent <a data-quicklook-from="chain-state">chain state</a>.

### [`ArbitrumDepositTx`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/types/arb_types.go#L338)

It represents a user deposit from a parent chain to a child chain. This representation increases the user's balance by the amount deposited on the parent chain.

### [`ArbitrumSubmitRetryableTx`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/types/arb_types.go#L232)

It represents a retryable submission and may schedule an [`ArbitrumRetryTx`](#arbitrumretrytx) if enough gas is available. Fore more info, please see the [retryables documentation](/how-arbitrum-works/10-l1-to-l2-messaging.mdx#retryable-tickets).

### [`ArbitrumRetryTx`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/types/arb_types.go#L161)

These calls get scheduled by calls using the `redeem` method of the [`ArbRetryableTx`](/build-decentralized-apps/precompiles/02-reference.mdx#arbretryabletx) precompile and via retryable auto-redemption. Fore more info, please see the [retryables documentation](/how-arbitrum-works/10-l1-to-l2-messaging.mdx#retryable-tickets).

### [`ArbitrumInternalTx`](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/internal_tx.go)

Because tracing support requires ArbOS's state changes to happen inside a transaction, ArbOS may create a transaction of this type to update its state between user-generated transactions. Such a transaction has a [`Type`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/types/arb_types.go#L387) field signifying the state it will update, though currently, this is just future-proofing as there's only one value it may have. Below are the internal transaction types.

### [`InternalTxStartBlock`](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/internal_tx.go#L22)

It updates the parent chain block number and the parent chain base fee. This transaction [generates](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/block_processor.go#L181) whenever a new block gets created. They are [guaranteed to be the first](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/block_processor.go#L182) in their child block chain.

## Transaction run modes and underlying transactions

A [geth message](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/types/transaction.go#L634) may get processed for various purposes. For example, a message may estimate the gas of a contract call, whereas another may perform the corresponding state transition. Nitro Geth denotes the intent behind a message using [`TxRunMode`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/types/transaction.go#L701), [which it sets](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/internal/ethapi/api.go#L955) before processing. ArbOS uses this info to decide the transaction the message ultimately constructs.

A message [derived from a transaction](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/types/transaction.go#L676) will carry that transaction in a field accessible via its [`UnderlyingTransaction`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/types/transaction.go#L700) method. While this relates to how a given message is used, they are not one-to-one. The table below shows the various run modes and whether each could have an underlying transaction.

| Run Mode                                                                                                                                               | Scope            | Carries an Underlying Transaction?                                                                             |
| ------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------- | -------------------------------------------------------------------------------------------------------------- |
| [`MessageCommitMode`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/types/transaction.go#L654)        | state transition | Always                                                                                                         |
| [`MessageGasEstimationMode`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/types/transaction.go#L655) | gas estimation   | When created via [`NodeInterface`](/build-decentralized-apps/nodeinterface/02-reference.mdx) or when scheduled |
| [`MessageEthcallMode`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/types/transaction.go#L656)       | eth_calls        | Never                                                                                                          |

## Arbitrum chain parameters

Nitro's Geth is configurable with the following [child chain-specific chain parameters](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/params/config_arbitrum.go#L25). These allow the rollup creator to customize their rollup at genesis.

### `EnableArbos`

Introduces [ArbOS](/how-arbitrum-works/04-state-transition-function/05-arbos.mdx), converting what would otherwise be a vanilla parent chain into a child chain Arbitrum rollup.

### `AllowDebugPrecompiles`

Allows access to debug precompiles. Not enabled for <a data-quicklook-from="arbitrum-one">Arbitrum One</a>. When false, calls to debut precompiles will always revert.

### `DataAvailabilityCommittee`

Currently, it does nothing besides indicate that the rollup will access a data availability service for preimage resolution in the future. On Arbitrum One, this indication isn't present, which is a strict state function of its parent chain inbox messages.

## Miscellaneous Geth changes

### ABI Gas Margin

Vanilla Geth's ABI library submits transactions with the exact estimate the node returns, employing no padding. This process means a transaction may revert should another arrive just before even slightly changing the transaction's code path. To account for this, we've added a `GasMargin` field to `bind.TransactOpts` that [pads estimates](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/accounts/abi/bind/base.go#L355) by the number of basis points set.

### Conservation of child chain `ETH`

The total amount of the child chain ether in the system should not change except in controlled cases, such as when bridging. As a safety precaution, ArbOS checks Geth's [balance delta](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/state/statedb.go#L42) each time a block gets created, [alerting or panicking](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/block_processor.go#L424) should conservation be violated.

### `MixDigest` and `ExtraData`

The root hash and leaf count of ArbOS's [send Merkle accumulator](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/merkleAccumulator/merkleAccumulator.go#L13) are stored in each child chain block's `MixDigest` and `ExtraData` fields to aid with <a data-quicklook-from="outbox">outbox</a> proof construction. The yellow paper specifies that the `ExtraData` field may be no larger than 32 bytes, so we use the first 8 bytes of the `MixDigest`, which has no meaning in a system without miners/bonders, to store the send count.

### Retryable support

ArbOS primarily implements retryables, while Geth requires some modifications to support them.

- Added `ScheduledTxes` field to `ExecutionResult`. This process lists transactions scheduled during the execution. To enable this field, we also pass the `ExecutionResult` to callers of `ApplyTransaction`.
- Added `gasEstimation` param to `DoCall`. When enabled, `DoCall` will also execute any retryable activated by the original call, which allows estimating gas to enable retryables.

### Added accessors

We added [`UnderlyingTransaction`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/state_transition.go#L69) to the Message interface, and [`GetCurrentTxLogs`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/state/statedb_arbitrum.go) to StateDB.

We created the `AdvancedPrecompile` interface, which executes and charges gas with the same function call. This interface is used by [Arbitrum precompiles](/build-decentralized-apps/precompiles/01-overview.mdx) and wraps Geth's standard precompiles.

### WASM build support

The <a data-quicklook-from="wasm">WASM</a> Arbitrum executable does not support file operations. We created [`fileutil.go`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/rawdb/fileutil.go) to wrap `fileutil` calls, stubbing them out when building WASM. [`fake_leveldb.go`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/ethdb/leveldb/fake_leveldb.go) is a similar WASM-mock for `leveldb`. The WASM block-replayer does not require these.

### Types

Arbitrum introduces a new [`signer`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/types/arbitrum_signer.go) and multiple new [`transaction types`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/types/transaction.go).

### `ReorgToOldBlock`

Geth natively only allows reorgs to a fork of the currently known network. In Nitro, sometimes reorgs can be detected before computing the forked block. We added the [`ReorgToOldBlock`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/blockchain_arbitrum.go#L38) function to support re-orging to a block that's an ancestor of the current head.

### Genesis block creation

The genesis block in Nitro is not necessarily block #0. Nitro supports importing blocks that take place before genesis. We split out [`WriteHeadBlock`](https://github.com/OffchainLabs/go-ethereum/blob/7503143fd13f73e46a966ea2c42a058af96f7fcf/core/genesis.go#L415) from genesis. Commit and use it to commit non-zero genesis blocks.

---

## .mdx (04-state-transition-function/05-arbos.mdx)
---
title: 'A gentle introduction'
description: 'Learn the fundamentals of Nitro, Arbitrum stack.'
author: petevielhaber
sme: Mehdi
user_story: As a current or prospective Arbitrum user, I need learn more about Nitros design.
content_type: get-started
---

<a data-quicklook-from="arbos">ArbOS</a> is the <a data-quicklook-from="child-chain">child chain</a> hypervisor
that facilitates the execution environment of the child chain Arbitrum. ArbOS is a trusted "system glue"
component that runs at the child chain as part of the <a data-quicklook-from="state-transition-function">
  State Transition Function
</a> (STF). It accounts for and manages network resources, produces blocks from incoming messages and
cross-chain messaging, and operates its instrumented instance of <a data-quicklook-from="geth">
  Geth
</a> for <a data-quicklook-from="smart-contract">smart contract</a> execution.

In Arbitrum, much of the work that would otherwise have to be done expensively on the <a data-quicklook-from="parent-chain">parent chain</a> gets completed by ArbOS, who will trustlessly perform these functions at the speed and low cost of the child chain.

Supporting these functions in the child chain trusted software, rather than building them into the parent chain-enforced rules of the architecture as Ethereum does, offers significant advantages in cost because these operations can benefit from the lower cost of computation and storage at the child chain, instead of having to manage those resources as part of a parent chain contract. Having a trusted operating system at the child chain also has significant advantages in flexibility because the child chain code is easier to evolve or to customize for a particular chain than a parent chain enforced architecture would be.

## Precompiles

ArbOS provides child chain-specific precompiles with methods smart contracts can call the same way they can Solidity functions. Visit the [precompiles conceptual page](/build-decentralized-apps/precompiles/01-overview.mdx) for more information about how these work and the [precompiles reference page](/build-decentralized-apps/precompiles/02-reference.mdx) for a full reference of the precompiles available in Arbitrum chains.

A precompile consists of a Solidity interface in `contracts/src/precompiles/` and a corresponding Golang implementation in `precompiles/`. Using Geth's ABI generator, `solgen/gen.go` generates `solgen/go/precompilesgen/precompilesgen.go`, which collects the ABI data of the precompiles. The [runtime installer](https://github.com/OffchainLabs/nitro/blob/bc6b52daf7232af2ca2fec3f54a5b546f1196c45/precompiles/precompile.go#L379) uses this generated file to check the type safety of each precompile's implementer.

[The installer](https://github.com/OffchainLabs/nitro/blob/bc6b52daf7232af2ca2fec3f54a5b546f1196c45/precompiles/precompile.go#L379) uses runtime reflection to ensure each implementer has all the right methods and signatures. This reflection includes restricting access to stateful objects like the EVM and `statedb` based on the declared purity. Additionally, the installer verifies and populates event function pointers to provide each precompile the ability to emit logs and know their gas costs. Additional configuration, like restricting a precompile's methods to only be callable by chain owners is possible by adding precompile wrappers like `ownerOnly` and `debugOnly` to their [installation entry](https://github.com/OffchainLabs/nitro/blob/bc6b52daf7232af2ca2fec3f54a5b546f1196c45/precompiles/precompile.go#L403).

Completion of calling, dispatching, and recording of precompile methods occurs via runtime reflection, which avoids any human error manually parsing and writing bytes could introduce, and uses Geth's stable APIs for [packing and unpacking](https://github.com/OffchainLabs/nitro/blob/bc6b52daf7232af2ca2fec3f54a5b546f1196c45/precompiles/precompile.go#L438) values.

Each time a <a data-quicklook-from="transaction">transaction</a> calls a method of a child chain-specific precompile, a [`call context`](https://github.com/OffchainLabs/nitro/blob/f11ba39cf91ee1fe1b5f6b67e8386e5efd147667/precompiles/context.go#L26) gets created to track and record the gas burnt. For convenience, it also provides access to the public fields of the underlying [`TxProcessor`](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/tx_processor.go#L38). Because sub-transactions could revert without updates to this struct, the `TxProcessor` only makes public what is safe, such as the amount of parent chain calldata paid by the top-level transaction.

## Precompile descriptions

- `ArbAggregator`: Provides methods for configuring transaction aggregation. The <a data-quicklook-from="arbitrum-one">Arbitrum One</a> <a data-quicklook-from="sequencer">Sequencer</a> is the default aggregator unless changed by users. The precompile manages fee collection and <a data-quicklook-from="batch">batch</a> posters for chain aggregation.

- `ArbGasInfo`: Retrieves various gas-related statistics, including gas prices in both `wei` and `Ink`, parent chain gas price estimates, transaction fees, and congestion metrics. It is crucial in estimating transaction costs and adjusting gas pricing dynamically.

- `ArbRetryableTx`: Handles retryable transactions, enabling parent-to-child messaging with delayed execution. The precompile manages retryables, checks expiration times, extends their lifetime, and cancels pending retryables.

- `ArbSys`: Provides system-level utilities for interacting with the <a data-quicklook-from="arbitrum-chain">Arbitrum chain</a>. It exposes methods to retrieve chain-specific information such as block numbers, chain IDs, and gas availability. It also supports child-to-parent messaging and contract aliasing.

- `ArbWasm`: Facilitates the deployment and management of <a data-quicklook-from="stylus">Stylus</a> contracts on <a data-quicklook-from="arbitrum-nitro">Arbitrum Nitro</a>. It provides lifecycle management for <a data-quicklook-from="wasm">WASM</a>-based programs, including activation, expiration handling, memory footprint calculation, and version tracking.

- `ArbWasmCache`: Manages Stylus caching mechanisms. It allows chain owners or designated managers to cache WASM programs for efficient execution, reduce computation costs, and improve call performance.

- `ArbAddressTable`: Implements a compression scheme for frequently used addresses, allowing smart contracts to reference addresses using shorter representations, thus reducing calldata costs.

- `ArbBLS` (disabled): Previously used for registering BLS public keys but is deprecated in Nitro.

- `ArbDebug`: A debugging tool enabled by the `AllowDebugPrecompiles` flag. It provides mechanisms to emit test events, trigger reverts, and panic the chain for testing purposes.

- `ArbFunctionTable` (no longer used): Originally designed to assist with function table compression, but is now deprecated in Nitro.

- `ArbInfo`: Retrieves account information, including balances and deployed contract code, providing a lightweight alternative to calling `eth_getCode` or `eth_getBalance`.

- `ArbOwner`: Allows chain owners to manage key network parameters, including gas pricing, scheduling upgrades, and setting various chain limits. The precompile is only accessible to chain owners.

- `ArbOwnerPublic`: A read-only version of `ArbOwner` that allows users to query information about current chain owners and governance settings.

- `ArbosTest` (legacy): A historical precompile used for burning arbitrary amounts of gas. It has limited utility in Nitro.

- `ArbStatistics`: Provides historical data about the chain's state before the Nitro upgrade. Most functions that required this precompile in <a data-quicklook-from="arbitrum-classic">Arbitrum Classic</a> now have better alternatives.

## Messages

An [`L1IncomingMessage`](https://github.com/OffchainLabs/nitro/blob/4ac7e9268e9885a025e0060c9ec30f9612f9e651/arbos/incomingmessage.go#L54) represents an incoming sequencer message. A message includes one or more user transactions depending on load and is made into a [unique child chain block](https://github.com/OffchainLabs/nitro/blob/4ac7e9268e9885a025e0060c9ec30f9612f9e651/arbos/block_processor.go#L118). The child chain block may include additional system transactions while processing the message's user transactions. However, ultimately, the relationship is still bijective: for every `L1IncomingMessage`, there is a child chain block with a unique child chain block hash, and for every child chain block after chain initialization, there was an `L1IncomingMessage` that made it. A sequencer batch may contain more than one `L1IncomingMessage`.

## Retryables

A retryable is a special message type for creating atomic parent chain to child chain messages; for details, see [parent-to-child chain messaging](/how-arbitrum-works/10-l1-to-l2-messaging.mdx).

## ArbOS state

ArbOS's state is viewed and modified via [`ArbosState`](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/arbosState/arbosstate.go#L36) objects, which provide convenient abstractions for working with the underlying data of its [`backingStorage`](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/storage/storage.go#L51). The backing storage's [keyed subspace strategy](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/storage/storage.go#L21) makes [`ArbosState`](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/arbosState/arbosstate.go#L36)'s convenient getters and setters possible, minimizing the need to work directly with the specific keys and values of the underlying storage's [`stateDB`](https://github.com/OffchainLabs/go-ethereum/blob/0ba62aab54fd7d6f1570a235f4e3a877db9b2bd0/core/state/statedb.go#L66).

Because two [`ArbosState`](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/arbosState/arbosstate.go#L36) objects with the same [`backingStorage`](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/storage/storage.go#L51) contain and mutate the same underlying state, different `ArbosState` objects can provide different views of ArbOS's contents. [`Burner`](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/burn/burn.go#L11) objects, which track gas usage while working with the `ArbosState`, provide the internal mechanism. Some are read-only, causing transactions to revert with `vm.ErrWriteProtection` upon a mutating request. Others demand the caller have elevated privileges. Meanwhile, others dynamically charge users when doing stateful work. This view is chosen for safety when [`OpenArbosState()`](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/arbosState/arbosstate.go#L57) creates the object and may never change.

Must of ArbOS's state exists to facilitate its [precompiles](/build-decentralized-apps/precompiles/02-reference.mdx). The parts that aren't are detailed below.

[`arbosVersion`](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/arbosState/arbosstate.go#L37), [`updgradeVersion`](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/arbosState/arbosstate.go#L38) and [`upgradeTimestamp`](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/arbosState/arbosstate.go#L39)
ArbOS upgrades are scheduled to happen [when finalizing the first block](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/block_processor.go#L350) after the `upgradeTimestamp`.

### [`blockhashes`](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/blockhash/blockhash.go#L15)

This component maintains the last 256 parent chain block hashes in a circular buffer. This component allows the [`TxProcessor`](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/tx_processor.go#L38) to implement the `BLOCKHASH` and `NUMBER` opcodes and supports the precompile methods that involve the <a data-quicklook-from="outbox">Outbox</a>. To avoid changing the ArbOS state outside of a transaction, blocks made from messages with a new parent chain–block number update this info during an [`InternalTxUpdateL1BlockNumber`](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/internal_tx.go#L24) [`ArbitrumInternalTx`](https://github.com/OffchainLabs/nitro/blob/8e786ec6d1ac3862be85e0c9b5ac79cbd883791c/arbos/internal_tx.go) included as the first transaction of the block.

### [`l1PricingState`](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/l1pricing/l1pricing.go#L16)

In addition to supporting the [`ArbAggregator precompile`](/build-decentralized-apps/precompiles/02-reference.mdx#arbaggregator), the parent chain pricing state provides tools for determining the parent chain component of a transaction's gas costs. This part of the state tracks the total amount of funds collected from transactions in parent chain gas fees and the funds spent by batch posters to post data breaches on the parent chain.

Based on this information, ArbOS maintains a parent chain data fee, which is also tracked as part of this state, determining how much transaction fees will cost for parent chain fees. ArbOS dynamically adjusts this value so that fees collected are approximately equal to batch posting costs.

## Parent chain pricing model

Efficient handling of parent chain gas costs is crucial for the Arbitrum network's scalability and economic sustainability. The Sequencer implements a dynamic parent chain pricing model to ensure that fees collected from transactions closely match the actual costs incurred when posting data to the parent chain. This section details the challenges in pricing parent chain resources, how fees are collected and allocated, and how the system adjusts to maintain equilibrium over time.

## Challenges in pricing parent chain resources

There are two main challenges in accurately pricing parent chain resources:

### 1. Apportioning batch costs among transactions:

- **Compression complexity**: The data posted to the parent chain is compressed using a general-purpose compression algorithm (Brotli). The effectiveness of compression depends on shared patterns among transactions in a batch.

- **Contribution estimation**: It's difficult to determine how much a specific transaction contributes to the overall compressibility of the batch.

- **Ideal vs. practical**: Ideally, transactions that enhance compressibility would get charged less, but there's no efficient way to calculate this precisely within the constraints of the STF.

### 2. Assessing parent chain fees at sequencing time:

- **Determinism requirement**: The parent chain fee charged to a transaction must be known when the transaction is sequenced to maintain the determinism of the STF.

- **Future uncertainty**: At sequencing time, the actual cost of the batch is unknown because it depends on:

  - The parent chain base fee at the future time of batch posting

  - The remaining contents of the batch affect its size and compressability

- **Impossibility of exact charges**: Charging based on future information is not feasible, so the system must rely on estimations.

## Nitro's approach to addressing the challenges

To overcome these challenges, Arbitrum's Nitro implements a two-fold strategy:

### 1. Estimated relative footprint

- Estimated size is calculated for each transaction and measured in data units to approximate its impact on batch size.

### 2. Adaptive fee per data unit

- A dynamic fee per data unit is determined at any given time, adjusting to align collected fees with actual costs.

## Apprortioning costs among transactions

To approximate each transaction's contribution to parent chain costs, Arbitrum employs the following method:

- **Compression estimation**:

  - Each transaction individually compresses using the Brotli compressor at its lowest compression level (fastest setting).

  - This approach reduces computational overhead within the STF.

- **Data unit calculation**:

  - The size of the compressed transaction is multiplied by 16 (since Ethereum charges 16 gas per non-zero byte).

  - This product represents the transaction's estimated footprint in data units.

- **Rationale**:

  - This method approximates the transaction's size after full batch compression.

  - While not exact, it's computationally efficient and suitable for real-time processing.

## Determining cost per data unit

Charging a transaction based on the parent chain base fee is not viable due to:

- **ArbOS limitations**:

  - ArbOS cannot directly measure the parent chain base fee

  - Relying on the Sequencer to report the parent chain base fee isn't secure, as it could manipulate fees for profit

- **Approximation errors**:

  - The estimated data units per transaction don't precisely reflect parent chain costs

  - The total number of data units charged may not be directly proportional to the Sequencer's expenses

### Adaptive pricing algorithm

To align collected fees with actual costs, Arbitrum uses an adaptive algorithm with two primary goals:

1. **Cost alignment**

   - Minimize the long-term difference between collected fees and the Sequencer's parent chain costs

2. **Stability**

   - Avoid sudden fluctuations in the data price, ensuring a stable fee environment

### Pricer components

The pricer module within ArbOS tracks:

- **Amount owed to the Sequencer**:

  - The cumulative parent chain costs incurred by the Sequencer for batch posting

- **Reimbursement fund**:

  - Collects all funds charged to transactions for parent chain fees

  - Acts as a pool to reimburse the Sequencer

- **Data unit count**:

  - The total number of recent data units processed

  - Increases with each transaction's estimated data units

- **Current parent chain data unit price**:

  - The adaptive fee per data unit expressed in `wei`

### Algorithm for price adjustment

When the Sequencer posts a batch to the parent chain inbox:

1. **Batch posting report generation**:

   - The parent chain inbox inserts a "batch posting report" transaction into the chain's <a data-quicklook-from="delayed-inbox">Delayed Inbox</a>

   - After a delay, this report gets processed by ArbOS's pricer module

2. **Processing the batch posting report**:

   - **Compute batch cost**:

     - ArbOS calculates the actual cost of posting the batch by:

       - Retrieving the batch data from the inbox state

       - Counting zero and non-zero bytes to determine parent chain gas usage

     - The cost is added to the amount owed to the Sequencer

   - **Update data units**:

     - Calculate the data units assigned to this update $(U_{\text{upd}})$

       $$
       U_{\text{upd}} = U \times \frac{T_{\text{upd}} - T_{\text{prev}}}{T - T_{\text{prev}}}
       $$

     - $U$: Total recent data units

     - $T$: Current time

     - $T_{\text{upd}}$: Time when the update occurred

     - $T_{\text{prev}}$: Time of the previous update

     - Subtract $U_{\text{upd}}$ from the total $U$

   - **Reimburse the Sequencer**:

     - Pay the Sequencer from the reimbursement fund:

       - The amount paid is the lesser of the amount owed or the fund balance.

     - Deduct the paid amount from both the reimbursement fund and the amount owed

   - **Compute surplus and derivative**:

     - Surplus ($S$):

       $$
       S = \text{Reimbursement Fund Balance} - \text{Amount Owed}
       $$

     - Derivative of surplus ($D$):

       - $D = \frac{S - S_{\text{prev}}}{U_{\text{upd}}}$

       - $S_{\text{prev}}$: Surplus at the previous update

   - **Compute derivative goal($D'$)**:

     - Establish a target derivative to eliminate surplus over time:

       - $D' = -\frac{S}{E}$

       - $E$: Equilibration constant (time horizon for balancing surplus).

   - **Adjust price ($\Delta P)$)**:

     - Calculate the change in the data unit price:

       - $\Delta P = \frac{(D' - D) \times U_{\text{upd}}}{\alpha + U_{\text{upd}}}$

       - $\alpha$: Smoothing parameter to prevent abrupt changes

     - Update the price:

       - $P = \max(0, P_{\text{prev}} + \Delta P)$

   - **Outcome**:

     - The adaptive algorithm adjusts the parent chain-data unit price to align collected fees with actual costs.

     - Ensures that the Sequencer gets fairly reimbursed while avoiding surpluses or deficits.

### Additional Considerations

- **Per-unit rewards**:

  - An optional per-unit reward can be included and payable to a designated address.

  - Useful for covering additional expenses such as infrastructure or operations.

- **Recompression scenarios**:

  - Recompression of existing batch segments may occur if:

    - The batch exceeds the maximum size limits

    - The batch hasn't been properly closed

- **Compression levels**:

  - Dynamic adjustments of compression levels based on backlog size ($B$):

    - **Compression level ($CL$)**:

      - For $B \leq 20$

        - $CL = \min(6, UC)$

      - For $20 < B < 60$

        - $CL = UC$

      - For $B > 60$

        - $CL = \min(4, UC)$

    - **Recompression level ($RL$)**:

      - For $B < 40$:

        - $RL = UC$

      - For $B \geq 40$:

        - $RL = \min(6, UC)$

    - $UC$: User-configured compression level

## Retrieving parent chain fee information

Users and developers can access parent chain fee-related data through the following methods:

- Parent chain gas base fee estimate:

  - **Method**: `ArbGasInfo.getL1BaseFeeEstimate()`

  - **Purpose**: Retrieves the current estimated parent–gas base fee for calculating transaction costs.

- Estimating transaction parent chain fees:

  - **Methods**:

    - `NodeInterface.gasEstimateComponents()`

    - `NodeInterface.gasEstimateL1Component()`

  - **Purpose**: Provides an estimate of the parent chain gas a transaction will consume.

- Transaction receipts:

  - **Field**: `gasUsedForL1`

  - **Description**: Indicates the child chain gas used to cover parent chain costs.

### [`l2PricingState`](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/l2pricing/l2pricing.go#L14)

The child chain pricing state tracks child chain resource usage to determine a reasonable child chain gas price. This process considers various factors, including user demand, the state of Geth, and the computational <a data-quicklook-from="speed-limit">speed limit</a>. The primary mechanism for doing so consists of a pair of pools, one larger than the other, that drain as child chain–specific resources are consumed and filled as time passes. Parent chain-specific resources, such as parent chain `calldata`, are not accounted for in the pools since they do not directly impact the computational workload of network actors. Instead, the design of the speed limit mechanism regulates execution resources to ensure consistent system performance and synchronization.

While much of this state is accessible through the [`ArbGasInfo`](/build-decentralized-apps/precompiles/02-reference.mdx#arbgasinfo) and [`ArbOwner`](/build-decentralized-apps/precompiles/02-reference.mdx#arbowner) precompiles, most changes are automatic and happen during [block production](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/block_processor.go#L77) and the [transaction hooks](/how-arbitrum-works/04-state-transition-function/04-modified-geth-on-arbitrum.mdx#hooks). Each of an incoming message's transactions removes the parent chain component of the gas it uses from the pool. Afterward, the message's timestamp [informs the pricing mechanism](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/block_processor.go#L336) of the time passed as ArbOS [finalizes the block](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/block_processor.go#L350).

ArbOS's larger gas pool [determines](https://github.com/OffchainLabs/nitro/blob/2ba6d1aa45abcc46c28f3d4f560691ce5a396af8/arbos/l2pricing/pools.go#L98) the per-block gas limit, setting a dynamic [upper limit](https://github.com/OffchainLabs/nitro/blob/2ba6d1aa45abcc46c28f3d4f560691ce5a396af8/arbos/block_processor.go#L146) on the amount of compute gas a child chain block may have. This limit is always enforced, though it's done in the [`GasChargingHook`](/how-arbitrum-works/04-state-transition-function/04-modified-geth-on-arbitrum.mdx#gascharginghook) for the first transaction to avoid sharp decreases in the parent chain gas price from over-inflating the compute component purchased to above the gas limit. Enforicng this improves UX by allowing the first transaction to succeed rather than requiring a resubmission. Because the first transaction lowers the space left in the block, subsequent transactions do not employ this strategy and may fail from such compute-component inflation. This space is acceptable because such transactions are only present in cases where the system is under heavy load. The result is that the user's transaction is dropped without charges since the state transition fails early. Those trusting the Sequencer can rely on the transaction being automatically resubmitted in such a scenario.

We need a per-block gas limit because arbitrator WAVM execution is much slower than native transaction execution. This limit means there can only be so much gas, roughly translating to wall-block time–in a child chain block. It also allows ArbOS to limit the size of blocks should demand continue to surge even as the price rises.

ArbOS's per-block gas limit is distinct from Geth's block limit, which ArbOS [sets sufficiently high](https://github.com/OffchainLabs/nitro/blob/2ba6d1aa45abcc46c28f3d4f560691ce5a396af8/arbos/block_processor.go#L166) to never run out. This approach is safe since Geth's block limit exists to constrain the work done per block, which ArbOS already does via its own per-block gas limit. Though it'll never run out, a block's transactions use the [same Geth gas pool](https://github.com/OffchainLabs/nitro/blob/2ba6d1aa45abcc46c28f3d4f560691ce5a396af8/arbos/block_processor.go#L199) to maintain the invariant that the pool decreases monotonically after each transaction. Block headers [use the Geth block limit](https://github.com/OffchainLabs/nitro/blob/2ba6d1aa45abcc46c28f3d4f560691ce5a396af8/arbos/block_processor.go#L67) for internal consistency and to ensure gas estimation works. There are distinct from the [`gasLeft`](https://github.com/OffchainLabs/nitro/blob/2ba6d1aa45abcc46c28f3d4f560691ce5a396af8/arbos/block_processor.go#L146) variable, which ephemerally exists outside of the global state to keep child chain blocks from exceeding ArbOS's per-block gas limit and to deduct space where the state transition failed or [used negligible amounts](https://github.com/OffchainLabs/nitro/blob/faf55a1da8afcabb1f3c406b291e721bfde71a05/arbos/block_processor.go#L328) of compute gas. ArbOS does not need to persist `gasLeft` because its pool induces a revert, and transactions use the Geth block limit during EVM execution.

## Child chain gas pricing

The child chain gas price on a given Arbitrum chain has a set floor, which is queriable via [`ArbGasInfo`](/build-decentralized-apps/precompiles/02-reference.mdx#arbgasinfo)'s `getMinimumGasPrice` method (currently 0.01 `gwei` on Arbitrum One and 0.01 `gwei` on Nova).

## Estimating child chain gas

Calling an Arbitrum Node's `eth_estimateGas` RPC gives a value sufficient to cover the full transaction fee at the given child chain gas price, i.e., the value returned from `eth_estimateGas` multiplied by the child chain gas price tells you how much total Ether is required for the transaction to succeed. Note that this means that for a given operation the value returned by `eth_estimateGas` will change over time (as the parent chain calldata price fluctuates). See [2-D fees](https://medium.com/offchainlabs/understanding-arbitrum-2-dimensional-fees-fd1d582596c9) and [How to estimate gas in Arbitrum](/build-decentralized-apps/02-how-to-estimate-gas.mdx) for more information.

## Child chain gas fees

Child chain gas fees work very similarly to gas on Ethereum. The amount of gas is multiplied by the current basefee to get the child chain gas fee charged to the transaction.

The child chain basefee is set by a version of the "exponential mechanism" widely discussed in the Ethereum community and shown to be equivalent to Ethereum's EIP-1559 gas pricing mechanism.

The algorithm compares gas usage against the [speed limit](/how-arbitrum-works/09-gas-fees.mdx#the-speed-limit) parameter, the target amount of gas per second that the chain can handle sustainably over time. (The speed limit on Arbitrum One is 7,000,000 gas per second.) The algorithm tracks a gas backlog. Whenever a transaction consumes gas, it gets added to the backlog. Whenever the clock ticks one second, the speed limit subtracts from the backlog, but the backlog can never go below zero.

Intuitively, if the backlog grows, the algorithm should increase the gas price to slow gas usage because usage is above the sustainable level. If the backlog shrinks, the price should decrease again because usage has been below the sustainable limit so more gas usage can be welcomed.

More precisely, the basefee is an exponential function of the backlog, `F = exp(-a(B-b))`, where `a` and `b` are suitably chosen constants: `a` controls how rapidly the price escalates with the backlog, and `b` allows a small backlog before the basefee escalation begins.

## Child chain tips

The Sequencer prioritizes transactions on a first-come, first-served basis. Because tips do not make sense in this model, they are ignored. Arbitrum users always pay the basefee regardless of the tip they choose.

## Gas estimating retryables

When a transaction schedules another, the subsequent transaction's execution [will be included](https://github.com/OffchainLabs/go-ethereum/blob/d52739e6d54f2ea06146fdc44947af3488b89082/internal/ethapi/api.go#L999) when estimating gas via the node's RPC. Finding a gas estimate for a transaction is only possible if all the transactions succeed at a given gas limit. This estimation is especially important when working with retryables and scheduling `redeem` attempts.

Because a call to `redeem` donates all of the call's gas, doing multiple requires limiting the amount of gas provided to each sub-call. Otherwise, the first will take all of the gas and force the second to fail, irrespective of the estimation's gas limit.

Gas estimation for retryable submissions is possible via the [`NodeInterface`](/build-decentralized-apps/nodeinterface/02-reference.mdx) and similarly requires the auto-redeem attempt to succeed.

## The speed limit

The security of Nitro chains depends on the assumption that when one <a data-quicklook-from="validator">validator</a> creates an <a data-quicklook-from="assertion">assertion</a>, other validators will check it and respond with a correct assertion and a <a data-quicklook-from="challenge">challenge</a> if it is wrong. This assumption requires that the other validators have the time and resources to check each assertion quickly enough to issue a timely challenge. The Arbitrum protocol takes this into account in setting deadlines for assertions.

This approach sets an effective speed limit on execution of a Nitro chain: in the long run, the chain cannot make progress faster than a validator can emulate its execution. If assertions are published at a rate faster than the speed limit, their deadlines will get farther and farther in the future. Due to the limit enforced by the Rollup protocol contracts on how far a deadline can be in the future, this will eventually cause new assertions to be slowed down, thereby enforcing the effective speed limit.

Being able to set the speed limit accurately depends on being able to estimate the time required to validate an assertion, with some accuracy. Any uncertainty in estimating validation time will force us to lower the speed limit to be safe. We do not want to lower the speed limit, so we try to enable accurate estimation.

---

## .mdx (04-state-transition-function/06-stylus-execution-path.mdx)
---
title: 'Stylus execution path'
description: 'Learn the fundamentals of Nitro, Arbitrum stack.'
author: petevielhaber
sme: Mehdi
user_story: As a current or prospective Arbitrum user, I need learn more about Nitros design.
content_type: get-started
---

This article details how <a data-quicklook-from="stylus">Stylus</a> integrates into the <a data-quicklook-from="state-transition-function">State Transition Function</a> (STF), covering execution flow, messaging handling, caching, and interactions with <a data-quicklook-from="arbos">ArbOS</a> and <a data-quicklook-from="geth">Geth</a>.

## 1. Execution flow of a Stylus transaction

When a <a data-quicklook-from="transaction">Transaction</a> interacts with a Stylus contract, its execution follows a distinct path compared to EVM transactions:

### Transaction submission and routing

- The transaction is included in a <a data-quicklook-from="child-chain">child chain</a> block by the <a data-quicklook-from="sequencer">Sequencer</a>.

- Geth processes the transaction and determines its target contract.

- If the target is a Stylus contract, ArbOS routes execution to the <a data-quicklook-from="wasm">WASM</a> runtime instead of the EVM.

### Stylus execution within ArbOS

- ArbOS retrieves the Stylus program from its cache (`stylus/src/cache.rs`) or loads it from storage if not cached.

- The WebAssembly System Interface (Go-WASI) initializes a secure execution environment.

- The WASM module executes within ArbOS, processing instructions efficiently and calling host I/O functions.

### Host I/O operations for <a data-quicklook-from="blockchain">blockchain</a> state access

- Stylus contracts do not use EVM opcodes. Instead, they interact with the blockchain through host I/O calls handled by ArbOS.

- These include storage access (`TLOAD` `TSTORE`), arithmetic operations (`MULMOD`, `ADDMOD`), and context retrieval (`GETCALLER`, `GETCALLVALUE`).

- ArbOS ensures these operations are efficient and compatible with Ethereum's state model.

#### State commitment and finalization

- Once execution is complete, ArbOS finalizes storage changes and updates logs and receipts.

- Geth processes the final transaction result and commits it to the state tree.

This process bypasses the EVM interpreter entirely, allowing Stylus contracts to execute significantly faster than their Solidity counterparts.

## 2. Stylus caching and gas pricing

### Stylus gas pricing model

Unlike standard EVM gas pricing, Stylus pricing follows a multi-dimensional cost model, incorporating:

- **<a data-quicklook-from="ink">Ink</a> cost (memory and execution cost)**

  - Measure in `Ink` units (Stylus's equivalent of computational gas).

  - `Ink` pricing varies based on execution complexity, memory usage, and computation steps.

  - Complex WASM operations consume more `Ink`, directly impacting execution costs.

- **Opcode pricing**

  - WASM instructions are assigned individual execution costs similar to EVM opcodes.

  - Heavy computation opcodes are priced higher.

  - Cheap opcodes (e.g., simple arithmetic, bitwise operations) have minimal costs.

- **Host I/O pricing**

  - Stylus introduces fine-grained pricing for different I/O calls:

    - **Storage read/writes**: Priced based on access pattern and data size.

    - **Precompile calls**: Stylus-specific precompiles have fixed execution costs.

    - **External calls to EVM contracts**: Encapsulated within ArbOS transaction handling, with additional gas considerations.

### Stylus caching

Stylus contracts leverage an advanced caching system to minimize execution overhead within ArbOS:

- **LRU (Least Recently Used) caching**: Keeps the most recently accessed Stylus contracts in memory for fast execution.

- **Persistent long-term caching**: Caching for selected contracts may occur across blocks based on an economic auction model.

- **Init costs and execution pricing**: Instead of a flat gas cost, Stylus contracts have dynamic execution costs based on WASM complexity. ArbOS maintains pricing parameters (`initCost`, `cachedCost`) that adjust based on future optimizations in WASM execution.

## 3. Interaction with ArbOS and Geth

| Execution Stage        | Handled By                                                |
| ---------------------- | --------------------------------------------------------- |
| Transaction submission | **Geth** (identifies target contract)                     |
| Stylus execution       | **ArbOS** (switches to WASM runtime)                      |
| Host I/O calls         | **ArbOS** (handles storage, call data, context retrieval) |
| State commitment       | **Geth and ArbOS** (finalizes updates, commits to state)  |

## 4. Go-WASI and co-threads in Stylus execution

ArbOS executes Stylus contracts using Go-WASI, a WASM-compatible runtime with custom optimizations for <a data-quicklook-from="arbitrum">Arbitrum</a>. Key features include:

- **Memory management**: WASM modules execute in a sandboxed environment with strict memory allocation policies.

- **Co-threads for efficient execution**: Instead of traditional synchronous execution, Stylus employs co-threading, enabling lightweight task switching and parallelism where possible.

- **Deterministic execution**: Ensures that Stylus contracts remain fully deterministic and compatible with Ethereum's consensus model.

These optimizations make Stylus an extremely efficient execution environment, capable of outperforming the EVM while maintaining security and compatibility with Ethereum's state model.

---

## .mdx (05-validation-and-proving/01-validation-and-proving.mdx)
---
title: Validation and Proving Mechanisms
description: 'Learn the fundamentals of validation and proving mechanisms on Arbitrum.'
author: pete-vielhaber
sme: Mehdi
user_story: As a current or prospective Arbitrum user, I need to learn more about validation and proving mechanisms.
content_type: get-started
---

The validation and proving mechanism in <a data-quicklook-from="arbitrum">Arbitrum</a> ensures the integrity of offchain execution while maintaining Ethereum's security guarantees. As an Optimistic Rollup, Arbitrum assumes that transactions execute correctly unless challenged. Arbitrum employs interactive fraud proofs to resolve conflicts efficiently, if a dispute arises.

At the core of Arbitrum's validation model is the separation of execution and proving: transactions execute optimistically, while state transitions can be verified onchain if necessary. Arbitrum achieves this through WAVM (WebAssembly for Arbitrum Virtual Machine), a modified <a data-quicklook-from="wasm">WASM</a> format optimized for proving, and the <a data-quicklook-from="bold">BoLD</a> dispute protocol, which resolves disagreements using an interactive, multi-round <a data-quicklook-from="challenge">challenge</a> system.

This section will explore two primary topics:

## [Rollup protocol and validation](/how-arbitrum-works/05-validation-and-proving/02-rollup-protocol.mdx)

Reviews how Arbitrum records and validates transactions, the role of validators, and the mechanics of state confirmations.

## [Proving and challenge protocols](/how-arbitrum-works/05-validation-and-proving/03-proving-and-challenges.mdx)

Discusses how the protocol handles resolving disputes using interactive proofs and narrowing conflicts down to a single execution step.

---

## .mdx (05-validation-and-proving/02-rollup-protocol.mdx)
---
title: Rollup Protocol and Validation
description: 'Learn the fundamentals of the Arbitrum Rollup Protocol and validation.'
author: pete-vielhaber
sme: Mehdi
user_story: As a current or prospective Arbitrum user, I need to learn more about the Rollup protocol and validation.
content_type: get-started
---

import ImageZoom from '@site/src/components/ImageZoom';

<a data-quicklook-from="arbitrum">Arbitrum</a> operates as an Optimistic Rollup, which advances the chain's
state based on <a data-quicklook-from="transaction">transaction</a> assertions unless a dispute arises.
This approach enables scalability, lower costs, and high throughput while maintaining security through
Ethereum's <a data-quicklook-from="trustless">trustless</a> validation model.

Optimistic Rollups work by separating execution from proving to ensure efficient transaction processing without immediate validation. The execution process is covered in detail in the [State Transition Function](/how-arbitrum-works/04-state-transition-function/01-stf-gentle-intro.mdx). This section focuses on validation and proving mechanisms. We will cover how child chain validators confirm the chain's state and how assertions are posted to parent chain for finalization, with the potential for future challenges if an <a data-quicklook-from="assertion">assertion</a> is determined to be incorrect.

## Execution vs. proving: Separating responsibilities

A fundamental design choice in Arbitrum is the separation of execution from proving, which allows for:

#### Efficient execution

Transactions are processed optimistically without immediate verification.

#### Deterministic proving

If challenged, state transitions are replayable and verified onchain.

To achieve this, Arbitrum compiles the <a data-quicklook-from="state-transition-function">State Transition Function</a> (STF) into different formats:

#### Execution mode

Uses Go's native compiler for high-performance execution on <a data-quicklook-from="validator">validator</a> nodes.

#### Proving mode

Compiles to WebAssembly (<a data-quicklook-from="wasm">WASM</a>), which transforms into WebAssembly for Arbitrum Virtual Machine (WAVM) for fraud-proof verification.

This dual compilation approach ensures that Arbitrum nodes execute transactions efficiently while allowing deterministic trustless verification on parent chain.

### Stylus execution and proving

<a data-quicklook-from="stylus">Stylus</a> extends Arbitrum's execution model by enabling WASM-based
smart contracts. While execution occurs natively in WAVM, the proving process follows the same interactive
fraud-proof mechanism as the EVM, ensuring Stylus transactions are challengeable and deterministically
verifiable. Fraud proofs for Stylus transactions involve WAVM opcode evaluation, leveraging Arbitrum's
existing one-step proof (OSP) system.

## The role of validators and the Rollup chain

Arbitrum's Rollup Protocol relies on validators––network participants responsible for ensuring state correctness. The protocol enforces security through the following principles:

#### Permissionless validation

Anyone can become a validator by running an Arbitrum node.

#### Trustless verification

Validators confirm assertions, ensuring transactions adhere to protocol rules.

#### Fraud-proof system

If an incorrect assertion is detected, validators can <a data-quicklook-from="challenge">challenge</a> it and trigger an interactive fraud-proof.

Validators interact with the Rollup chain, a sequence of assertions representing state updates. Each assertion includes:

#### Predecessor assertion

The last confirmed valid state.

#### State transition output

The result of applying transactions.

#### Inbox message consumption

A record of processed messages from the <a data-quicklook-from="parent-chain">parent chain</a>.

#### Execution claim

A cryptographic commitment to the computed state.

### Assertions progress through different stages:

#### Proposed

A validator submits a state assertion.

#### Challenged (if necessary)

If another validator disputes the assertion, an interactive fraud-proof initiates.

#### Confirmed

It becomes final if no one challenges the assertion within the dispute window (6.4 days).

This model ensures that as long at least one honest validator participates, the correct execution will always be confirmed.

## Arbitrum Rollup protocol

Before diving into the Rollup protocol, it is important to clarify two key points:

1. End users and developers do not need to interact with the Rollup Protocol.
   - Like a train passenger relying on the engine without needing to understand its mechanics, users, and developers can interact with Arbitrum without engaging with the validation process.
2. The protocol does not determine transaction results, it only confirms them.
   - The ordered sequence of messages in the chain's inbox dictates the transaction outcomes. The protocol ensures correctness and finality but does not alter execution results.

Since transaction execution is deterministic, the Rollup Protocol exists to confirm results and prevent fraud. This mechanism serves two key purposes:

- **Detecting dishonest behavior**: If a validator submits an incorrect state, others can challenge it.
- **Anchoring Rollup state to the parent chain**: The parent chain itself does not execute every <a data-quicklook-from="child-chain">child chain</a> transaction but serves as the final arbiter of state correctness.

## Validators and assertions in the Rollup protocol

Validators play a central role in maintaining Arbitrum's integrity. Some validators act as proposers, submitting assertions to Ethereum and placing a `WETH` bond as collateral. If their assertion is determined to be incorrect, they lose their bond.

Since at least one honest validator can always confirm the correct state, Arbitrum remains as trustless as Ethereum.

### The Rollup chain assertions vs. child chain blocks

The Rollup chain consists of assertions, which serve as checkpoints summarizing multiple child chain blocks.

- Child chain blocks contain individual transaction data
- Assertions provide state summaries recorded on Ethereum
- Each assertion may represent multiple child chain blocks, optimizing gas costs and reducing Ethereum storage usage.

Validators submit assertions by calling `createNewAssertion` in the Rollup contract. Assertions contain structured data known as `AssertionInputs`, which capture the before-state and after-state of execution for future validation.

### Contents of an assertion

Each assertion consists of:

- **Assertion number**: A unique identifier
- **Predecessor assertion**: The last confirmed assertion
- **Number of child chain blocks**: The total child chain blocks included
- **Number of inbox messages**: Messages consumed during execution
- **Output hash**: A cryptographic commitment to the resulting state

Arbitrum ensures assertions are automatically confirmed or rejected based on protocol rules:

1. An assertion is confirmed if:
   - Its predecessor is the latest confirmed assertion
   - The dispute window has passed without challenges
2. An assertion is rejected if:
   - Its predecessor assertion is invalid
   - A conflicting assertion has been confirmed

For more details on how the Rollup chain works under <a data-quicklook-from="bold">BoLD</a>, the [gentle introduction](/how-arbitrum-works/bold/gentle-introduction.mdx) provides an overview that touches on the Rollup chain.

:::note

Validators and proposers serve different roles. Validators validate transactions by computing the next <a data-quicklook-from="chain-state">chain state</a> using the chain's STF, whereas proposers can also assert and challenge the chain state on the parent chain.

:::

Except for the assertion number, the contents of the assertion are all just claims by the assertion's proposer. Arbitrum doesn't know at first whether any of these fields are correct. The protocol should eventually confirm the assertion if all of these fields are correct. The protocol should eventually reject the assertion if one or more of these fields are incorrect.

An assertion implicitly claims that its predecessor assertion is correct, which means that it also claims the correctness of a complete history of the chain: a sequence of ancestor assertions that reaches back to the birth of the chain.

An assertion also implicitly claims that its older siblings (older assertions with the same predecessor), if there are any, are incorrect. If two assertions are siblings, and the older sibling is correct––then the younger sibling is considered incorrect, even if everything else in the younger sibling is true.

The assertion is assigned a deadline, which indicates how much time other validators have to respond to it. For an assertion `R` with no older siblings, this will equal the time the assertion was posted, plus an interval of time known as the <a data-quicklook-from="challenge-period">challenge Period</a>; subsequent younger siblings will have the same deadline as their oldest sibling (`R`). You don't need to do anything if you're a validator and agree that an assertion is correct. If you disagree with an assertion, you can post another assertion with a different result, and you'll probably end up in a challenge against the party who proposed the first assertion (or another party acting in support of that assertion). More on challenges below:

In the normal case, the Rollup chain will look like this:

<ImageZoom src="/img/haw-normal-rollup.svg" alt="Normal Rollup" className="img-200px" />

On the left, representing an earlier part of the chain's history, we have confirmed assertions. These have been fully accepted and recorded by the parent chain contracts that manage the chain. The newest of the confirmed assertions, assertion 94, is called the "latest confirmed assertion."

On the right, we see a set of newer proposed assertions. The protocol can't yet confirm or reject them because their deadlines haven't run out yet. The oldest assertion whose fate has yet to be determined, assertion 95, is called the "first unresolved assertion."

Notice that a proposed assertion can build on an earlier proposed assertion. This process allows validators to continue proposing assertions without waiting for the protocol to confirm the previous one. Normally, all of the proposed assertions will be valid, so they will all eventually be accepted.

Here's another example of what the chain state might look like if several validators are being malicious. It's a contrived example designed to illustrate a variety of cases that can come up in the protocol, all smashed into a single scenario.

<ImageZoom
  src="/img/haw-rollup-malicious-validator.svg"
  alt="Malicious validator Rollup"
  className="img-600px"
/>

There's a lot going on here, so let's unpack it:

- Assertion 100 was confirmed.
- Assertion 101 claimed to be a correct successor to assertion 100, but 101 was rejected.
- Assertion 102 eventually receives <a data-quicklook-from="confirmation">confirmation</a> as the correct successor to 100.
- Assertion 103 was confirmed and is now the latest confirmed assertion.
- Assertion 104 was proposed as a successor to assertion 103, and 105 was proposed as a successor to 104. 104 was rejected as incorrect, consequently, 105 was rejected because its predecessor was rejected.
- Assertion 106 is unresolved. It claims to be a correct successor to assertion 103 but the protocol hasn't yet decided whether to confirm or reject it. It is the first unresolved assertion.
- Assertions 107 and 108 claim to chain from 106. They are also unresolved. If 106 gets rejected, they will be automatically rejected too.
- Assertion 109 disagrees with assertion 106 because they both claim the same predecessor. At least one of them will eventually be rejected, but the protocol hasn't yet resolved them.
- Assertion 110 claims to follow 109. It is unresolved. If 109 gets rejected, 110 will be automatically rejected too.
- Assertion 111 claims to follow 104. 111 will inevitably get rejected because its predecessor has already been rejected. However, it remains active, because the protocol resolves assertions in assertion number order, so the protocol will have to resolve 106 through 110 in order, before it can resolve 111. After 110 resolves, the protocol will immediately reject 111.

Again this sort of thing is very unlikely in practice. In this diagram, at least four different bonds are on incorrect assertions, and when the dust settles, at least four bonds will be forfeited. The protocol handles these cases correctly, but they're rare corner cases. This diagram illustrates the possible situations that are possible in principle and how the protocol would deal with them.

## Delays

Even if the Assertion Tree has multiple conflicting assertions and multiple disputes are in progress, validators can continue making new assertions. Honest validators will build on one valid assertion (intuitively, an assertion is also an implicit claim of the validity of all of its parent assertions). Likewise, users can continue transacting on the child chain since transactions will continue to post in the chain's inbox.

The only delay users experience during a dispute is their [Child to parent chain messages](/how-arbitrum-works/11-l2-to-l1-messaging.mdx) (i.e., withdrawals). A key property of BoLD is that we can guarantee that, in the common case, their withdrawals/messages will only get delayed for one challenge period. In the case of an actual dispute, the withdrawals/messages will be delayed by no more than two challenge periods, regardless of the adversaries' behavior during the challenge.

## Who will be validators?

Anyone can do it, but most people will choose not to. In practice, we expect people to validate a chain for several reasons.

- It is possible to pay validators for their work by the party that created the chain or someone else. A chain is configurable, so some of the funds from user transaction fees are paid directly to validators.
- Parties with significant assets at bond on a chain, such as <a data-quicklook-from="dapp">dApp</a> developers, exchanges, power users, and liquidity providers, may choose to validate in order to protect their investment.
- Anyone who wants to validate can do so. Some users may choose to validate to protect their interests or be good citizens. But ordinary users don't need to validate, and we expect most users won't.

### Staking and validator incentives

Arbitrum requires validators to bond `ETH` as a security deposit to ensure honest participation and prevent malicious behavior. This mechanism enforces economic accountability:

- **Proposers** (validators submitting assertions) must bond `ETH` to support their claims.
- **Challenges** against incorrect assertions result in bond forfeiture for dishonest validators.
- **Successful challengers** receive a portion of the dishonest validator's bond as a reward.

Validators can adopt different roles:

1. **Active validators**: Regularly propose new assertions.
2. **Defensive validators**: Monitor the network and challenge incorrect assertions.
3. **Watchtower validators**: Passively observe and raise alarms when fraud is detected.

The protocol design requires only one honest validator to secure the system, making Arbitrum trustless and resistant to Sybil attacks.

### Staking mechanism

Some validators will act as bonders at any given time, while others remain passive. Bonders deposit `ETH` bonds into Arbitrum's smart contracts, which are forfeited if they lose a challenge.

:::note

Nitro chains exclusively accept `ETH` as collateral for staking.

:::

A single bond can secure a sequence of assertions, meaning a validator's bond applies to multiple checkpoints of the chain's history. This checkpoint allows efficient resource use while maintaining security.

A validator must be bonded to its predecessor to create a new assertion. The bond ensures that validators have economic risk in any assertion they make.

### Staking rules

1. **New validators**: If unbonded, a validator can bond on the latest confirmed assertion by depositing the required `ETH` bond.
2. **Extending an existing bond**: The validator can extend its bond to one successor assertion if already bonded. - If a validator submits a new assertion, they automatically extend their existing bond to cover it.
3. **Unbonding**: A validator can only request a refund if they have a bond on the latest confirmed assertion.
4. **Losing a challenge**: If a validator's assertion is challenged successfully (i.e., they are malicious), they lose **all** their bonded `ETH` across any unresolved assertions.

Since unbonding is not allowed mid-assertion validators must commit until their assertion is confirmed or disproven.

### Handling disputes and delays

Multiple disputes may be active simultaneously if conflicting assertions arise in the Assertion Tree. However, Arbitrum's protocol ensures that:

- **Honest validators can continue asserting**, building on the last correct assertion.
- **Users can keep transacting** on the child chain without disruption.
- **Child-to-parent chain withdrawals** may experience delays - Typically, withdrawals experience a single challenge period (6.4 days) delay. - A key property of BoLD is that we can guarantee that in the common case, withdrawals/messages will only experience delay of one challenge period. In the case of an actual dispute, the withdrawals/messages will be delayed by no more than two challenge periods, regardless of the adversaries' behavior during the challenge.

Despite these delays, Arbitrum guarantees that honest assertions always succeed, maintaining Ethereum-level security.

### Who becomes a validator?

Arbitrum's validation process is permissionless, allowing anyone to participate. However, in practice, most users will not act as validators.

Common validator motivations include:

- **Financial incentives**: Some validators receive payment from network fees or the chain's owner.
- **Asset protection**: dApp developers, exchanges, and liquidity providers may validate the chain to protect their holdings.
- **Public interest**: Some participants validate purely for network integrity, ensuring fair execution.

For most users, validation is unnecessary, as the network relies on economic incentives and fraud-proof mechanisms to maintain security.

---

## .mdx (05-validation-and-proving/03-proving-and-challenges.mdx)
---
title: Proving and Challenge Protocol
description: 'Learn the fundamentals of the Arbitrum proving and challenge protocols'
author: pete-vielhaber
sme: Mehdi
user_story: As a current or prospective Arbitrum user, I need to learn more about the Arbitrum proving and challenge protocols.
content_type: get-started
---

import ImageZoom from '@site/src/components/ImageZoom';

This document details the mechanisms for proving correct execution and resolving disputes in the Rollup Protocol. It explains how the system separates ordinary execution from proving, transforms <a data-quicklook-from="wasm">WASM</a> into a specialized proving format (WAVM), and employs interactive fraud proofs to resolve disputes efficiently.

## 1. Separating execution from proving

<ImageZoom src="/img/haw-geth-sandwich.svg" alt="" className="img-600px" />

One of the key challenges in designing a practical Rollup system is balancing fast, ordinary execution with the need for reliable proofs of that execution. Nitro addresses this <a data-quicklook-from="challenge">challenge</a> by using the same source code for both purposes but compiling it to different targets:

- **Execution compilation**:
  - The [Nitro node software](https://github.com/OffchainLabs/nitro) is compiled with the ordinary Go compiler into native code for the target architecture.
  - This native binary is distributed as source code and as a Docker image, making it deployable on various nodes.
- **Proving compilation**:
  - For proving, only the portion of the code that is the <a data-quicklook-from="state-transition-function">State Transition Function</a> (STF) gets compiled by the Go compiler into WebAssembly (WASM).
  - The generated WASM code transforms into a specialized format called WAVM.
  - In any dispute regarding the STF, the WAVM code is referenced to resolve the correct result.

## 2. WAVM: A specialized proving format

WAVM is a modified version of WASM designed to meet the needs of fraud proofs. The following features describe it:

- **Portability and Structure**:
  - WASM was selected because it is portable, well-specified, and supported by robust tools.
- **Modifications made in WAVM**:
  - **Removal of unused features**:
    - WAVM removes some WASM features that the Go compiler never generates. The transformation phase ensures these features are absent.
  - **Restrictions on certain features**:
    - Floating-point instructions: WAVM does not contain floating-point instructions. Instead, any floating-point operations get replaced by calls to the Berkeley SoftFloat library.
      - Rationale: Using software floating-point libraries reduces the risk of incompatibilities between different architecture.
  - **Nested control flow**:
    - WAVM flattens control flow constructs by converting them into jump instructions.
  - **Variable-time instructions**:
    -Some instructions in WASM may take a variable amount of time to execute. WAVM transforms these into constructs with fixed-cost instructions to simplify proving.
  - **Additional opcodes for <a data-quicklook-from="blockchain">blockchain</a> interaction**:
    - WAVM adds new opcodes to enable the code to interact with the blockchain environment.
    - For example, new instructions allow the WAVM code to read and write the chain's global state, retrieve the next message from the chain's inbox, or signal a successful end to the STF execution.

## 3. `ReadPreImage` and the hash oracle trick

An interesting instruction in WAVM is `ReadPreImage`, which the "hash oracle trick" uses. This mechanism works as follows:

- **Functionality of `ReadPreImage`**:
  - **Input**: A hash `H` and an offset `I`.
  - **Output**: The word of data at offset `I` in the preimage of `H` along with the number of bytes written (zero if `I` is at or beyond the end).
- **Constraints for safety**:
  - Generating a preimage for an arbitrary hash is infeasible. Therefore, the instruction is allowed only when:
    - The preimage is publicly known
    - The preimage's size is known to be less than a fixed upper bound (approximately 110 kbytes).
- **Usage examples in Nitro**:
  - **State tree access**:
    - Nitro's state tree is stored offchain as a Merkle tree, with nodes indexed by their Merkle hashes.
    - The STF only stores the root hash but can retrieve any node's contents using `ReadPreImage`.
  - **Block headers**:
    - The instruction fetches recent publicly known <a data-quicklook-from="child-chain">child chain</a> block headers of bounded size.

:::info Historical Note

This "hash oracle trick" originates from the original <a data-quicklook-from="arbitrum">Arbitrum</a> design, where the Merkle hash of a data structure is stored and full contents are retrievable on demand.

:::

## 4. Resolving disputes using interactive fraud proofs

Disputes resolution in Optimistic Rollups is a critical design decision. The protocol must decide which execution version is correct when two parties disagree.

### 4.1 Overview of dispute resolution

- **Optimistic Rollups**:
  - Assume that assertions about execution are correct unless challenged.
- **Dispute Resolution Options**:
  - **Interactive proving**:
    - Parties engage in a challenge game to narrow down the dispute executed steps.
  - **Re-executing transactions**:
    - The <a data-quicklook-from="parent-chain">parent chain</a> re-executes transactions, checking each state transition.

:::note

Zero-knowledge Rollups avoid the dispute situation by directly proving correctness via ZK proofs.

:::

### 4.2 Interactive proving

Arbitrum favors interactive proving due to its efficiency and flexibility.

<ImageZoom
  src="/img/haw-interactive-fraud-proof.svg"
  alt="Interactive fraud proof"
  className="img-600px"
/>

#### The interactive proving process

1. **Initial claim and dispute start**:
   - Suppose Alice claims that the chain will produce a specific result. Bob disagrees.
2. **Bisection of the dispute**:
   - **First round**:
     - Alice posts an <a data-quicklook-from="assertion">assertion</a> covering `N` steps.
     - Bob challenges the assertion.
     - Alice then provides two sub-assertions, each covering `N/2` steps.
   - **Subsequent rounds**:
     - Bob chooses one half to challenge, and the process repeats––halving the disputed steps until only a single instruction remains.
3. **Final one-step proof**:
   - Once the dispute narrows to a single execution step, the parent chain referee verifies that one-step claim.

#### Simplified bisection protocol

- **Step-by-step breakdown**:
  - Alice provides a commitment to the entire computation history.
  - She bisects her claim into two halves.
  - Bob selects the half he disputes.
  - This iteration of the bisection continues until the dispute reaches a single instruction.

#### Why bisection correctly identifies a cheater

1. **If Alice's claim is correct**:
   - Every bisection provides truthful intermediate states.
   - Bob's challenge eventually leads to a correct one-step proof, allowing Alice to win.
2. **If Alice's claim is incorrect**:
   - One of Alice's halves will be false at some bisection.
   - Bob can always choose the incorrect half, ultimately forcing an incorrect one-step proof that is not verifiable.
   - Thus, an honest party will always be able to challenge a dishonest claim.

### 4.3 Re-executing transactions (alternative approach)

- **Mechanism**:
  - Each transaction would include a state hash after its execution.
  - In case of a dispute, the parent chain would re-execute every transaction to verify the correctness.
- **Drawbacks**:
  - Requires a state claim for every transaction, increasing computational load.
  - In a dispute, the entire transaction must be re-executed onchain, leading to high gas costs.

:::info

Interactive proving is more efficient in optimistic and pessimistic cases, as it minimizes onchain computation and adapts to Ethereum's gas limits.

:::

## 5. Interactive fraud proofs and the challenge protocol

Interactive fraud proofs form the basis for resolving proposer disputes when conflicting assertions are made.

### 5.1 Dispute scenario

- **Example setup**:
  - Suppose the Rollup chain includes assertions 93 and 95, which are siblings (both following assertion 92).
  - Alice is bonded on assertion 93, while Bob is bonded on assertion 95.
  - Bob's bonding on 95 implies that assertion 93 must be incorrect.
- **Initiation of the challenge**:
  - An interactive challenge is initiated automatically when two proposers place bonds on sibling assertions.
  - Multiple assertions may participate in a single challenge.
  - The protocol records and referees the challenge and eventually declares a winner, confiscating the bonds of the losing parties and removing them as proposers.

### 5.2 The two-phase challenge game

1. **Bisection phase**:
   - The goal is to narrow the dispute to a single execution step.
   - At each stage, claims are bisected into smaller segments (first over child chain blocks, then over "big steps" of 2²⁶ instructions, and finally to a single instruction).
   - The parent chain referee only checks that the moves have "the right shape" (e.g., that the bisection was performed correctly) without evaluating the correctness of execution.
2. **One-step proof phase**:
   - Once the dispute reduces to one instruction, any player can initiate the one-step proof.
   - The parent chain executes the disputed instruction using the proof data provided.
   - If the one-step proof is correct, the disputed edge is confirmed, and the challenges resolves.

### 5.3 Efficiency considerations

- **Optimistic case**:
  - Interactive proving allows a single assertion to cover all transactions in a block.
- **Pessimistic case**:
  - Only one instruction is re-executed onchain during a dispute.
- **Gas limits**:
  - The interactive proving model can work with higher per-transaction gas limits since most work is done offchain.
- **Implementation flexibility**:
  - The system only requires verification a one-step proof onchain, rather than full transaction re-execution.

### 5.4 `ChallengeManager`: The arbiter

<ImageZoom
  src="/img/haw-challenge-manager.svg"
  alt="Transaction lifecycle diagram showing various pathways for submitting transactions"
  className="img-600px"
/>

- **Role**:
  - The `ChallengeManager` contract arbitrates the challenge game.
  - It tracks assertion histories, timers, and state commitments.
- **State machine**:
  - A state machine represents the challenge with several distinct phases:
    - **Block challenge**:
      - The challenge begins by bisecting over global states (including block hashes) to narrow down the dispute to a single block.
    - **Big-step execution challenge**:
      - Upon identifying the block, the execution challenge bisects a "chunk" of 2²⁶ instructions.
    - **Small-step execution challenge**:
      - This phase bisects the chunk until the dispute narrows to a single instruction.
    - **One-step proof**:
      - A final onchain one-step proof confirms the disputed instruction.
- **Winning the challenge**:
  - Winning is not instantaneous; a built-in time delay safeguards against erroneous challenge resolutions.
  - This delay allows time for diagnosing and correcting errors via contract upgrades if necessary.

## 6. One-step proof assumptions

The one-step proof (OSP) is the final arbiter of a single disputed instruction. Its design accounts for several key assumptions:

### 6.1 Assumptions for correct execution

- **Correct cases**:
  - In a challenge involving at least one honest party, the honest side will always prove a reachable case.
  - An arbitrator generates the WAVM code (from a valid WASM compile) and should not encounter unreachable cases in correct execution.

### 6.2 Handling unreachable cases

- **Definition**:
  - An unreachable case is a situation that is assumed never to arise during correct execution.
- **Behavior in malicious challenges**:
  - In challenges between malicious assertions, unreachable cases may occur and can be handled arbitrarily by the one-step proof.
- **Safety in honest challenges**:
  - An honest party never needs to prove an unreachable case.
  - If a dishonest party attempts to force an unreachable case, it will be preceded by an invalid reachable case, ensuring that the dishonest party eventually loses the challenge.

### 6.3 Additional safety assumptions

- **WAVM code validity**:
  - The WAVM code produced by the arbitrator comes from valid WASM compile. It is checked using `wasm-validate` from the WebAssembly Binary Toolkit (WABT).
- **Inbox message size**:
  - Inbox messages must be small enough (no larger than 117,964 bytes) to be fully available for proving.
- **Preimage requests**:
  - Preimages requested via `ReadPreImage` must be known and within the size limit (117,964 bytes).
  - Examples include block headers, state trie nodes, and recent child chain block headers.

## 7. WASM to WAVM transformation

Not all WASM instructions are mapped directly 1:1 to WAVM opcodes. The transformation design simplifies execution for proving and enables additional blockchain interactions.

### 7.1 Transformation process overview

- **Source**:
  - The Go compiler produces WASM code.
- **Transformation**:
  - A simple transformation stage converts the WASM code into WAVM code.
- **Purpose**:
  - Ensure that the WAVM code is free from unused features, restricted in its functionality where necessary, and augmented with additional opcodes for blockchain interaction.

### 7.2 Key differences between WASM and WAVM

- **Removed features**:
  - Any WASM features not generated by the Go compiler are removed.
- **Restricted features**:
  - Berkeley SoftFloat library calls replace floating-point instructions.
  - The nested control flow flattens into jumps.
  - Variable-time instructions transform into fixed-cost instructions.
- **Added opcodes**:
  - New opcodes additions for interacting with the blockchain (e.g., reading global state, fetching inbox messages, signaling completion).

### 7.3 Translation of specific WASM constructs

- **Block and loop**:
  - In WASM, blocks contain instructions and branch instructions for exiting blocks.
  - In WAVM, since instructions are flat, branch instructions are replaced with jumps to predetermined destinations.
- **If and Else**:
  - Translated into a block with an `ArbitraryJumpIf` instructions.
    - **Structure example**:
      - Begin block with endpoint `end`
      - Conditional jump to `else` block
      - [if-statement instructions]
      - Unconditional branch (to skip else)
      - `Else` block: [else-statement instructions]
      - `End` block
- **Branch instructions (`br` and `br_if`)**:
  - Translated into `ArbitraryJump` and `ArbitraryJumpIf` with jump destinations known at compile time.
- **Branch table (`br_table`)**:
  - Translated to a series of conditional checks for each possible branch, with a default branch if no conditions are met.
- **`Local.tee`**:
  - Implemented by duplicating the local value (using a `Dup` opcode) followed by a `LocalSet`.
- **Return**:
  - The function signature knows the number of return values:
    - A `MoveFromStackToInternal` opcode is added for each return value.
    - A loop uses `IsStackBoundary` to clean up the stack.
    - Finally, `MoveFromInternalToStack` opcodes are added for each return value, followed by a `Return` opcode.
- **Floating-point instructions**:
  - `f32` and `f64` values are bitcast to `i32` and `i64`.
  - Cross-module calls go to the floating-point library.
  - Return values are bitcast back to floating-point types.

### 7.4 WAVM custom opcodes not in WASM

WAVM includes several unique opcodes that simplifying proving. They breakdown into several categories:

#### Invariants and Codegen internal opcodes

- **General assumptions**:
  - Many opcodes assume specific items on the stack (e.g., "pops an `i32" means the top stack item must be an `i32`).
  - WASM validation and arbitrator code generation maintain these invariants.
- **Examples of custom opcodes**:

| Opcode | Name                      | Description                                                                                                                                                                    |
| ------ | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 0x8000 | `EndBlock`                | Pops an item from the block stack.                                                                                                                                             |
| 0x8001 | `EndBlockIf`              | Peeks the top value (assumed `i32`) and, if non-zero, pops an item from the block stack.                                                                                       |
| 0x8002 | `InitFrame`               | Pops a caller module index (`i32`), a caller module internals offset (`i32`), and a return `InternalRef`; creates a stack frame with these details and the locals Merkle root. |
| 0x8003 | `ArbitraryJumpIf`         | Pops an `i32`; if non-zero, jumps to the program counter provided in the argument data.                                                                                        |
| 0x8004 | `PushStackBoundary`       | Pushes a stack boundary marker onto the stack.                                                                                                                                 |
| 0x8005 | `MoveFromStackToInternal` | Pops an item from the stack and pushes it to an internal stack.                                                                                                                |
| 0x8006 | `MoveFromInternalToStack` | Pops an item from the internal stack and pushes it back onto the stack.                                                                                                        |
| 0x8007 | `IsStackBoundary`         | Pops an item from the stack; if it is a stack boundary, pushes an `i32` with value 1; otherwise, 0.                                                                            |
| 0x8008 | `Dup`                     | Peeks at the top stack item and pushes a duplicate.                                                                                                                            |

#### Linking opcode

- **Purpose**:
  - Generated for linking modules together:
- **Opcode**:

| Opcode | Name              | Description                                                                                                                                                                         |
| ------ | ----------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0x8009 | `CrossModuleCall` | Pushes current program counter, module number, and module internals offset; splits the argument data into function index and module index; jumps to the beginning of that function. |

#### Host calls

- **Used in host call implementations**:
  - They enable libraries to perform host calls and access the caller's memory.
- **Examples of host call opcodes**:

| Opcode | Name                       | Description                                                                                                                                                                                   |
| ------ | -------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0x800A | `CallerModuleInternalCall` | Pushes the current program counter, module number, and module internals offset; retrieves caller module internals offset; jumps to caller module function if valid, otherwise errors.         |
| 0x8010 | `GetGlobalStateBytes32`    | Pops a pointer and index; writes the global state bytes32 value (if all alignment and bounds checks pass) to the pointer.                                                                     |
| 0x8011 | `SetGlobalStateBytes32`    | Pops a pointer and index; reads a bytes32 value from memory and writes it into the global state.                                                                                              |
| 0x8012 | `GetGlobalStateU64`        | Pops a pointer and index; writes the global state u64 value to memory (with alignment and bounds checking).                                                                                   |
| 0x8013 | SetGlobalStateU64          | Pops a pointer and index; reads a u64 from memory and writes it into the global state.                                                                                                        |
| 0x8020 | `ReadPreImage`             | Pops an offset and pointer; reads a 32-byte Keccak-256 hash from memory; writes up to 32 bytes of its preimage starting from the offset; pushes the number of bytes written.                  |
| 0x8021 | `ReadInboxMessage`         | Pops an offset, pointer, and i64 message number; attempts to read an inbox message; if out-of-bounds, enters a “too far” state; writes up to 32 bytes and pushes the number of bytes written. |
| 0x8022 | `HaltAndSetFinished`       | Sets the machine status to finished, halting execution and marking it as successful.                                                                                                          |

## 8. WAVM floating-point implementation

Floating-point operations in WAVM are handled differently to ensure determinism and consistency:

- **Implementation using Berkeley SoftFloat**:
  - Instead of implementation IEEE 754-2019 directly in WAVM, floating-point instructions are replaced with calls to the Berkeley SoftFloat-3e library.
  - The soft float library is linked, and floating-point operations are implemented via cross-module calls.
- **Known divergences**:
  - For example, floating-point to integer truncation will saturate on overflow (instead of erroring), which is safer and more consistent with x86 behavior.
  - These divergences follow proposals (e.g., saturating opcodes) that are not yet widely adopted.

## 9. WAVM modules and linking

WASM's notion of modules is extended in WAVM to support multi-module programs. This extension allows:

### 9.1 Entrypoint module

- **Definition**:
  - The entry point module is the starting point of execution.
- **Behavior**:
  - It calls any modules' start functions if specified.
  - If then calls the main module's `main` function:
    - **For Go**:
      - Sets `argv` to `["js"]` (to mimic the JS environment) and calls `run`.
    - **For Rust**:
      - Simply calls `main` with no arguments.

### 9.2 Library exports

- **Export naming convention**:
  - Libraries export function using the pattern `module__name`.
  - This convention allows future libraries or the main module to import these functions.
- **Example**:
  - The `wasi-stub` library provides functions that Rust imports, adhering to the WebAssembly System Interface (WASI).

### 9.3 WAVM guest calls

- **Mechanism**:
  - Libraries can call exports of the main module using the naming scheme `"env"` `"wavm_guest_call__*"`.
- **Usage example**:
  - `go-stub` calls Go's resume function when queueing asynchronous events via `wavm_guest_call_resume()`, and retrieves the new stack pointer with `wavm_guest_call_getsp()`.

### 9.4 Caller module internals call

- **Purpose**:
  - Each stack frame retains information about the caller module and its internals offset.
- **Implementation**:
  - WAVM appends four "internal" functions to each module that perform small memory load or store operations.
  - Libraries use opcodes such as `wavm_caller_{load,store}{8,32}` to access their caller's memory.
  - Only libraries can access their caller's memory; restrictions prevent the main module from doing so.

## 10. Stylus proving and fraud detection

<a data-quicklook-from="stylus">Stylus</a> introduces an execution path alongside the EVM, enabling <a data-quicklook-from="smart-contract">
  smart Contract
</a> execution in WebAssembly (WASM). Given that Stylus program are fraud-proven directly rather than
interpreted via Geth's EVM implementation––the proving mechanism required modifications to the standard
Arbitrum proving architecture.

### 10.1 Proving Stylus programs

Unlike EVM-based contracts, Stylus programs are dynamically linked into the replay machine when proving fraud. This is achieved through two key opcodes:

- **`LinkModule`**: Loads the Stylus contract's WASM module into the replay machine.
- **`UnlinkModule`**: Removes the module once execution completes.

These opcodes ensure that Stylus contracts are treated as part as part of the replay machine, allowing for precise fraud-proof bisection at the WASM level.

### 10.2 Execution-proving separation

Stylus leverages Arbitrum's execution-proving separation, where the replay machine runs in WAVM, and disputes over Stylus transactions follow the standard interactive fraud-proof protocol (<a data-quicklook-from="bold">BoLD</a>), which means:

1. A challenge initiates if a <a data-quicklook-from="validator">validator</a> disputes the execution of a Stylus transaction.
2. The dispute narrows to a single WAVM opcode via the bisection protocol.
3. The One-Step Proof (OSP) mechanism extends to handle Stylus-specific operations, proving the correctness of WASM execution at the opcode level.

### 10.3 Internal function calls in proofs

Since Stylus programs interact with the blockchain through host I/O's, fraud proofs must ensure correct execution of cross-module calls. The following additional proving opcodes facilitate Stylus program execution:

- **`CrossModuleInternalCall`**: Allows internal functions within a Stylus module to be proven independently.
- **`CrossModuleForward`**: Ensures host I/O calls are accurately relayed through `forward.wat`, maintaining consistency in fraud proofs.

### 10.4 Error recovery and chain safety

Stylus introduces a novel Error Guard mechanism to prevent malicious programs from halting the chain. If an error occurs within a Stylus program during proving:

1. Execution reverts to the main replay machine context.
2. The replay machine resets the program's execution state using an Error Guard stack.
3. If recovery is not possible, the transaction is flagged as invalid, ensuring Stylus fraud proofs remain deterministic.

### 10.5 One-Step Proofs for Stylus

The One-Step Prover extends to validate Stylus-specific opcodes, which include:

- **Memory operations** (e.g., `MemoryGrow`, `HeapAlloc`)
- **Host I/O's** (e.g., `storage_load_bytes32`, `return_data`)
- **Gas metering** within WASM execution

By integrating these functionalities, Stylus ensures WASM contract execution can be fraud-proven like EVM transactions, while leveraging Arbitrum's existing BoLD fraud-proof architecture.

---

## .mdx (08-anytrust-protocol.mdx)
---
title: AnyTrust Protocol
description: 'Learn the fundamentals of the Arbitrum AnyTrust protocol.'
author: pete-vielhaber
sme: TucksonDev
user_story: As a current or prospective Arbitrum user, I need to learn more about AnyTrust.
content_type: get-started
---

AnyTrust is a variant of <a data-quicklook-from="arbitrum">Arbitrum</a> Nitro technology that lowers costs by accepting a mild trust assumption.

The Arbitrum protocol requires that all Arbitrum nodes, including validators (nodes that verify correctness of the chain and are prepared to place a bond on correct results), have access to the data of every child chain <a data-quicklook-from="transaction">Transaction</a> in the <a data-quicklook-from="arbitrum-chain">Arbitrum chain</a>'s inbox. An Arbitrum rollup provides data access by posting the data (in batched, compressed form) on parent chain Ethereum as calldata. The Ethereum gas to pay for this is the largest component of cost in Arbitrum.

AnyTrust relies instead on an external Data Availability Committee (DAC) to store data and provide it on demand. The DAC has `N` members, of which AnyTrust assumes at least two are honest. This means that if `N - 1` DAC members promise to provide access to some data, at least one of the promising parties must be honest. Since there are two honest members, and only one failed to make the promise, it follows that at least one of the promisers must be honest — and that honest member will provide data when it is needed to ensure the chain can properly function.

## Keysets

A Keyset specifies the BLS public keys of DAC members and the number of signatures required for a <a data-quicklook-from="data-availability-certificate">Data Availability Certificate</a> to be valid. Keysets make DAC membership changes possible and provide DAC members the ability to change their keys.

A Keyset contains:

- the number of DAC members, and
- for each DAC member, a BLS public key, and
- the number of DAC signatures required.

Keysets are identified by their hashes.

An parent chain `KeysetManager` contract maintains a list of currently valid Keysets. The child chain chain's Owner can add or remove Keysets from this list. When a Keyset becomes valid, the `KeysetManager` contract emits an parent chain Ethereum event containing the Keyset's hash and full contents. This allows the contents to be recovered later by anyone, given only the Keyset hash.

Although the API does not limit the number of Keysets that can be valid at the same time, normally only one Keyset will be valid.

## Data Availability Certificates

A central concept in AnyTrust is the Data Availability Certificate (hereafter, a "DACert"). A DACert contains:

- the hash of a data block, and
- an expiration time, and
- proof that `N-1` DAC members have signed the (hash, expiration time) pair, consisting of
  - the hash of the Keyset used in signing, and
  - a bitmap saying which DAC members signed, and
  - a BLS aggregated signature (over the BLS12-381 curve) proving that those parties signed.

Because of the `2-of-N` trust assumption, a DACert constitutes proof that the block's data (i.e., the preimage of the hash in the DACert) will be available from at least one honest DAC member, at least until the expiration time.

In ordinary (non-AnyTrust) Nitro, the Arbitrum <a data-quicklook-from="sequencer">Sequencer</a> posts data blocks on the parent chain chain as calldata. The hashes of the data blocks are committed by the parent chain Inbox contract, allowing the data to be reliably read by the child chain code.

AnyTrust gives the sequencer two ways to post a data block on parent chain: it can post the full data as above, or it can post a DACert proving availability of the data. The parent chain inbox contract will reject any DACert that uses an invalid Keyset; the other aspects of DACert validity are checked by the child chain code.

The child chain code that reads data from the inbox reads a full-data block as in ordinary Nitro. If it sees a DACert instead, it checks the validity of the DACert, with reference to the Keyset specified by the DACert (which is known to be valid because the parent chain Inbox verified that). The child chain code verifies that

- the number of signers is at least the number required by the Keyset, and
- the aggregated signature is valid for the claimed signers, and
- the expiration time is at least two weeks after the current child chain timestamp.

If the DACert is invalid, the child chain code discards the DACert and moves on to the next data block. If the DACert is valid, the child chain code reads the data block, which is guaranteed to be available because the DACert is valid.

## Data Availability Servers

DAC members run Data Availability Server (DAS) software. The DAS exposes two APIs:

- The Sequencer API, which is meant to be called only by the Arbitrum chain's Sequencer, is a JSON-RPC interface allowing the Sequencer to submit data blocks to the DAS for storage. Deployments will typically block access to this API from callers other than the Sequencer.
- The REST API, which is meant to be available to the world, is a RESTful HTTP(S) based protocol that allows data blocks to be fetched by hash. This API is fully cacheable, and deployments may use a caching proxy or CDN to increase scale and protect against DoS attacks.

Only DAC members have reason to support the Sequencer API. We expect others to run the REST API, and that is helpful. (More on that below.)

The DAS software, based on configuration options, can store its data in local files, or in a Badger database, or on Amazon S3, or redundantly across multiple backing stores. The software also supports optional caching in memory (using Bigcache) or in a Redis instance.

## Sequencer-DAC Interaction

When the Arbitrum sequencer produces a data <a data-quicklook-from="batch">batch</a> that it wants to post using the DAC, it sends the batch's data, along with an expiration time (normally three weeks in the future) via RPC to all DAC members in parallel. Each DAC member stores the data in its backing store, indexed by the data's hash. Then the member signs the (hash, expiration time) pair using its BLS key, and returns the signature with a success indicator to the sequencer.

Once the Sequencer has collected enough signatures, it can aggregate the signatures and create a valid DACert for the (hash, expiration time) pair. The Sequencer then posts that DACert to the parent chain inbox contract, making it available to the AnyTrust chain software at the child chain.

If the Sequencer fails to collect enough signatures within a few minutes, it will abandon the attempt to use the DAC, and will "fall back to rollup" by posting the full data directly to the parent chain chain, as it would do in a non-AnyTrust chain. The child chain software can understand both data posting formats (via DACert or via full data) and will handle each one correctly.

---

## .mdx (09-gas-fees.mdx)
---
title: Gas and Fees
description: 'Learn the fundamentals of how to calculate fees on Arbitrum.'
author: pete-vielhaber
sme: TucksonDev
user_story: As a current or prospective Arbitrum user, I need to learn more about gas/fee calculations on Arbitrum.
content_type: get-started
---

Gas is used by <a data-quicklook-from="arbitrum">Arbitrum</a> to track the cost of execution on a Nitro chain. It works the same as Ethereum gas, in the sense that every EVM instruction costs the same amount of gas that it would on Ethereum.

There are two parties a user pays when submitting a transaction:

- the poster, if reimbursable, for the parent chain resources such as the parent chain calldata needed to post the transaction
- the network fee account for the child chain resources, which include the computation, storage, and other burdens child chain nodes must bear to service the transaction

The parent chain component is the product of the <a data-quicklook-from="transaction">Transaction</a>'s estimated contribution to its <a data-quicklook-from="batch">Batch</a>'s size — computed using Brotli on the transaction by itself — and the child chain's view of the parent chain data price, a value which dynamically adjusts over time to ensure the batch-poster is ultimately fairly compensated.

The child chain component consists of the traditional fees <a data-quicklook-from="geth">Geth</a> would pay to bonders in a vanilla parent chain, such as the computation and storage charges applying the <a data-quicklook-from="state-transition-function">State Transition Function</a> entails. <a data-quicklook-from="arbos">ArbOS</a> charges additional fees for executing its child chain-specific [precompiles](/build-decentralized-apps/precompiles/01-overview.mdx), whose fees are dynamically priced according to the specific resources used while executing the call.

The following sections will detail how to calculate parent and child chain fees. If you do not need precise calculations or a technical understanding, skip to the next section, [Parent chain to child chain messaging](/how-arbitrum-works/10-l1-to-l2-messaging.mdx).

## Parent chain gas pricing

ArbOS dynamically prices the parent chain gas, with the price adjusting to ensure that the amount collected in the parent chain gas fees is as close as possible to the costs that must be covered, over time.

### Parent chain costs

There are two types of parent chain costs: batch posting costs, and rewards.

Batch posting costs reflect the actual cost a batch poster pays to post batch data on the parent chain. Whenever a batch is posted, the parent chain contract that records the batch will send a special "batch posting report" message to child chain ArbOS, reporting who paid for the batch and what the parent chain basefee was at the time. This message is placed in the chain's <a data-quicklook-from="delayed-inbox">Delayed Inbox</a>, so it will be delivered to child chain ArbOS after some delay.

When a batch posting report message arrives to the child chain, ArbOS computes the cost of the referenced batch by multiplying the reported basefee by the batch's data cost. (ArbOS retrieves the batch's data from its inbox state, and computes the parent chain gas that the batch would have used by counting the number of zero bytes and non-zero bytes in the batch.) The resulting cost is recorded by the pricer as funds due to the party who is reported to have submitted the batch.

The second type of parent chain cost is an optional (per chain) per-unit reward for handling transaction calldata. In general the reward might be paid to the <a data-quicklook-from="sequencer">Sequencer</a>, or to members of the Data Availability Committee in an AnyTrust chain, or to anyone else who incurs per-calldata-byte costs on behalf of the chain. The reward is a fixed number of `wei` per data unit, and is paid to a single address.

The parent chain pricer keeps track of the funds due to the reward address, based on the number of data units handled so far. This amount is updated whenever a batch posting report arrives at the child chain.

### Parent chain calldata fees

The parent chain calldata fees exist because the Sequencer, or the batch poster which posts the Sequencer's transaction batches on Ethereum, incurs costs in the parent chain gas to post transactions on Ethereum as calldata. Funds collected in the parent chain calldata fees are credited to the batch poster to cover its costs.

Every transaction that comes in through the Sequencer will pay a parent chain calldata fee. Transactions that come in through the Delayed Inbox do not pay this fee because they don't add to batch posting costs--but these transactions pay gas fees to Ethereum when they are put into the Delayed Inbox.

The parent chain pricing algorithm assigns a parent chain calldata fee to each Sequencer transaction. First, it computes the transaction's size, which is an estimate of how many bytes the transaction will add to the compressed batch it is in; the formula for this includes an estimate of how compressible the transaction is. Second, it multiplies the computed size estimate by the current price per estimated byte, to determine the transaction's parent chain calldata `wei`, in `wei`. Finally, it divides this cost by the current child chain basefee to translate the fee into the child chain gas units. The result is reported as the "poster fee" for the transaction.

The price per estimated byte is set by a dynamic algorithm that compares the total parent chain calldata fees collected to the total fees actually paid by batch posters, and tries to bring the two as close to equality as possible. If the batch posters' costs have been less than fee receipts, the price will increase, and if batch poster costs have exceeded fee receipts, the price will decrease.

### Parent chain fee collection

A transaction is charged for the parent chain gas if and only if it arrived as part of a sequencer batch. This means that someone would have paid for the parent chain gas to post the transaction on the parent chain chain.

The estimated cost of posting a transaction on the parent chain is the product of the transaction's estimated size, and the current parent chain Gas Basefee. This estimated cost is divided by the current child chain gas basefee to obtain the amount of child chain gas that corresponds to the parent chain operation (more information about this can be found in [this article](https://medium.com/offchainlabs/understanding-arbitrum-2-dimensional-fees-fd1d582596c9).

The estimated size is measured in the parent chain gas and is calculated as follows: first, compress the transaction's data using the brotli-zero algorithm, then multiply the size of the result by 16. (16 is because the parent chain charges 16 gas per byte. The parent chain charges less for bytes that are zero, but that doesn't make sense here.) Brotli-zero is used in order to reward users for posting transactions that are compressible. Ideally we would like to reward for posting transactions that contribute to the compressibility (using the brotli compressor) of the entire batch, but that is a difficult notion to define and in any case would be too expensive to compute at the child chain. Brotli-zero is an approximation that is cheap enough to compute.

Parent chain gas fee funds that are collected from transactions are transferred to a special [`L1PricerFundsPool`](https://github.com/OffchainLabs/nitro/blob/3f4939df1990320310e7f39e8abb32d5c4d8045f/arbos/l1pricing/l1pricing.go#L46) account, so that account's balance represents the amount of funds that have been collected and are available to pay for costs.

The parent chain pricer also records the total number of "data units" (the sum of the estimated sizes, after multiplying by 16) that have been received.

## Child chain gas pricing

The child chain gas price on a given <a data-quicklook-from="arbitrum-chain">Arbitrum chain</a> has a set floor, which can be queried via [ArbGasInfo](/build-decentralized-apps/precompiles/02-reference.mdx#arbgasinfo)'s `getMinimumGasPrice` method (currently @@arbOneGasFloorGwei=0.01@@ gwei on <a data-quicklook-from="arbitrum-one">Arbitrum One</a> and @@novaGasFloorGwei=0.01@@ gwei on Nova).

### Estimating child chain gas

Calling an Arbitrum Node's `eth_estimateGas` RPC gives a value sufficient to cover the full transaction fee at the given child chain gas price; i.e., the value returned from `eth_estimateGas` multiplied by the child chain gas price tells you how much total Ether is required for the transaction to succeed. Note that this means that for a given operation, the value returned by `eth_estimateGas` will change over time (as the parent chain calldata price fluctuates.) (See [2-D fees](https://medium.com/offchainlabs/understanding-arbitrum-2-dimensional-fees-fd1d582596c9) and [How to estimate gas in Arbitrum](/build-decentralized-apps/02-how-to-estimate-gas.mdx) for more.)

### Child chain gas fees

Child chain gas fees work very similarly to gas on Ethereum. A transaction uses some amount of gas, and this is multiplied by the current basefee to get the child chain gas fee charged to the transaction.

The child chain basefee is set by a version of the "exponential mechanism" which has been widely discussed in the Ethereum community, and which has been shown equivalent to Ethereum's EIP-1559 gas pricing mechanism.

The algorithm compares gas usage against a parameter called the [speed limit](#the-speed-limit) which is the target amount of gas per second that the chain can handle sustainably over time. (Currently the <a data-quicklook-from="speed-limit">Speed Limit</a> on Arbitrum One is @@arbOneGasSpeedLimitGasPerSec=7,000,000@@ gas per second.) The algorithm tracks a gas backlog. Whenever a transaction consumes gas, that gas is added to the backlog. Whenever the clock ticks one second, the speed limit is subtracted from the backlog; but the backlog can never go below zero.

Intuitively, if the backlog grows, the algorithm should increase the gas price, to slow gas usage, because usage is above the sustainable level. If the backlog shrinks, the price should decrease again because usage has been below the below the sustainable limit so more gas usage can be welcomed.

To make this more precise, the basefee is an exponential function of the backlog, _F = exp(-a(B-b))_, where a and b are suitably chosen constants: _a_ controls how rapidly the price escalates with backlog, and _b_ allows a small backlog before the basefee escalation begins.

### Child chain Tips

The sequencer prioritizes transactions on a first-come first-served basis. Because tips do not make sense in this model, they are ignored. Arbitrum users always just pay the basefee regardless of the tip they choose.

### Gas estimating retryables

When a transaction schedules another, the subsequent transaction's execution [will be included](https://github.com/OffchainLabs/go-ethereum/blob/d52739e6d54f2ea06146fdc44947af3488b89082/internal/ethapi/api.go#L999) when estimating gas via the node's RPC. A transaction's gas estimate, then, can only be found if all the transactions succeed at a given gas limit. This is especially important when working with retryables and scheduling redeem attempts.

Because a call to [`redeem`](/build-decentralized-apps/precompiles/02-reference.mdx#arbretryabletx) donates all of the call's gas, doing multiple requires limiting the amount of gas provided to each subcall. Otherwise the first will take all of the gas and force the second to necessarily fail irrespective of the estimation's gas limit.

Gas estimation for Retryable submissions is possible via the [NodeInterface](/build-decentralized-apps/nodeinterface/02-reference.mdx) and similarly requires the auto-redeem attempt to succeed.

### The Speed Limit

The security of Nitro chains depends on the assumption that when one <a data-quicklook-from="validator">validator</a> creates an assertion, other validators will check it, and respond with a correct assertion and a <a data-quicklook-from="challenge">challenge</a> if it is wrong. This requires that the other validators have the time and resources to check each assertion quickly enough to issue a timely challenge. The Arbitrum protocol takes this into account in setting deadlines for assertions.

This sets an effective speed limit on execution of a Nitro chain: in the long run the chain cannot make progress faster than a validator can emulate its execution. If assertions are published at a rate faster than the speed limit, their deadlines will get farther and farther in the future. Due to the limit, enforced by the Rollup protocol contracts, on how far in the future a deadline can be, this will eventually cause new assertions to be slowed down, thereby enforcing the effective speed limit.

Being able to set the speed limit accurately depends on being able to estimate the time required to validate an assertion, with some accuracy. Any uncertainty in estimating validation time will force us to set the speed limit lower, to be safe. And we do not want to set the speed limit lower, so we try to enable accurate estimation.

## Total fee and gas estimation

The total fee charged to a transaction is the child chain basefee, multiplied by the sum of the child chain gas used plus the parent chain calldata charge. As on Ethereum, a transaction will fail if it fails to supply enough gas, or if it specifies a basefee limit that is below the current basefee. Ethereum also allows a "tip" but Nitro ignores this field and never collects any tips.

### Allocating funds and paying what is owed

When a batch posting report is processed at the child chain, the pricer allocates some of the collected funds to pay for costs incurred. To allocate funds, the pricer considers three timestamps:

- `currentTime` is the current time, when the batch posting report message arrives at the child chain
- `updateTime` is the time at which the reported batch was submitted (which will typically be around 20 minutes before `currentTime`)
- `lastUpdateTime` is the time at which the previous reported batch was submitted

The pricer computes an allocation fraction `F = (updateTime-lastUpdateTime) / (currentTime-lastUpdateTime)` and allocates a fraction `F` of funds in the `L1PricerFundsPool` to the current report. The intuition is that the pricer knows how many funds have been collected between `lastUpdateTime` and `currentTime`, and we want to figure out how many of those funds to allocate to the interval between `lastUpdateTime` and `updateTime`. The given formula is the correct allocation, if we assume that funds arrived at a uniform rate during the interval between `lastUpdateTime` and `currentTime`. The pricer similarly allocates a portion of the total data units to the current report.

Now the pricer pays out the allocated funds to cover the rewards due and the amounts due to batch posters, reducing the balance due to each party as a result. If the allocated funds aren't sufficient to cover everything that is due, some amount due will remain. If all of the amount due can be covered with the allocated funds, any remaining allocated funds are returned to the `L1PricerFundsPool`.

### Getting parent chain fee info

The parent chain gas basefee can be queried via [`ArbGasInfo.getL1BaseFeeEstimate`](/build-decentralized-apps/precompiles/02-reference.mdx#arbgasinfo). To estimate the parent chain fee a transaction will use, the [NodeInterface.gasEstimateComponents()](/build-decentralized-apps/nodeinterface/02-reference.mdx) or [NodeInterface.gasEstimateL1Component()](/build-decentralized-apps/nodeinterface/02-reference.mdx) method can be used.

Arbitrum transaction receipts include a `gasUsedForL1` field, showing the amount of gas used on the parent chain in units of the child chain gas.

### Adjusting the parent chain gas basefee

After allocating funds and paying what is owed, the parent chain Pricer adjusts the parent chain Gas Basefee. The goal of this process is to find a value that will cause the amount collected to equal the amount owed over time.

The algorithm first computes the surplus (funds in the `L1PricerFundsPool`, minus total funds due), which might be negative. If the surplus is positive, the parent chain Gas Basefee is reduced, so that the amount collected over a fixed future interval will be reduced by exactly the surplus. If the surplus is negative, the Basefee is increased so that the shortfall will be eliminated over the same fixed future interval.

A second term is added to the parent chain Gas Basefee, based on the derivative of the surplus (surplus at present, minus the surplus after the previous batch posting report was processed). This term, which is multiplied by a smoothing factor to reduce fluctuations, will reduce the Basefee if the surplus is increasing, and increase the Basefee if the surplus is shrinking.

---

## .mdx (10-l1-to-l2-messaging.mdx)
---
title: Bridging from a parent chain to a child chain
description: 'Learn the fundamentals of parent to child chain messaging on Arbitrum.'
author: pete-vielhaber
sme: Mehdi Salehi
user_story: As a current or prospective Arbitrum user, I need to learn more about messaging between parent to child chain messaging within Arbitrum.
content_type: get-started
---

import ImageZoom from '@site/src/components/ImageZoom';

In the [Bypassing the Sequencer](/how-arbitrum-works/02-transaction-lifecycle.mdx#bypassing-the-sequencer) section, we introduced an alternative way for users to submit transactions to a <a data-quicklook-from="child-chain">child chain</a> by going through the <a data-quicklook-from="parent-chain">parent chain</a>'s <a data-quicklook-from="delayed-inbox">Delayed Inbox</a> contract instead of sending them directly to the <a data-quicklook-from="sequencer">Sequencer</a>. This approach is one example of a parent-to-child messaging path. More broadly, parent-to-child chain messaging covers all ways to:

- Submit child chain bound <a data-quicklook-from="transaction">transaction</a> from a parent chain
- Deposit `ETH` or native tokens from a parent chain to a child chain
- Send arbitrary data or instructions from a parent chain to a child chain

We generally categorize these parent-to-child chain messaging methods as follows:

1. **Native token bridging**: Refers to depositing a child chain's native token from the parent chain to the child chain. Depending on the type of <a data-quicklook-from="arbitrum">Arbitrum</a> chain, this can include:

- **`ETH` Bridging**: For Arbitrum chains that use `ETH` as their gas token, users can deposit `ETH` onto a child chain via the Delayed Inbox.
- **Custom gas token bridging**: For Arbitrum chains that use a custom gas token, users can deposit that chain's native token to a child chain using the same mechanism.

2. **Transaction via the Delayed Inbox**: As described in the [Bypassing the Sequencer](/how-arbitrum-works/02-transaction-lifecycle.mdx#bypassing-the-sequencer) section, this method allows users to send transactions through the parent chain. It includes two sub-types of messages:

- **Unsigned messages**: General arbitrary data or function calls
- **Signed messages**: Messages that include a signature, enabling certain authenticated actions

3. **Retryable tickets** are Arbitrum's canonical mechanism for creating parent-to-child messages–transactions initiated on a parent chain that trigger execution on a child chain. This method contains the following functionality:

- **General retryable messaging**: For sending arbitrary data or calls from a parent-to-child chain.
- **Customized feature messaging** (e.g., token bridging): Leveraging retryable tickets (and other messaging constructs) for specialized actions, such as bridging tokens from a parent-to-child chain.

This section will explore these categories in detail and explain how they work. The diagram below illustrates the various paths available for parent-to-child chain communication and asset transfers.

<ImageZoom src="/img/haw-l1-to-l2.svg" alt="Parent to child messaging" className="img-600px" />

## Native token bridging

Arbitrum chains can use `ETH` or any other `ERC-20` tokens as their gas fee currency. <a data-quicklook-from="arbitrum-one">Arbitrum One</a> and Nova use `ETH` as their native token, while some Arbitrum chains opt for a custom gas token. For more details about chains that use custom gas tokens, refer to the [Custom gas token SDK](/build-decentralized-apps/custom-gas-token-sdk.mdx).

Whether a chain uses `ETH` or a custom gas token, users can deposit the token from a parent chain (for Arbitrum One, it is Ethereum) to a child chain. Below, we describe how to deposit `ETH` on chains that use `ETH` as the native gas token. The process for depositing custom gas tokens follows the same steps, except it uses the chain's Delayed Inbox contract.

### Depositing ETH

A special message type exists for simple `ETH` deposits from parent-to-child chains. You can deposit `ETH` by calling the `Inbox` contract's `depositEth` method, for example:

```javascript
function depositEth(address destAddr) external payable override returns (uint256)
```

:::warning

Depositing `ETH` directly via `depositEth` to a contract on a child chain **will not** invoke that contract's fallback function.

:::

#### Using retryable tickets instead

While `depositEth` is often the simplest path, you can also use _retryable tickets_ to deposit `ETH`. This method may be preferable if you need additional flexibility–for example, specifying an alternative destination address or triggering a fallback function on a child chain.

#### How deposits work

When you call `Inbox.depositEth`, the `ETH` is sent to the <a data-quicklook-from="bridge">bridge</a> contract on the parent chain. The bridge then "credits" the deposited amount to the designated address on the child chain. From the L1 perspective, the funds are held in Arbitrum’s bridge contract on your behalf.

A diagram illustrating this deposit process is below:

Note on caller type and aliasing:

- **If the parent chain caller is an Externally Owned Account (EOA)**:
  - The deposited `ETH` will appear in the same EOA address on the child chain.
- **If the parent chain caller is a contract**:
  - The `ETH` will be deposited to the contract's aliased address on the child chain. In the next section, we will cover [Address aliasing](#address-aliasing).
- **If the caller is a `7702-enabled account` (EOA with temporary contract code)**:
  - The `ETH` goes to the aliased address, similar to contracts. This is due to the presence of runtime code during execution, and ensures consistent aliasing behavior post-EIP-7702.

<ImageZoom src="/img/haw-aliasing.svg" alt="Address aliasing" className="img-600px" />

### Address aliasing

All unsigned messages submitted through the Delayed Inbox have their sender addresses "aliased" when executed on the child chain. Instead of returning the parent chain sender's address as `msg.sender`, the child chain sees the "child alias" of that address. Formally, the child alias calculation is:

```solidity
Child_Alias = Parent_Contract_Address + 0x1111000000000000000000000000000000001111
```

#### Why aliasing?

Address aliasing in Arbitrum is a security measure that prevents cross-chain exploits. Without it, a malicious actor could impersonate a contract on a child chain by simply sending a message from that contract's parent chain address. By introducing an offset, Arbitrum ensures that child chain contracts can distinguish between parent-chain contract calls and those from child chain native addresses.

#### Computing the original parent chain address

If you need to recover the original parent chain address from an aliased child chain address onchain, you can use Arbitrum's `AddressAliasHelper` library. This library allows you to translate between the aliased child address and the original parent address in your contract logic.

```solidity
modifier onlyFromMyL1Contract() override {
    require(AddressAliasHelper.undoL1ToL2Alias(msg.sender) == myL1ContractAddress, "ONLY_COUNTERPART_CONTRACT");
    _;
}
```

## Transacting via the Delayed Inbox

Arbitrum provides a _Delayed Inbox_ contract on the parent chain that can deliver arbitrary messages to the child chain. This functionality is important for two reasons:

1. **General cross-chain messaging**: Allows parent chain EOAs or parent chain contracts to send messages or transactions to a child chain. This functionality is critical for bridging assets (other than the chain's native token) and performing cross-chain operations.
2. **Censorship resistance**: It ensures the <a data-quicklook-from="arbitrum-chain">Arbitrum chain</a> remains censorship-resistant, even if the Sequencer misbehaves or excludes certain transactions; refer to [Bypassing the Sequencer](/how-arbitrum-works/02-transaction-lifecycle.mdx#bypassing-the-sequencer) for more details.

Users can send child chain transactions through the Delayed Inbox in two primary ways:

1. [General child chain messaging](#general-child-chain-messaging)
2. [Retryable tickets](#retryable-tickets)

### General child chain messaging

Any message sent via the Delayed Inbox can ultimately produce a transaction on the child chain. These messages may or may not include a signature.

- **Signed messages**: Signed by an EOA on the parent chain. This signature proves the sender is an EOA rather than a contract, preventing certain cross-chain exploits and bypassing the need for aliasing.
- **Unsigned messages**: These do not include a signature from an EOA. For security reasons, the sender's address on the child chain must be _aliased_ when the message gets executed; see the [Address aliasing](#address-aliasing) section for details.

Below, we describe the Delayed Inbox methods for each scenario.

### Signed messages

Signed messages let a parent chain EOA prove ownership of an address, ensuring the child chain transaction will execute with `msg.sender` equal to the _signer's_ address on the child chain (rather than an alias). This mechanism is beneficial for bypassing the Sequencer if:

- You want to force-include a transaction on a child chain in case of Sequencer downtime or censorship.
- You need an operation on a child chain that explicitly requires EOA authorization (e.g., a withdrawal).

#### How signed messages work

When submitting through the Delayed Inbox, a child chain transaction signature gets included in the message's calldata. Because it matches the EOA's signature, the child chain can safely treat the signer's address as the sender.

Example use case:
[Withdraw Ether tutorial](https://github.com/OffchainLabs/arbitrum-tutorials/blob/a1c3f64a5abdd0f0e728cb94d4ecc2700eab7579/packages/delayedInbox-l2msg/scripts/withdrawFunds.js#L61-L65)

#### Delayed Inbox methods for signed messages

There are two primary methods for sending signed messages:

1. `sendL2Message`

- It can be called by either an EOA or a contract
- The complete signed transaction data is emitted in an event log so that nodes can reconstruct the transaction without replaying it
- More flexible

```solidity
function sendL2Message(
    bytes calldata messageData
) external whenNotPaused onlyAllowed returns (uint256)
```

2. `sendL2MessageFromOrigin`

- Only an EOA with no deployed code can call this ("codeless origin")
- The signed transaction is retrieved directly from calldata, so emitting a large event log is unnecessary
- Offers lower gas costs (cheaper)

```solidity
function sendL2MessageFromOrigin(
    bytes calldata messageData
) external whenNotPaused onlyAllowed returns (uint256);
```

### Unsigned messages

Unsigned messages allow a parent chain sender to specify transaction parameters without an EOA signature. Because there is no signature, **the sender's address must be aliased on the child chain** (see the [Address aliasing](#address-aliasing) section for the rationale). The Delayed Inbox provides four main methods for unsigned messages, divided based on whether the sender is an EOA or a contract and whether it includes parent chain funds:

1. **Unsigned from EOA's**: These methods incorporate a nonce for replay protection, similar to standard EOA-based transactions on Ethereum.

- `sendL1FundedUnsignedTransaction`
  - Transfers value from a parent chain to a child chain along with the transaction
  - Parameters: gas limit, fee, nonce, destination address, and calldata

```solidity
function sendL1FundedUnsignedTransaction(
    uint256 gasLimit,
    uint256 maxFeePerGas,
    uint256 nonce,
    address to,
    bytes calldata data
) external payable returns (uint256);
```

- `sendUnsignedTransaction`
  - No value transfers from the parent chain
  - Transaction fees and value on a child chain come from the child chain balance

```solidity
function sendUnsignedTransaction(
    uint256 gasLimit,
    uint256 maxFeePerGas,
    uint256 nonce,
    address to,
    uint256 value,
    bytes calldata data
) external whenNotPaused onlyAllowed returns (uint256);
```

2. **Unsigned from contracts**: Contracts typically rely on standard Ethereum replay protection using their contract address.

- `sendContractTransaction`
  - Sends a transaction from a parent chain with no new funds; uses the contract's existing child chain balance.

```solidity
function sendContractTransaction(
    uint256 gasLimit,
    uint256 maxFeePerGas,
    address to,
    uint256 value,
    bytes calldata data
) external whenNotPaused onlyAllowed returns (uint256);
```

- `sendL1FundedContractTransaction`
  - Sends the transaction _and_ transfers additional funds from a parent to child chain

```solidity
function sendL1FundedContractTransaction(
    uint256 gasLimit,
    uint256 maxFeePerGas,
    address to,
    bytes calldata data
) external payable returns (uint256);
```

In these methods, a "delayed message" is created and passed to the parent chain bridge contract, which then arranges its inclusion on a child chain.

### Messages types

<a data-quicklook-from="arbitrum-nitro">Arbitrum Nitro</a> defines various **message types** to distinguish
between the categories described above (signed vs. unsigned, EOAs vs. contracts, etc.). These message
types help the protocol route and process each incoming message securely.

You can find additional details on message types in the next section of this documentation.

:::note

Please refer to the [Address aliasing](#address-aliasing) discussion for more background on address aliasing. This mechanism ensures that a parent chain contract can't impersonate a child chain address unless it provides a vlaid signature as an EOA.

:::

## Retryable tickets

Retryable tickets are Arbitrum's canonical method for creating parent-to-child chain messages, i.e., parent-chain transactions that initiate a message to get executed on a child chain. A retryable is submittable for a fixed-cost (dependent only on its calldata size) paid at the parent chain; its _submission_ on the parent chain is separable/asynchronous with its _execution_ on the child chain. Retryables provide atomicity between the cross-chain operations; if the parent chain transaction to request submission succeeds (i.e., does not revert), then the execution of the retryable on the child chain has a strong guarantee to succeed.

### Retryable ticket lifecycle

Here, we walk through the different stages of the lifecycle of a <a data-quicklook-from="retryable-ticket">retryable ticket</a>: (1) submission, (2) auto-redemption, and (3) manual redemption.

#### Submission

1. Creating a retryable ticket is initiated with a call (direct or internal) to the `createRetryableTicket` function of the [`inbox` contract](https://github.com/OffchainLabs/nitro-contracts/blob/67127e2c2fd0943d9d87a05915d77b1f220906aa/src/bridge/Inbox.sol). A ticket is guaranteed to get created if this call succeeds. Here, we describe parameters that need adjusting with care. Note that this function forces the sender to provide _a reasonable_ amount of funds (at least enough for submitting and _attempting_ to execute the ticket), but that doesn't guarantee a successful auto-redemption.

| Parameter                                   | Description                                                                                                                                                                                                                                                                                                                      |
| ------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `l1CallValue` (also referred to as deposit) | Not a real function parameter; it is rather the `callValue` that gets sent along with the transaction                                                                                                                                                                                                                            |
| `address to`                                | The destination child chain address                                                                                                                                                                                                                                                                                              |
| `uint256 l2CallValue`                       | The `callvalue` for the retryable child chain message that is supplied within the deposit (`l1CallValue`)                                                                                                                                                                                                                        |
| `uint256 maxSubmissionCost`                 | The maximum amount of `ETH` payable for submitting the ticket. This amount is (1) supplied within the deposit (`l1CallValue`) to be later deducted from the sender's child chain balance and is (2) directly proportional to the size of the retryable's data and parent chain basefee.                                          |
| `address excessFeeRefundAddress`            | The unused gas cost and submission cost will deposit to this address, formula is: `(gasLimit x maxFeePerGas - execution cost) + (maxSubmission - (autoredeem ? 0 : submission cost))`. (**Note**: The excess deposit will transfer to the alias address of the parent chain transaction's `msg.sender` rather than this address) |
| `address callValueRefundAddress`            | The child chain address to which the `l2CallValue` is credited if the ticket times out or gets canceled (also called the `beneficiary`, who's got a critical permission to cancel the ticket).                                                                                                                                   |
| uint256 gasLimit                            | Maximum amount of gas used to cover the child chain execution of the ticket                                                                                                                                                                                                                                                      |
| uint256 maxFeePerGas                        | The gas price bid for child chain execution of the ticket supplied within the deposit (`l1CallValue`)                                                                                                                                                                                                                            |
| bytes calldata data                         | The calldata to the destination child chain address                                                                                                                                                                                                                                                                              |

2. The sender's deposit must be enough to make the parent chain submission succeed and for child chain execution to be _attempted_. If provided correctly, a new ticket with a unique `TicketID` is created and added to the retryable buffer. Also, funds (`submissionCost` + `l2CallValue`) are deducted from the sender and placed into escrow for later use in redeeming the ticket.

3. Ticket creation causes the [`ArbRetryableTx`](/build-decentralized-apps/precompiles/02-reference.mdx#arbretryabletx) precompile to emit a `TicketCreated` event containing the `TicketID` on the child chain.

**Ticket Submission**

1. User initiates a parent-to-child message
2. Initiating a parent-child message

- A call to `inbox.createRetryableTicket` function that puts the message in the child chain inbox that can be re-executed for some fixed amount of time if it reverts.

3. Check the user's deposit

- Logic that checks if the user has enough funds to create a ticket. A process that checks if the `msg.value` provided by the user is greater than or equal to `maxSubmissionCost + l2CallValue + gasLimit * maxFeePerGas`.

4. Ticket creation fails, and no funds get deducted from the user
5. Ticket creation

- A ticket is created and added to the retryable buffer on the child chain. Funds (`l2CallValue + submissionCost`) get deducted to cover the `callValue` from the user and are placed into escrow (on the child chain) for later use in redeeming the ticket.

#### Automatic redemption

1. It is very important to note that submitting a ticket on the parent chain is separable/asynchronous from its execution on the child chain, i.e., a successful parent chain ticket creation does not guarantee successful redemption. Upon successful ticket creation, checks validate the two following conditions:

- if the user's child chain balance is greater than (or equal to) `maxFeePerGas * gasLimit` **and**
- if the `maxFeePerGas` (provided by the user in the ticket submission process) is greater than (or equal to) the `l2BaseFee`. If these conditions are both met, an attempt to execute the ticket on the child chain triggers (i.e., **auto-redeem** using the supplied gas, as if the `redeem` method of the `[ArbyRetryableTx]` precompile had been called). Depending on how much gas the sender has provided in Step 1, the ticket's redemption can either (1) immediately succeed or (2) fail. We explain both situations below:

**Immediate success**

If the ticket is successfully auto-redeemed, it will execute with the original submission's sender, destination, callvalue, and calldata. The submission fee is refunded to the user on the child chain (`excessFeeRefundAddress`). Note that to ensure successful auto-`redeem` of the ticket, one could use the Arbitrum SDK, which provides a [convenience function](https://github.com/OffchainLabs/arbitrum-sdk/blob/4cedb1fcf1c7302a4c3d0f8e75fb33d82bc8338d/src/lib/message/L1ToL2MessageGasEstimator.ts#L215) that returns the desired gas parameters when sending parent-to-child messages.

**Fail**

If a `redeem` is not done at submission or the submission's initial `redeem` fails (for example, because the child chain's gas price has increased unexpectedly), the submission fee is collected on the child chain to cover the resources required to temporarily keep the ticket in memory for a fixed period (one week), and only in this case, a manual redemption of the ticket is required (see the next section).

**Automatic redemption of the `TicketManual` redemption of the ticket**

1. Does the auto-`redeem` succeed?

- Logic that determines if the user's child chain balance is greater than (or equal to) `maxFeePerGas * gasLimit && maxFeePerGas` is greater than (or equal to) the `l2BaseFee`.

2. Ticket is executed

- The actual `submissionFee` is refunded to the `excessFeeRefundAddress` because the ticket cleared from the buffer of the child chain.

3. The ticket is deleted from the child chain retryable buffer.
4. Refund `callValueRefundAddress`

- Refunded with (`maxGas - gasUsed) * gasPrice`. Note that the cap amount is the `l2CallValue` in the auto-`redeem`.

#### Manual redemption

1. At this point, _anyone_ can attempt to manually redeem the ticket again by calling [`ArbRetryableTx`](/build-decentralized-apps/precompiles/02-reference.mdx#arbretryabletx) `redeem` precompile method, which donates the call's gas to the next attempt. Note that the amount of gas is **not** limited by the original `gasLimit` set during the ticket creation. <a data-quicklook-from="arbos">ArbOS</a> will [enqueue the `redeem`](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/block_processor.go#L245) before moving on to the next non-`redeem` transaction in the block it's forming. In this manner, `redeem`'s are scheduled to happen as soon as possible and will always be in the same block as the transaction that scheduled it. Note that the `redeem` attempt's gas comes from the call to `redeem`, so there's no chance to reach the block's gas limit before execution.

2. If the fixed period (one week) elapses without a successful `redeem`, the ticket **expires** and will be [automatically discarded](https://github.com/OffchainLabs/nitro/blob/fa36a0f138b8a7e684194f9840315d80c390f324/arbos/retryables/retryable.go#L262), unless some party has paid a fee to [keep the ticket alive](https://github.com/OffchainLabs/nitro-precompile-interfaces/blob/fe4121240ca1ee2cbf07d67d0e6c38015d94e704/ArbRetryableTx.sol#L45) for another full period. A ticket can live indefinitely as long as it continues to renew each time before it expires.

**Process flow**

1. Is the ticket manually created or not redeemed within seven days?

- Logic that determines if the ticket is manually canceled or not redeemed within seven days (i.e., it is expired).

2. Refunding `callValueRefundAddress`

- The `l2CallValue` is refunded to the `callValueRefundAddress`

3. The ticket is deleted from the child chain retryable buffer.
4. Is the ticket manually redeemed?

- Logic that determines if the ticket is manually redeemed.

#### Avoid losing funds!

Any associated messages and values, excluding the escrowed `callValue`, may be permanently lost if a ticket is not redeemed or rescheduled within seven days.

On success, the `To` address receives the escrowed `callValue`, and any unused gas returns to ArbOS's gas pools. On failure, the `callValue` is returned to the escrow for the future `redeem` attempt. In either case, the network fee gets paid during the scheduling transaction, so no fee charges or refunds get made.

During ticket redemption, attempts to cancel the same ticket or schedule another `redeem` of the same ticket will revert. In this manner, retryable tickets are not self-modifying.

If a ticket with a `callValue` is eventually discarded (canceled or expired) having never successfully run, the escrowed `callValue` will be paid out to a `callValueRefundAddress` account that was specified in the initial submission (Step 1).

:::info Important notes

If a `redeem` is not done at submission or the submission's initial `redeem` fails, anyone can attempt to redeem the retryable again by calling [`ArbRetryableTx`](/build-decentralized-apps/precompiles/02-reference.mdx#arbretryabletx) `redeem` precompile method, which donates the call's gas to the next attempt.

- One can `redeem` live tickets using the [Arbitrum Retryables Transaction Panel](https://retryable-dashboard.arbitrum.io/tx)
- The calldata of a ticket is saved on the child chain until it is redeemed or expired
- Redeeming the cost of a ticket will not increase over time; it only depends on the current gas price and gas required for execution.

:::

#### Receipts

In the lifecycle of a retryable ticket, two types of child chain transaction receipts will emit:

- **Ticket creation receipt**: This receipt indicates successful ticket creation; any successful parent chain call to the `Inbox`'s `createRetryableTicket` method is guaranteed to create a ticket. The ticket creation receipt includes a `TicketCreated` event (from `ArbRetryableTx`), which includes a `ticketId` field. This `ticketId` is computable via RLP encoding and hashing the transaction; see `[calculateSubmitRetryableId]`(https://github.com/OffchainLabs/arbitrum-sdk/blob/6cc143a3bb019dc4c39c8bcc4aeac9f1a48acb01/src/lib/message/L1ToL2Message.ts#L109).
- **`redeem` attempt**: A `redeem` attempt receipt represents the results of an attempted child chain execution of a ticket, i.e., success/failure of that specified `redeem` attempt. It includes a `RedeemScheduled` event from `ArbRetryableTx`, with a `ticketId` field. At most, one successful `redeem` attempt can ever exist for a given ticket; if, e.g., the auto-`redeem` upon initial creation succeeds, only the receipt from the auto-`redeem` will ever get emitted for that ticket. If the auto-`redeem` fails (or was never attempted–i.e., the provided `child chain gas limit * child chain gas price = 0`), each initial attempt will emit a `redeem` attempt receipt until one succeeds.

#### Alternative "unsafe" retryable ticket creation

The `Inbox.createRetryableTicket` convenience method includes sanity checks to help minimize the risk of user error: the method will ensure enough funds are provided directly from a parent chain to cover the current cost of ticket creation. It also will convert the provided `callValueRefundAddress` and `excessFeeRefundAddress` to their [address alias](#address-aliasing) if either is a contract (determined by if the address has code during the call), providing a path for the parent chain contract to recover funds. A power-user may bypass these sanity-check measures via the `Inbox`'s `unsafeCreateRetryableTicket` method; as the method's name desperately attempts to warn you, only a user who knows what they are doing should access it.

## Token bridging

We can build **customized feature messaging** (for example, token bridging) using the messaging systems described in previous sections. In particular, **retryable tickets** power Arbitrum's **canonical token bridge**, which <a data-quicklook-from="offchain-labs">Offchain Labs</a> developed. By leveraging retryable tickets under the hood, this token bridge provides a seamless user experience for transferring assets from a parent-to-child chain. An overview of how the system works is below:

### `ERC-20` token bridging

The Arbitrum protocol technically has no native notion of any token standards and gives no built-in advantage or special recognition to any particular token bridge. In this page, we describe the "canonical bridge", which was implemented by Offchain Labs and should be the primary bridge most users and applications use; it is (effectively) a decentralized app (<a data-quicklook-from="dapp">dApp</a>) with contracts on both parent and child chains that leverages Arbitrum's [cross-chain message passing system](/build-decentralized-apps/04-cross-chain-messaging.mdx) to achieve basic desired token-bridging functionality. We recommend that you use it!

### Design rationale

In our token bridge design, we use the term "gateway" as per [this proposal](https://ethereum-magicians.org/t/outlining-a-standard-interface-for-cross-domain-erc20-transfers/6151); i.e., one of a pair of contracts on two different domains (i.e., Ethereum and an Arbitrum One chain), used to facilitate cross-domain asset transfers.

We will now outline some core goals that motivated the design of our bridging system.

#### Custom gateway functionality

For many `ERC-20` tokens, "standard" bridging functionality is sufficient, which entails the following: a token contract on the parent chain (i.e., Ethereum) is associated with a "paired" token contract on the child chain (i.e., Arbitrum).

Depositing a token entails escrowing some of the tokens in a parent chain bridge contract and minting the same amount of the paired token contract on a child chain. On the child chain, the paired contract behaves much like a standard `ERC-20` token contract. Withdrawing entails burning some amount of the token in the child chain contract, which is claimable from the parent chain bridge contract at a later time.

Many tokens, however, require <a data-quicklook-from="custom-gateway">custom gateway</a> systems, the possibilities which are hard to generalize:

- Tokens that accrue interest to their holders must ensure that the interest is dispersed properly across layers, and doesn't simply accrue to the bridge contracts.
- Our cross-domain `WETH` implementations require wrapping and unwrapping tokens as they move across layers.

Thus, our bridge architecture must allow the standard deposit and withdrawal functionalities, and new, custom gateways can be added dynamically over time.

#### Canonical child chain representation per parent chain token contract

Having multiple custom gateways is well and good. Still, we also want to avoid a situation in which a single parent chain token that uses our bridging system can be represented at multiple addresses/contracts on the child chain, as this adds significant friction and confusion for users and developers. Thus, we need a way to track which parent chain token uses which gateway and, in turn, to have a canonical address oracle that maps the token addresses across the parent and child chain domains.

### Canonical token bridge implementation

With this in mind, we provide an overview of our token-bridging architecture.

Our architecture consists of three types of contracts:

1. **Asset contracts**: These are the token contracts, i.e., an `ERC-20` on a parent chain and its counterpart on the child chain.
2. **Gateways**: Pairs of contracts (one on the parent and one on the child chain) implementing a particular type of cross-chain asset bridging.
3. **Routers**: Exactly two contracts (one on parent and one on child chain) route each asset to its designated gateway.

<ImageZoom src="/img/haw-canonical-bridge.svg" alt="Canonical bridge" className="img-600px" />

All parent-to-child token transfers initiate via the router contract on the parent chain, the `L1GatewayRouter` contract. `L1GatewayRouter` forwards the token's deposit call to the appropriate gateway contract on the parent chain, the `L1ArbitrumGateway` contract. `L1GatewayRouter` is responsible for mapping the parent chain token addresses to L1Gateway contracts, thus acting as a parent/child address oracle and ensuring each token corresponds to only one gateway. The `L1ArbitrumGateway` then communicates to its counterpart gateway contract on the child chain, the `L2ArbitrumGateway` contract (typically/expectedly via [retryable tickets](#retryable-tickets)).

<ImageZoom src="/img/haw-token-gateway.svg" alt="Token gateway" className="img-600px" />

For any given gateway pairing, we require that calls initiate through the corresponding router (`L1GatewayRouter`) and that the gateways conform to the [TokenGateway](https://github.com/OffchainLabs/token-bridge-contracts/blob/main/contracts/tokenbridge/libraries/gateway/TokenGateway.sol) interfaces; the `TokenGateway` interfaces should be flexible and extensible enough to support any bridging functionality a particular token may require.

### The standard `ERC-20` gateway

Any `ERC-20` token on a parent chain not registered to a gateway can be permissionlessly bridged through the `StandardERC20Gateway` by default.

You can use the [bridge UI](https://bridge.arbitrum.io/) or follow the instructions in [How to bridge tokens via Arbitrum's standard `ERC-20` gateway](/build-decentralized-apps/token-bridging/bridge-tokens-programmatically/02-how-to-bridge-tokens-standard.mdx) to bridge a token to a child chain via this gateway.

#### Example: Standard `Arb-ERC-20` deposit and withdraw

To help illustrate what this all looks like in practice, let's go through the steps of what depositing and withdrawing `SomeERC20Token` via our standard `ERC-20` gateway looks like. We assume that `SomeERC20Token` has already been registered in the `L1GatewayRouter` to use the standard `ERC-20` gateway.

#### Deposits

1. A user calls `L1GatewayRouter.outboundTransferCustomRefund` (with `SomeERC20Token`'s parent chain address as an argument).

:::warning

Please keep in mind that some older custom gateways might not have `outboundTransferCustomRefund` implemented, and `L1GatewayRouter.outboundTransferCustomRefund` does not fall to `outboundTransfer`. In those cases, please use the function `L1GatewayRouter.outboundTransfer`.

:::

2. `L1GatewayRouter` looks up `SomeERC20Token`'s gateway and finds its standard `ERC-20` gateway (the `L1ERC20Gateway` contract).

3. `L1GatewayRouter` calls `L1ERC20Gateway.outboundTransferCustomRefund`, forwarding the appropriate parameters.

4. `L1ERC20Gateway` escrows the tokens sent and creates a retryable ticket to trigger `L2ERC20Gateway`'s `finalizeInboundTransfer` method on the child chain.

5. `L2ERC20Gateway.finalizeInboundTransfer` mints the appropriate token amount at the `arbSomeERC20Token` contract on the child chain.

Note that `arbSomeERC20Token` is an instance of [`StandardArbERC20`](https://github.com/OffchainLabs/token-bridge-contracts/blob/main/contracts/tokenbridge/arbitrum/StandardArbERC20.sol), which includes `bridgeMint` and `bridgeBurn` methods only callable by the `L2ERC20Gateway`.

### The Arbitrum generic-custom gateway

Just because a token has requirements beyond what the standard `ERC-20` gateway offers doesn't necessarily mean that a unique gateway needs to be tailor-made for the token in question. Our <a data-quicklook-from="generic-custom-gateway">generic-custom gateway</a> design is flexible enough to be suitable for most (but not necessarily all) custom fungible token needs.

:::info As a general rule

Suppose your custom token can increase its supply (i.e., mint) directly on the child chain, and you want the child-chain-minted tokens to be withdrawable back to the parent chain and recognized by the parent chain contract. In that case, a special gateway is likely required. Otherwise, the generic-custom gateway is likely the solution for you!

:::

Some examples of token features suitable for the generic-custom gateway:

- A child chain token contract upgradable via a proxy
- A child chain token contract that includes address allowlisting/denylisting
- The deployer determines the address of the child chain token contract

#### Setting up your token with the generic-custom gateway

The following steps can set up your token for the generic-custom gateway. You can also find more detailed instructions on the page [How to bridge tokens via Arbitrum's generic-custom gateway](/build-decentralized-apps/token-bridging/bridge-tokens-programmatically/03-how-to-bridge-tokens-generic-custom.mdx).

**Pre-requisites**

Your token on the parent chain should conform to the [ICustomToken](https://github.com/OffchainLabs/token-bridge-contracts/blob/main/contracts/tokenbridge/ethereum/ICustomToken.sol) interface (see [`TestCustomTokenL1`](https://github.com/OffchainLabs/token-bridge-contracts/blob/main/contracts/tokenbridge/test/TestCustomTokenL1.sol) for an example implementation). Crucially, it must have an `isArbitrumEnabled` method in its interface.

1. **Deploy your token on the child chain**

- Your token should conform to the minimum [IArbToken](https://github.com/OffchainLabs/token-bridge-contracts/blob/main/contracts/tokenbridge/arbitrum/IArbToken.sol) interface, i.e., it should have `bridgeMint` and `bridgeBurn` methods only callable by the `L2CustomGateway` contract, and the address of its corresponding Ethereum token accessible via `l1Address`. For an example implementation, see [`L2GatewayToken`](https://github.com/OffchainLabs/token-bridge-contracts/blob/main/contracts/tokenbridge/libraries/L2GatewayToken.sol).
- Token compatibility with available tooling
  - If you want your token to be compatible out of the box with all the tooling available (e.g., the [Arbitrum bridge]), we recommend that you keep the implementation of the IArbToken interface as close as possible to the [`L2GatewayToken`](https://github.com/OffchainLabs/token-bridge-contracts/blob/main/contracts/tokenbridge/libraries/L2GatewayToken.sol) implementation example.
  - For example, if an allowance check is added to the `bridgeBurn()` function, the token will not be easily withdrawable through the <a data-quicklook-from="arbitrum-bridge-ui">Arbitrum Bridge UI</a>, as the UI does not prompt an approval transaction of tokens by default (it expects the tokens to follow the recommended `L2GatewayToken` implementation).

2. **Register your token on the parent chain to your token on the child chain via the `L1CustomGateway` contract**

- Have your parent chain token's contract make an external call to `L1CustomGateway.registerTokenToL2`. Alternatively, this registration is submittable as a chain-owner registration via an [Arbitrum DAO](https://forum.arbitrum.foundation/) proposal.

3. **Register your token on the parent chain to the `L1GatewayRouter`**

- After your token's registration to the generic-custom gateway is complete, have your parent chain token's contract make an external call to `L1GatewayRouter.setGateway`; Alternatively, this registration is submittable as a chain-owner registration via an [Arbitrum DAO](https://forum.arbitrum.foundation/) proposal.

:::info We are here to help

If you have questions about your custom token needs, please reach out on our [Discord](https://discord.gg/arbitrum) server.

:::

### Other flavors of gateways

Note that in the system described above, one pair of gateway contracts handles the bridging of many `ERC-20`'s; i.e., many `ERC-20`'s on parent chain's are each paired with their own `ERC-20`'s on child chain's via a single gateway contract pairing. Other gateways may bear different relations with the contracts they bridge.

Take our wrapped Ether implementation; for example, a single `WETH` contract on a parent chain connects to a single `WETH` contract on a child chain. When transferring `WETH` from one domain to another, the parent/child gateway architecture unwraps the `WETH` on domain A, transfers the now-unwrapped Ether, and re-wraps it on domain B. This process ensures that `WETH` can behave on the child chain the way users are used to it behaving on the parent chain while ensuring that all `WETH` tokens are always fully collateralized on the layer in which they reside.

No matter the complexity of a particular token's bridging needs, a gateway can, in principle, be created to accommodate it within our canonical bridging system.

You can find an example of the implementation of a custom gateway on the page [How to bridge tokens via a custom gateway](/build-decentralized-apps/token-bridging/bridge-tokens-programmatically/04-how-to-bridge-tokens-custom-gateway.mdx).

### Demos

Our [How to bridge tokens](/build-decentralized-apps/token-bridging/bridge-tokens-programmatically/01-get-started.mdx) section provides examples of interacting with Arbitrum's token bridge via the [Arbitrum SDK](https://github.com/OffchainLabs/arbitrum-sdk).

### "I've got a bridge to sell you," a word of caution

Cross-chain bridging is an exciting design space. Alternative bridge designs can potentially offer faster withdrawals, interoperability with other chains, different trust assumptions with potentially valuable UX tradeoffs, and more. However, they can also potentially be completely insecure and/or outright scams. Users should treat other non-canonical bridge applications the same way they treat any application running on Arbitrum, exercise caution, and do their due diligence before entrusting them with their value.

---

## .mdx (11-l2-to-l1-messaging.mdx)
---
title: Child to parent chain messaging
description: 'Learn the fundamentals of child to parent chain messaging on Arbitrum.'
author: pete-vielhaber
sme: TucksonDev
user_story: As a current or prospective Arbitrum user, I need to learn more about messaging between child chain and parent chain messaging on Arbitrum.
content_type: concept
---

import ImageZoom from '@site/src/components/ImageZoom';

<a data-quicklook-from="arbitrum">Arbitrum</a>'s <a data-quicklook-from="outbox">outbox</a> system
allows for arbitrary <a data-quicklook-from="child-chain">child chain</a> to <a data-quicklook-from="parent-chain">
  parent chain
</a> contract calls, i.e., messages initiated from the child chain, which eventually resolve in
execution on the parent chain. Child-to-parent chain messages (aka "outgoing" messages) bear some
things in common with Arbitrum's [parent chain to child chain
messages](/how-arbitrum-works/10-l1-to-l2-messaging.mdx), which are "in reverse" though with
differences, which we'll explore in this section.

## Protocol flow

Messages from child to parent are included in Arbitrum's Rollup state and finalized through the Rollup Protocol. The process follows these steps:

1. **Message creation on child chain**:
   - To initiate a child-to-parent chain message, a user or contract calls the `sendTxToL1` method on the `ArbSys` precompile.
2. **Message inclusion in an <a data-quicklook-from="assertion">assertion</a>**:
   - The message is batched with other transactions and included in a Rollup assertion.
   - Assertions are submitted to the Rollup contract on the parent chain and enter the dispute window (~one week).
3. **Assertion confirmation**:
   - If the assertion remains unchallenged after the dispute window, the Rollup contract finalizes the assertion.
   - The assertion's Merkle root gets posted to the outbox contract on the parent chain.
4. **Execution on the parent chain**:
   - Once the assertion is confirmed, anyone can execute the message on the parent chain by proving its inclusion.
   - Execution is possible using `Outbox.executeTransaction`, which takes a Merkle proof proving the message exists in the finalized assertion.

## Client flow (How to send and execute messages)

### Sending a message from a child to parent chain

A contract or user sends a message from the child chain by calling:

```solidity
ArbSys(100).sendTxToL1(destAddress, calldata);
```

### Executing the message on the parent chain

After the seven-day <a data-quicklook-from="challenge">Challenge</a> period, if the assertion is confirmed, anyone can execute the message on the parent chain.

1. **Retrieve proof data**:
   - Call the `constructOutboxProof` method from the `NodeInterface` contract to get proof data.

:::note

We refer to `NodeInterface` as a "virtual" contract; its methods are accessible via calls `0x00000000000000000000000000000000000000C8`, but it doesn't live onchain. It isn't a precompile but behaves like a precompile that can't receive calls from other contracts. This approach is a cute trick that lets us provide Arbitrum-specific data without implementing a custom RPC.

:::

2. **Execute on the parent chain**;
   - Call `Outbox.executeTransaction` with the proof data to execute the message on the parent chain.

## Protocol design details

### Constant overhead for node confirmation

- Calling `confirmNode` on the Rollup contract has constant gas overhead, regardless of the number of messages in the assertion.
- This confirmation ensures malicious actors cannot grief the network by submitting assertions with many outgoing messages.

### Why child to parent chain messages require manual execution

Unlike [retryable tickets](/how-arbitrum-works/10-l1-to-l2-messaging.mdx), which can execute automatically with pre-funded gas, child-to-parent chain message must undergo manual execution because Ethereum (parent chain) does not support scheduled execution.

However, applications can implement execution markets that allow third parties to execute messages for a fee.

### Persistence and expiry

- **Child-to-parent chain messages**: - Persist indefinitely on the parent chain once included in the outbox.

## Parent-to-child chain message lifecycle

Each message progresses through three primary states:
| Stage | Description |
| ------------------------ | ----------- |
| Posted on child chain | The message is sent via `ArbSys.sendTxToL1`. |
| Waiting for finalization | The assertion containing the message is in the <a data-quicklook-from="challenge-period">challenge period</a> (~6.4 days) |
| Confirmed and executable on the parent chain | If no <a data-quicklook-from="fraud-proof">fraud proof</a> is submitted, the assertion is confirmed, and the message is available for execution in the outbox. |

## Withdrawing Ether

Withdrawing Ether can be done using the [ArbSys precompile](/build-decentralized-apps/precompiles/02-reference.mdx#arbsys)'s `withdrawEth` method:

```solidity
ArbSys(100).withdrawEth{ value: 2300000 }(destAddress)
```

Upon withdrawing, the Ether balance is burnt on the child chain side and will later be made available on the parent chain side.

`ArbSys.withdrawEth` is a convenience function equivalent to calling `ArbSys.sendTxToL1` with empty `calldataForL1`. Like any other `sendTxToL1` call, it will require an additional call to `Outbox.executeTransaction` on the parent chain after the dispute period elapses for the user to finalize claiming their funds on the parent chain. Once the withdraw executes (removed from the outbox), the user's Ether balance will receive credit on the parent chain.

The following diagram depicts the process that funds follow during a withdrawal operation.

<ImageZoom
  src="/img/dapps-withdrawing-ether.svg"
  alt="Process funds follow during withdrawal operation"
  className="img-800px"
/>

## `ERC-20` token withdrawal

Arbitrum has a canonical <a data-quicklook-from="bridge">bridge</a> design and architecture, which we explain in detail in the [Token bridging](/how-arbitrum-works/10-l1-to-l2-messaging.mdx#token-bridging) section of Bridging from a parent chain to a child chain article. This section will explain how the Arbitrum canonical bridge works on the child-to-parent chain token bridging.

1. Initiation of a child-to-parent chain transfer occurs via the `L2GatewayRouter` contract on the child chain.
2. The token's gateway contract (`L2ArbitrumGateway`) on the child chain gets called.
3. `L2ArbitrumGateway` finally communicates to its corresponding gateway contract on the parent chain using the `L1ArbitrumGateway` contract.

<ImageZoom
  src="/img/dapps-bridge_withdrawals.png"
  alt="Withdrawal process using the gateway"
  className="img-800px"
/>

For any given gateway pairing, calls have to be initiated through the corresponding router (`L2GatewayRouter`), and the gateways must conform to the `TokenGateway` interfaces; the `TokenGateway` interfaces should be flexible and extensible enough to support any bridging functionality a particular token may require.

Token withdrawals are the most common usage of child-to-parent chain messaging. The standard withdrawal happens as follows (for standard gateway):

1. On the child chain, a user calls `L2GatewayRouter.outBoundTransfer`, which calls `outBoundTransfer` on `arbSomeERC20Token`'s gateway (i.e., `L2ERC20Gateway`).
2. This call burns `arbSomeERC20Token` tokens and calls `ArbSys` with an encoded message to `L1ERC20Gateway.finalizeInboundTransfer`, which will eventually execute on the parent chain.
3. After the dispute window expires and the assertion with the user's <a data-quicklook-from="transaction">transaction</a> receives confirmation, a user can call `Outbox.executeTransaction`, which calls the encoded `L1ERC20Gateway.finalizeInboundTransfer` message, releasing the user's tokens from the `L1ERC20Gateway` contract's escrow.

---

## .mdx (12-data-availability.mdx)
---
title: Data Availability
description: 'Learn how data availability works in arbitrum'
author: JasonWan
sme: JasonWan
user_story: As a current or prospective Arbitrum user, I need to learn more about how data availability works on Arbitrum.
content_type: get-started
---

# How Arbitrum data availability works

## What is the general view of Arbitrum data flow?

Arbitrum currently supports two primary data availability mechanisms:

### Rollup Mode

In this mode, all transaction data is included in either the calldata of transactions submitted to the parent chain (e.g., Ethereum mainnet for Arbitrum One) or the blobs submitted by the transaction. This inclusion ensures that all data is readily available onchain for anyone to download and verify.

### AnyTrust Mode

In AnyTrust mode, transaction data initially gets submitted to a group of nodes known as the Data Availability Committee (DAC). The DAC stores and distributes the data. Instead of including the entire dataset onchain, only a cryptographic proof that the data has been stored by the DAC (Data Availability Certificate, or DACert) is submitted to the parent chain. This proof significantly reduces the amount of data stored onchain, reducing costs.

Because of those data availability mechanisms, Arbitrum Nitro nodes synchronize their data differently than Ethereum nodes or other layer-one network nodes. While Go-Ethereum nodes utilize a sophisticated P2P network to synchronize with the Ethereum blockchain by discovering other nodes, exchanging data, and participating in the consensus mechanism, Arbitrum nodes diverge from this traditional approach and use a trustless process.

Here's how Arbitrum data flow works:

1. Batching and submission:
   1. The sequencer queues transactions and batches them together.
   2. These batches get submitted to the parent chain:
      1. In Rollup mode, the sequencer submits the batch of transactions directly to the sequencer inbox contract on the parent chain. (Blob or calldata directly)
      2. In AnyTrust mode, the sequencer sends the batch to the Data Availability Committee (DAC) and then submits the Data Availability Certificate (DACert) which is returned and generated by the DAS to the parent chain.
2. Node synchronization:
   1. Upon joining the network, a full node:
      1. In Rollup mode, data is read directly from the parent chain calldata or blobs (depending on how the sequencer posts the data).
      2. In AnyTrust mode, it checks the DACert to verify data availability and queries the data from the DAC.
   2. The node continues to follow this process to catch up with the latest chain height.
   3. Once caught up, the node receives updates on new sequencer-queued messages directly from the sequencer feed (we will provide details of this process in the last section).
3. Catching up:
   1. If a node falls behind the chain, it reverts to the process described in Step 2 to resynchronize with the latest state.

In essence, Arbitrum nodes prioritize data retrieval from the parent chain and rely on the sequencer for real-time updates, deviating from the traditional P2P synchronization approach used by Ethereum nodes.

## How full nodes decode the data from the parent chain

Arbitrum full nodes decode data received from the parent chain (and, in the case of AnyTrust chains, the DAC) to update their local state. This process involves monitoring events, parsing data, and processing messages.

1. Event querying:
   1. Full nodes subscribe to the `SequencerBatchDelivered` event emitted by the inbox contract on the parent chain. This event signifies the arrival of a new batch of transactions.
2. Event parsing:
   1. Upon receiving the `SequencerBatchDelivered` event, the node parses the event data into a `SequencerInboxBatch` struct. This struct typically includes:
      1. `BlockHash`: The hash of the parent chain block containing the batch.
      2. `ParentChainBlockNumber`: The block number of the parent chain block.
      3. `SequenceNumber`: The sequence number of the batch.
      4. `TimeBounds`: Time constraints for the batch.
      5. `AfterDelayedAcc`: Accumulator hash after processing delayed messages.
      6. `AfterDelayedCount`: Count of delayed messages.
      7. `rawLog`: The raw event log data.
3. Data serialization:
   1. The `SequencerInboxBatch` struct serializes into a byte array.
   2. The serialized data adheres to a specific format:
      1. `TimeBounds.MinTimestamp` (8 bytes)
      2. `TimeBounds.MaxTimestamp` (8 bytes)
      3. `TimeBounds.MinBlockNumber` (8 bytes)
      4. `TimeBounds.MaxBlockNumber` (8 bytes)
      5. `AfterDelayedCount` (8 bytes)
      6. `payload` (variable length)
         1. The `payload` field further contains the following:
            1. **Type:** Indicates the header of payload (e.g., DACert, blob message).
            2. **Content:** The actual data associated with the payload header (e.g., DACert, BlobHashes, brotli compressed data).
4. Data decoding and retrieval:
   1. Based on the `payload` header:
      1. **DAS Message header:** The node queries the Data Availability Servers (DAS) to retrieve the raw data.
      2. **Blob message header:** The node decodes the blob message to obtain the raw data.
      3. **Brotli Message header:** No extra steps are needed here; continue to the next step.
   2. Data decompression: If the raw data is Brotli-compressed, the node decompresses it. It's worth noting that the raw data we get from above i and ii might also be Brotli-compressed data.
5. Message processing:

   1. After decoding and decompressing the data, the node obtains a series of batch segment messages.

      1. Message Types:

         | Batch Segment Message type             | What is the usage of this message                                                                        |
         | -------------------------------------- | -------------------------------------------------------------------------------------------------------- |
         | `BatchSegmentKindL2Message`            | This message will contain raw data on a series of transactions. Usually, this is a single block.         |
         | `BatchSegmentKindL2MessageBrotli`      | The message is the same as the above one, but this is brotli compressed data.                            |
         | `BatchSegmentKindDelayedMessages`      | This message contains a new delayed message read from the parent chain Delayed Inbox.                    |
         | `BatchSegmentKindAdvanceTimestamp`     | This message will notify the State Transition Function (STF) to advance a second of the timestamp state. |
         | `BatchSegmentKindAdvanceL1BlockNumber` | This message will notify STF to advance a new parent chain block number.                                 |

   2. State transition: finally, the State Transition Function (STF) processes these messages, and the STF will follow the rules to execute and update the Arbitrum node's local state.

## How full nodes sync the data from the sequencer feed

Once Arbitrum full nodes have caught up with the chain, they switch from initial synchronization to a real-time update mode. This switch involves receiving data from the sequencer feed, which continuously broadcasts updates about newly queued transactions.

1. Data acquisition:
   1. Full nodes maintain a connection to the sequencer feed or your private feed. For how to run a private feed, please refer to [**How to run a feed relay**](/run-arbitrum-node/run-feed-relay.mdx)
   2. The sequencer feed transmits data packets containing information about the latest queued transactions.
2. Data decoding:
   1. Full nodes decode the received data packets using the methods described in [How to read the sequencer feed](/run-arbitrum-node/sequencer/02-read-sequencer-feed.mdx).
3. Message processing:

   1. After successful decoding, the full nodes obtain the same type of data as outlined in the previous section's Step 5.
   2. Send the message to the State Transition Function (STF) and execute.

   (This step is the same as the previous section's Step 5)

---

## .mdx (bold/bold-economics-of-disputes.mdx)
---
title: 'Economics of Disputes in Arbitrum BoLD'
sidebar_label: 'Economics of Disputes'
description: 'An in-depth explanation on BoLD economic mechanisms.'
user_story: 'As a user or researcher of the Arbitrum product suite, I want to understand BoLD economics mechanisms.'
content_type: concept
author: leeederek
target_audience: 'Developers, users and researchers interested in the Arbitrum product suite.'
sme: leeederek
---

_The following document explains the economics and denial-of-service protection mechanisms built into <a data-quicklook-from="arbitrum">Arbitrum</a> <a data-quicklook-from="bold">BoLD</a>. It covers trade-offs Arbitrum has to make to enable permissionless validation, explaining the key problems in an accessible way._

## Background

[Arbitrum One](https://arbitrum.io/) is currently one of the most widely used Ethereum scaling solutions, with [~\$14bn `USD` in total-value-locked](https://l2beat.com/scaling/projects/arbitrum) at the time of writing. Not only do its scaling properties, such as its 250ms block times, make it popular, but so do its security properties and approach to decentralization. Currently, <a data-quicklook-from="arbitrum-one">Arbitrum One</a> is governed by the Arbitrum DAO, one of the most active and robust onchain organizations.

However, Arbitrum technology has not yet achieved its [full promise of being fully decentralized](https://docs.arbitrum.foundation/state-of-progressive-decentralization). Currently, <a data-quicklook-from="child-chain">child chain</a> to <a data-quicklook-from="parent-chain">parent chain</a> messaging from Arbitrum One back to Ethereum are verified by a permissioned list of validators. These validators can still <a data-quicklook-from="challenge">challenge</a> invalid withdrawals, but the system prevents anyone outside this list from holding them accountable. This permissioned list of validators limits Arbitrum One and <a data-quicklook-from="arbitrum-nova">Arbitrum Nova</a> to being categorized as a "Stage 1 Rollup" on the [L2Beat website](https://l2beat.com/scaling/summary), meaning it still has training wheels preventing it from reaching its full potential.

The Rollup technology powering Arbitrum One is called "optimistic" because claims about its state settled to and confirmed on Ethereum after a period of approximately seven days. During those seven days, the claimed states can be disputed. To make an analogy, a check can be cashed immediately but can be taken to court to dispute if there is a problem within a specific time frame. Because Arbitrum's state is deterministic, a <a data-quicklook-from="validator">validator</a> that is running a node and following the chain will always know if a posted claim is invalid. A key decentralization property allows **anyone** who knows the correct claim to challenge invalid claims and **_win_** the challenge. This preserves the accurate history of Arbitrum settling to Ethereum and protects the integrity of users' funds and withdrawals using a "single honest party" property. As long as there is a single entity following the chain and willing to dispute a claim, Arbitrum's security guarantees are maintained.

Before the deployment of BoLD, Arbitrum One's security properties were defined by the size of its permissioned set of validators. Validators could collude or finalize/confirm an incorrect state, and users have no recourse aside from the Arbitrum One security council stepping in. Elevating Arbitrum One's decentralization requires a different approach.

In the Fall of 2023, <a data-quicklook-from="offchain-labs">Offchain Labs</a> announced [Arbitrum BoLD](https://medium.com/offchainlabs/bold-permissionless-validation-for-arbitrum-chains-9934eb5328cc), a new dispute resolution protocol built from the ground up that will bring Arbitrum chains to the next level of decentralization. BoLD, which is an acryonym for **Bo**unded **L**iquidity **D**elay, allows permissionless validation of Arbitrum chains. This means that chain owners can remove the list of permissioned validators for their chains to allow anyone to challenge invalid claims made about Arbitrum states on their parent chain and win.

In this document, we'll explore the economics and trade-offs enabling permissionless validation.

## Settling Arbitrum states to Ethereum

We frequently state that "Arbitrum chains settle their states to a parent chain", and we'll elaborate on what that means. All Arbitrum One transactions can be recreated by reading data from the parent chain (Ethereum), as compressed batches of all child chain transactions are frequently posted to Ethereum. Once a batched <a data-quicklook-from="transaction">transaction</a> is included in a finalized block on Ethereum, its history will likely never be reverted on Arbitrum One. However, when Ethereum receives a <a data-quicklook-from="batch">batch</a> of transactions, it does not know what the correct result of executing those transactions is. To verify the correct result, there is a separate process that confirms batch correctness on Ethereum: it is called the "<a data-quicklook-from="assertion">assertion</a>."

For Arbitrum One specifically, approximately every hour, entities known as validators check the correctness of batches by following the <a data-quicklook-from="arbitrum-chain">Arbitrum chain</a>. Validators can choose to become proposers and propose something called an "assertion", which attests to the validity of a batch, saying, "I have verified this batch". As Ethereum does not know what is correct on Arbitrum One, it allows about seven days for anyone to dispute one of these assertions. Prior to the deployment of BoLD, there was a permissioned list of proposers who could post assertions and challenge assertions for all Arbitrum chains. Arbitrum BoLD enables any <a data-quicklook-from="chain-owner">chain owner</a>, such as the ArbitrumDAO, to remove this permissioned list. Note that validators who opt to post assertions are otherwise known as "assertion proposers".

### Withdrawing assets back to Ethereum from Arbitrum

Users of Arbitrum One that have bridged assets from Ethereum can initiate the process of withdrawing said assets at any time. However, for this withdrawal to be fully executed, its corresponding claim must match a confirmed assertion on Ethereum. For instance, if Alice starts a withdrawal transaction on Arbitrum One, it gets posted in a batch on Ethereum. Then, a validator will post an assertion about that batch on Ethereum an hour later. The assertion has a seven-day window in which anyone can dispute it. After that window passes, the assertion is confirmed by the protocol and Alice will receive her withdrawn assets on Ethereum and is free to use them as she pleases.

"Settling states" and having a seven-day dispute window is crucial to ensuring assets can be withdrawn safely. Allowing anyone to dispute invalid claims and win keeps withdrawals protected by strong security guarantees without needing to trust a group of validators. This "permissionless validation" separates optimistic rollups from side chains.

### The dispute period

The reason there is a dispute window for assertions about Arbitrum One on Ethereum is because Ethereum itself **has no knowledge about what is correct** on Arbitrum One. The two blockchains are different domains with different states. Ethereum, however, can be used as a neutral referee for parties to dispute claims about Arbitrum One. The dispute period is seven days because it is seen as the maximum period of time an adversary could delay Ethereum before social intervention, originally proposed by Vitalik Buterin. This window gives enough time for parties to catch invalid claims and challenge them accordingly.

### Dispute resolution times

An actual dispute occurs if an honest party disagrees with an assertion on Ethereum and posts an assertion they know to be correct as a counter-claim, or if a dishonest party decides to post to Ethereum a spurious assertion they know to be wrong, after another assertion has already been posted. This creates a "fork" in the chain of assertions, requiring a resolution process. We'll get into the high-level details of how disputes are resolved later in this document.

Once an actual dispute is ongoing, it will also take time to resolve, as, once again, Ethereum has no knowledge of the correctness of Arbitrum One states. Ethereum must then give sufficient time for parties to submit their proofs and declare a winner. The new Arbitrum BoLD protocol **guarantees that a dispute will be resolved within seven days** so long as an honest party or parties are present to defend against invalid claims, and have access to enough resources to pay for the costs of participating in the protocol, for details see the [Preventing Spam](#preventing-spam) section below.

As assertions have a dispute window of seven days, and disputes require an additional seven days to resolve, a dispute made at the last second would **delay assertion <a data-quicklook-from="confirmation">confirmation</a> to a maximum of 14 days**, or two weeks. BoLD is the only dispute protocol we are aware of that guarantees this bound.

### The cost of delaying withdrawals

Delaying withdrawals incurs opportunity costs and impacts user experience for users who want to withdraw their assets. In the happy case of no disputes, withdrawals already have a baked-in, seven-day delay. A dispute adds seven days to that delay. The problem is that disputes delay _all_ pending withdrawals from Arbitrum One back to Ethereum, not just a single claim. As such, **disputing a claim must have a cost for the initiator** proportional to the opportunity cost they impose on Arbitrum users.

#### Requiring a bond to become a validator

By default, all Arbitrum nodes act as validators, monitoring the chain to verify assertions posted to the parent chain and flagging any invalid assertions. On Arbitrum One, running a validator, known as a “watchtower” node, is permissionless and requires no additional cost other than the infrastructure for the node.

Another type of validator, called a "proposer," performs additional tasks on top of their regular duties as a regular validator. Proposers compute Arbitrum states and propose assertions to the parent chain. To prevent abuse and delays in withdrawals, proposers must make a security deposit or "bond" to gain the privilege of proposing assertions. This bond can be withdrawn once their latest assertion is confirmed, ending their responsibilities as a proposer.

Arbitrum BoLD allows validators to become proposers and challengers without permission. Proposers must bond `ETH` to propose state assertions to the parent chain. Only one proposer is needed for chain progress, allowing most validators to simply verify assertions. In case of disputes over state assertions, BoLD enables anyone to put up a "challenge bond" of `ETH` to dispute invalid assertions, acting as a challenger in defense of an Arbitrum chain.

For more details on different strategies validators can use refer to [How to run a validator](/run-arbitrum-node/more-types/02-run-validator-node.mdx).

#### Pricing bonds

Ensuring assertions are frequently posted is a requirement for Arbitrum, but at the same time, it should not be a privilege that is easily obtained, which is why the pricing of this "security deposit" is based on opportunity cost.

To be highly conservative, we want to account for a "bank run"-like scenario, in which everyone wants to withdraw their assets from Arbitrum One at the same time. The [Arbitrum One bridge](https://etherscan.io/address/0x8315177ab297ba92a06054ce80a67ed4dbd7ed3a) contains approximately \$3.4B USD worth of assets at the time of writing on Oct 23rd, 2024. Assuming funds could earn a 5% APY if invested elsewhere, the opportunity cost of 1 extra week of delay in withdrawing them from Arbitrum One is approximately \$3.27M USD. Given this scenario, we recommend a bond for assertion posters to be greater than \$3.7M USD.

Honest proposers can always withdraw their bond once their assertions are confirmed. However, adversaries stand to lose the entirety of their bond if they propose invalid assertions. A large bond size drastically improves the economic security of the system based on these two axes by making the cost to propose high and by guaranteeing that malicious actors will lose their entire bond when they are proven wrong by the protocol.

Given that participation in BoLD is permissionless, we recommend that the size of bonds required to participate be high enough to disincentivize malicious actors from attacking Arbitrum One and to mitigate against spam (that would otherwise delay confirmations up to one <a data-quicklook-from="challenge-period">challenge period</a>). High bonding values do not harm decentralization because (1) <a data-quicklook-from="trustless">trustless</a> bonding (or staking) pools can be deployed permissionlessly to open challenges and post assertions, and (2) any number of honest parties of unknown identities can emerge to bond their funds to the correct assertion and participate in the defense of Arbitrum at any time within a challenge. As with the current dispute resolution protocol, there are no protocol level incentives for parties who opt in to participate in validating Arbitrum One with BoLD.

While both of these bonds can be any `ERC-20` token and be set to any size, we recommend the use of the `WETH` ERC-20 token & the following bond sizes for Arbitrum One:

- Assertion bonds: 3600 `ETH` is required from validators to bond their funds to an assertion in the eventual hopes of having that assertion be confirmed by the Rollup protocol. This is a one-time bond required to be able to start posting assertions. This bond can be withdrawn once a validator’s assertion is confirmed and can alternatively be put together via a trustless bonding pool.

- Challenge-bonds, per level: 555 `WETH` at the "big-step" level; 79 `WETH` at the "small-step" level - required from validators to open challenges against an assertion observed on the parent chain (Ethereum, in the case of Arbitrum One), for each level. Note that “level” corresponds to the level of granularity over which the interactive bisection game gets played, starting at the block level, moving on to a range of <a data-quicklook-from="wasm">WASM</a> execution steps, and then finally to the level of a single execution step. For more details on the concept of "levels" in BoLD challenges, see [Challenge resolution](/how-arbitrum-works/bold/bold-technical-deep-dive.mdx#challenge-resolution) section in the Technical deep dive.

These values were carefully calculated to optimize for the resource ratio (explained later) and gas costs in the event of an attack, as explained in [BoLD whitepaper](https://arxiv.org/abs/2404.10491). This effectively means that an entity that has already put up a bond to propose an assertion does not need to put up a separate assertion bond to challenge an invalid state assertion that they observe. To be explicitly clear, the validator would still require 555 `ETH` and 79 `ETH` for ongoing challenges. These additional challenge bond amounts are needed to participate in the interactive dispute game (back and forth) and narrows down the disagreement to a single step of execution that is then proven on Ethereum. The 555 `ETH` and 79 `ETH` challenge bonds can be put together via a trustless bonding pool, and do not all have to be put up by the validator that opened the challenge. These bonds can be refunded at the end of a challenge and can also alternatively be put together by the community using a trustless bonding pool.

#### Centralization concerns

Requiring a high bond to post assertions about Arbitrum seems centralizing, as we are replacing an allowlist of validators with a system that requires substantial funds to participate. However, **BoLD ships with a trustless bonding pool** for assertion posting. That is, any group of honest parties can pool funds into a simple contract that will post an assertion to Ethereum without needing to trust each other. We believe that making it easy to pool the funds to become an assertion poster without needing trust to dispute invalid claims does not affect the safety or decentralization of BoLD.

We claim optimizing for the unhappy case is more important than the happy case. As there only needs to be one honest assertion poster, we believe it falls into the security budget of the chain to set a high bond fee in order to become a proposer. It _should_ be expensive to delay Arbitrum One withdrawals, and it should also have a high barrier to entry to perform a key responsibility. As long as disputes can be made in a trustless manner, and trustless pools are available in production, we claim the security properties of assertion posting hold equally.

## Resolving disputes

One of the core properties BoLD achieves is providing a fixed upper bound for dispute resolution times. This section will discuss the constraints required to achieve this from first principles.

### Dispute game overview

Every game between adversarial parties needs a referee: a neutral party that can enforce the rules to declare a fair winner. Arbitrum BoLD relies on Ethereum as its referee because of its properties as the most decentralized, censorship-resistant <a data-quicklook-from="smart-contract">smart contract</a> chain in the world.

When a dispute happens about Arbitrum One assertions on Ethereum, there is a protocol for resolving them. At its core, a dispute is about the blockhash of an Arbitrum One block at a given height. Ethereum does not know which claim is correct and, instead, relies on a dispute game to be played out. The game involves different parties making claims with proof to eventually narrow down their disagreement to a single step of execution within the execution of a block, called a one-step proof (OSP). Ethereum can then verify this OSP by itself and, as the neutral referee, declare a winner.

The "rules" of the dispute involve parties making claims with proof to reach the single point of disagreement. Parties "narrow down" their claims via moves called bisections. After a party has made a bisection, there is nothing else left to do until another party comes in and counters it. The core of the system is that an honest party winning a one-step proof leaves the malicious party with no other moves to make. Once the honest party has accumulated enough time without being countered, it is declared the winner.

Compared to other dispute protocols, however, BoLD is **not** a dispute between two specific Ethereum addresses, such as Alice and Bob. Instead, it is a dispute between an absolute, correct history vs. an incorrect one. Claims in BoLD are not attached to a particular address or validator but instead to Merkle commitments of an Arbitrum chain's history. If Alice and Charlie are both honest, and Bob is malicious, Alice and Charlie can play the game as part of a single "team". If Alice goes offline in the middle of a dispute-game, Charlie can continue resolving the game on behalf of the honest team because Charlie and Alice claim and make moves on the correct history. This is why we say BoLD enables "trustless cooperation," as there is no need for communication between honest parties. We believe committing a set of chain history hashes instead of a specific hash at a moment in time is crucial for securing dispute protocols.

For more technical details on the BoLD dispute protocol, see the [Technical deep dive](/how-arbitrum-works/bold/bold-technical-deep-dive.mdx) or the [BoLD research whitepaper](https://arxiv.org/abs/2404.10491).

### Spamming the dispute game

BoLD is a dispute game in which the assertion that has accumulated seven days "not-countered" wins. That is, parties are incentivized to counter any new claims as soon as they appear to "block" their rivals from increasing their timers. For honest parties, responding to claims may sometimes require offchain computational work and, therefore, resources such as CPUs. However, malicious parties can make claims that are eventually found to be junk while making honest parties do actual work.

Because malicious parties can submit incorrect claims that make honest parties do work, there has to be an economic cost associated with making moves in the dispute-game. Said differently, we need a way to prevent **spam attacks** in dispute games.

#### The cost of moves

When pricing the bonds required to make claims within disputes, we consider the marginal costs that the honest party incurs for each claim a malicious party makes. The BoLD research paper includes information such as the number of adversary moves multiplied by the gas cost of making bisections and claims and some estimates of the offchain computational costs. We deem this the **marginal cost** of a party in a dispute.

With BoLD, the space of disagreements between parties is of max size 2^69. As such, the dispute game has to be played at different levels of granularity to make it computationally feasible.

Let's use an analogy: say we have two one-meter sticks that seem identical, and we want to determine where they differ. They appear identical at the centimeter level, so we need to go down to the millimeter level, then the micrometer level, and then figure out where they differ at the nanometer level.

This is what BoLD does over the space of disputes. Parties play the same game at different levels of granularity. At the centimeter level, each centimeter could trigger a millimeter dispute, and each millimeter dispute could have many micrometer disputes, etc. This dispute pattern could be abused unless spam is discouraged.

#### Preventing spam

Since Ethereum knows nothing about which claims are honest or malicious until a one-step proof is provided, how can the protocol detect and discourage spam? A key insight is that honest parties only need to make one honest claim. Honest parties will never spam and create thousands of conflicting claims with themselves. Given this, we can put a price tag on making moves by looking at something called the "resource ratio" between honest and malicious parties, as defined in the BoLD research paper.

This ratio is computed as gas plus staking (or bonding) marginal costs of the adversary to the honest party. This means that certain values input into the equations can lead to **different ratios**. For instance, we can say the adversary has to pay **10x** the marginal costs of the honest party. However, aiming to increase this ratio significantly by plugging in different values leads to higher costs for all parties.

#### Dispute mini-bonds

We require parties to lock up some capital called a "mini-bond" when making big claims in a dispute. These bonds are not needed when making bisection moves but are critical for posting an initial claim. Pricing these mini-bonds helps achieve a high resource ratio of dishonest parties to honest parties.

:::note

"Mini-bonds" is another term for "challenge-bonds" mentioned above in [Pricing bonds](#pricing-bonds).

:::

It is clear that if we can multiply the cost to the malicious party by some multiplier of the honest party, we will get significant security benefits. For instance, imagine if a one billion dollar attack can be defended by simply pooling together 10 million dollars. Is it possible to achieve such a ratio?

Let's explore the limitations of making the cost to malicious parties higher than the honest parties.

If we aim to have a constant resource ratio > 1, we have to do the following: if the adversary makes `N` bonds at any level, they can force the honest party to make `N` bonds at the next level down, where the adversary can choose not to place any bonds at all. Regarding resource ratio, to make the adversary always pay 10x in staking, we need to make the bond amount at one level 10x more than the next (as we go "upward" from sub-challenges towards the assertion-level challenge). As there are multiple levels, the equations for the bond size include an exponential factor on the desired constant resource ratio > 1.

Below, we plot the bond size vs. the resource ratio of malicious to honest costs. The source for these equations can be found in the research paper and is [represented in this calculator](https://www.desmos.com/calculator/digjlq4vly).

If we desire a constant resource ratio of malicious to honest costs > 1, the required bond size in `ETH` increases as a polynomial at a particular challenge level.

#### Trade-offs

Having a 1000x resource ratio would be nice in theory, but it would, unfortunately, require a bond of 1M `ETH` (\$2.56B USD at time of writing) to open a challenge in the first place, which is unreasonable. Instead, we can explore a more feasible ratio.

The resource ratio will drive the price of disputes claims, impacting both honest and malicious parties. However, claims can **always be made** through a **trustless pool**. Honest parties can pool together funds to participate in disputes.

#### The sweet spot

So, now that we've established that a higher resource ratio is better but comes with some trade-offs, what is the sweet spot?

We propose a resource ratio of 6.46 for Arbitrum One. While odd, this resource ratio was calculated taking the initial "bond" to become a proposer (mentioned earlier) and a worst-case scenario of 500 `gwei`/gas on the parent chain for posting assertions and making sub-challenge moves (i.e., if an attack were to happen, the malicious actor could choose to perform their attack during a period of elevated gas prices). Again, we should emphasize that the ratio of malicious to honest cost should be high to sufficiently deter attacks. Under our current assumptions (500 `gwei`/gas) and proposed parameters (bond sizes, etc.), for every \$6.46 spent by malicious parties attacking, only \$1 is needed to defend it successfully in BoLD. Here's a [direct link to the calculations](https://www.desmos.com/calculator/usmdcuopme) where the X-axis is parent chain gas costs in `gwei` and the Y-axis is the resource ratio.

Unfortunately, there is no "one size fits all" framework for choosing the resource ratio for your chain. Therefore, we recommend teams learn and understand the benefits and trade-offs of operating BoLD in a permissionless format - including performing the same type of economic risk analyses we have performed for Arbitrum One.

## Thinking about incentives

Although we have made claims with hard numbers about how to price disputes and withdrawal delays in Arbitrum BoLD, we also took a step back and considered the theoretical assumptions we were making. Arbitrum One is a complex protocol used by many groups of people with different incentives. The research team at Offchain Labs has spent considerable effort studying the game theory of validators in optimistic rollup. Honest parties represent everyone with funds onchain, and they have a significant amount to gain by winning the challenge––as they can prevent the loss of their assets rather than losing them.

A more complex model is proposed, which considers all parties staking and their associated costs paper ["Incentive Schemes for Rollup Validators"](https://arxiv.org/abs/2308.02880). The paper examines the incentives needed to get parties to check whether assertions are correct. It finds that there is no pure strategy, Nash equilibrium, and only a mixed equilibrium if there is no incentive for honest validators. However, the research showed a pure strategy equilibrium can be reached if honest parties are incentivized to **check** results. The problem of honest validators' "free riding" and not checking is well-documented as the [verifier's dilemma](https://www.smithandcrown.com/glossary/verifiers-dilemma). We believe future iterations of BOLD could include "attention challenges" that reward honest validators for also doing their job.

### Service fee for “Active” proposers

For Arbitrum BoLD's initial launch, we believe that chain owners should pay a service fee to active, top-level proposers as a way of removing the disincentive for participation by honest parties who bond their own capital and propose assertions for Arbitrum One. The fee should be denominated in `ETH` and should correlate to the annualized income that Ethereum mainnet validators receive, over the same time period. At the time of writing, the estimated annual income for Ethereum mainnet validators is approximately 3% to 4% of their bond (based on [CoinDesk Indices Composite Ether Staking Rate (CESR) benchmark](https://www.coindesk.com/indices/ether/cesr) and [Rated.Network](https://explorer.rated.network/network?network=mainnet&timeWindow=7d&rewardsMetric=average&geoDistType=all&hostDistType=all&soloProDist=stake)).

This service fee can be paid out upon an active proposer’s top-level assertion being confirmed on Ethereum and will be calculated using the duration of time that the proposer was considered active by the protocol. The procedure that calculates this will be handled offchain, using a procedure that will be published at a later date. BoLD makes it permissionless for any validator to become a proposer and also introduces a way to pay a service fee to honest parties for locking up capital to do so. Validators are not considered active proposers until they successfully propose an assertion with a bond.

:::note

We envision the Arbitrum Foundation (AF) running its own proposer. This proposer's bonding capital will be funded by the AF and/or the DAO, and (unlike other proposers) will not earn a service fee since it is being run as a public good using the community's own money.

:::

In order to become an active proposer for an Arbitrum chain, post-BoLD, a validator has to propose a state assertion to its parent chain. For Arbitrum One and Nova, the state assertion is posted onto the parent chain (Ethereum). If they do not have an active bond on the parent chain, they then need to attach a bond to their assertion in order to successfully post the assertion. Subsequent assertions posted by the same address will simply move the already-supplied bond to their latest proposed assertion. Meanwhile, if an entity, say Bob, has posted a successor assertion to one previously made by another entity, Alice, then Bob would be considered by the protocol to be the current active proposer. Alice would no longer be considered by the protocol as the active proposer and once Alice’s assertion is confirmed, then Alice gets her assertion bond refunded. There can only be one “active” proposer at any point in time.

For Arbitrum One specifically, all eligible entities who wish to be paid this service fee from the Arbitrum Foundation must undergo the Arbitrum Foundation’s KYC process as no AIP "may be in violation of applicable laws, in particular sanctions-related regulations". This is also written in the [ArbitrumDAO's Constitution](https://docs.arbitrum.foundation/dao-constitution#section-2-dao-proposals-and-voting-procedures).

### Rewards and Reimbursements for Defenders

The service fee described above is meant to incentivize or reimburse an honest, active proposer for locking up their capital to propose assertions and advance the chain. Similarly, in the event of an attack, a bounty is proposed to be paid out to honest defenders using confiscated funds from malicious actors (in the event of a challenge).

For Arbitrum One specifically, 1% (called the “defender’s bounty”) of the confiscated funds from a malicious actor is to be rewarded to honest parties who deposit a challenge bond and post assertions as part of a sub-challenge, proportional to the amount that a defender has put up to defend a correct state assertion during the challenge. This bounty applies to all challenges (block challenges, sub challenges and one-step challenges). Note that any gas costs spent by honest parties to defend Arbitrum One during a challenge are 100% refundable by the Arbitrum Foundation. In this model, honest defenders and proposers of Arbitrum One are incentivized to participate while malicious actors stand to lose everything they spent attacking Arbitrum One. We believe chain owners who are interested in adopting BoLD for their own chain should follow a similar approach, described above for Arbitrum One, to incentivize challenge participation (but not necessarily assertion proposing).

In this design, defenders are only eligible for the defender's bounty if they deposit a challenge bond (for Arbitrum One, this is either 555 or 79 `ETH`, depending on the level), posted to an onchain assertion as part of a sub-challenge (i.e., not the top-level assertion), and have had their onchain sub-challenge assertion get confirmed by the protocol. For Arbitrum One, the calculation for the defender's bounty is conducted offchain by the Arbitrum Foundation, and payment will be made via an ArbitrumDAO governance vote (since confiscated funds go to an ArbitrumDAO-controlled address). Honest parties are not automatically rewarded with all the funds seized from malicious actors to avoid creating a situation where honest parties wastefully compete to be the first to make each honest move in the interactive fraud-proof game. Additionally, BoLD resolves disputes by determining which top-level assertion is correct, without necessarily being able to classify every move as “honest” or “malicious” as part of the interactive fraud-proof game without offchain knowledge.

Once all of a validator’s proposed assertions are confirmed, a validator can withdraw their bond in full. Additionally, the protocol will automatically handle refunds of challenge bonds for honest parties and confiscation of bonds from malicious parties in the event of a challenge. In other words, bonds put up by honest parties will always be returned and bonds put up by malicious parties will always be confiscated. For Arbitrum One specifically, parent chain gas costs for honest parties defending a challenge will be reimbursed by the Arbitrum Foundation using a procedure that will be published at a later date. The chain owner could therefore consider the cost of incentivizing or lending the assets to a single honest proposer in the happy case as the **security budget of the chain**.

For Arbitrum One specifically, all eligible entities who wish to be paid the defender's bounty from the ArbitrumDAO must undergo the Arbitrum Foundation’s KYC process as no AIP "may be in violation of applicable laws, in particular sanctions-related regulations". This is also written in the [ArbitrumDAO's Constitution](https://docs.arbitrum.foundation/dao-constitution#section-2-dao-proposals-and-voting-procedures).

## Conclusion

This page summarizes the rationale behind choosing bond sizes and the cost of spam prevention in optimistic rollup dispute protocols. We recommend that bond sizes be high enough to discourage challenges from being opened, as malicious parties will always stand to lose when playing the game. As Arbitrum BoLD does not tie disputes to specific addresses, honest parties can have trustless cooperation to resolve disputes if desired. We posit that making the cost of the malicious parties 10x that of the honest party leads to nice economic properties that help us reason about how to price bonds. We describe how a 6.46x ratio (which BoLD as deployed will achieve) represents a pragmatic point in the design space that strikes a balance between concerns about staking costs against concerns about spam. Finally, we look at a high-level game theory discussion of optimistic rollups and argue that solving the verifier's dilemma by incentives to honest validators is an important addition to work towards.

The topic of further improvements and new economic and incentive models for BoLD are valuable and we believe it deserves the full focus and attention of the community in future proposals and discussions. Details around additional or new proposed economic or incentive models for BoLD will need continued research and development work. Still, the deployment of BoLD as-is represents a substantial improvement to the security of Arbitrum even without all economic-related concerns being fully resolved.

---

## .mdx (bold/bold-technical-deep-dive.mdx)
---
title: 'BoLD: a technical deep dive'
sidebar_label: 'Technical deep dive'
description: 'A technical deep dive into the BoLD protocol.'
user_story: 'As a user or researcher of the Arbitrum product suite, I want to understand the technical choices behind the BoLD protocol.'
content_type: concept
author: leeederek
target_audience: 'Developers, users and researchers interested in the Arbitrum BoLD protocol.'
sme: leeederek
sidebar_position: 1
---

import ImageZoom from '@site/src/components/ImageZoom';

## Overview

<a data-quicklook-from="arbitrum">Arbitrum</a>'s current dispute protocol involves defending against
challengers individually in a 1-vs-1 tournament setting. In contrast, <a data-quicklook-from="bold">
  BoLD
</a> enables an all-vs-all battle royale between Good and Evil, with a single winner always
determined. This dynamic is made possible by BoLD's time-bounded, permissionless validation using
deterministic Merkle proofs and hashes. This allows any party to bond in the correct state and prove
their claim through interactive fraud proofs, ensuring that a single honest party bonding in the
correct state will always prevail in disputes.

Validators on Arbitrum can post their claim on the validity of state roots, known as **assertions**. Ethereum, of course, does not know anything about the validity of these Arbitrum state roots, but it _can_ help prove their correctness. _Anyone_ in the world can then initiate a <a data-quicklook-from="challenge">challenge</a> over any unconfirmed <a data-quicklook-from="assertion">assertion</a> to start the protocol’s game.

The assertions being disputed concern block hashes of an <a data-quicklook-from="arbitrum-chain">Arbitrum chain</a> at a given <a data-quicklook-from="batch">batch</a>/inbox position. Given that Arbitrum chains are deterministic, there is only one correct history for all parties running the standard Nitro software. Using the notion of one-step proof, Ethereum can check whether someone is making a fraudulent assertion after an interactive game is played to narrow down a dispute.

If a claim is honest, it can be confirmed on Ethereum after a 6.4-day period (although the DAO can change this period). If a claim is malicious, anyone who knows the correct Arbitrum state can successfully challenge it within that 6.4-day window _and always win_ within a <a data-quicklook-from="challenge-period">challenge period</a> plus some small delta.

The current implementation of BoLD involves both onchain and offchain components:

1. Rollup contracts to be deployed on Ethereum.

2. New challenge management contracts to be deployed on Ethereum.

3. [Honest validator software](https://github.com/OffchainLabs/nitro) equipped to submit assertions and perform challenge moves on any assertions it disagrees with. The honest <a data-quicklook-from="validator">validator</a> is robust enough to win against malicious entities and always ensures honest assertions are the only ones confirmed onchain.

### Key terminology

- **Arbitrum Rollup contracts:** The set of smart contracts on Ethereum <a data-quicklook-from="parent-chain">parent chain</a> that serve as both the data availability layer for Arbitrum and for confirming the rollup's state assertions after a challenge period has passed for each assertion made.

- **Assertions:** A claim posted to the Arbitrum Rollup contracts on Ethereum (parent chain) about the Arbitrum (<a data-quicklook-from="child-chain">child chain</a>) execution state. Each claim consumes messages from the Arbitrum Rollup inbox contract. Each assertion can be confirmed after a period of 6.4 days, and anyone can challenge it during that period. A BoLD challenge will add an additional upper bound of 6.4 days to confirm an assertion. Gaining the right to post assertions requires placing a large, one-time bond, which can get taken away in the case of losing a challenge to a competing assertion.

- **Bonding:** Participants in the protocol need to bond a certain amount of `WETH` to gain the privilege of posting assertions to the Rollup contracts by locking up an `WETH` bond in the protocol contracts. Whenever someone wants to create a sub-challenge, they must also place a smaller bond called a challenge bond when they do so. Bonds, their rationale, and magnitude will be covered in greater detail in the [Opening challenges](#opening-challenges) and [Sub-challenges](#sub-challenges) sections below, as well as the [Economics of disputes](/how-arbitrum-works/bold/bold-economics-of-disputes.mdx) page.

- **Bonding of funds:** Creating an assertion in the Rollup contracts requires the submitter to join the validator set by putting up a large bond as `WETH`. Subsequent assertions posted by the same party do not require more bonds. Instead, the protocol always considers validators bonded to their latest posted assertion. The bonded funds are taken away if another competing assertion is confirmed. When an assertion is confirmed, the associated bonded funds can be withdrawn.

- **Chain bindings:** Software that can interact with an Ethereum node in order to make calls and transactions to the onchain contracts needed for participating in the protocol. We utilize go-ethereum’s abigen utilities to create Go bindings to interact with the contracts above, with a few more developer-friendly wrappers.

- **ChallengeManager:** This is a contract that allows for initiating challenges on assertions and provides methods for anyone to participate in challenges in a permissionless fashion. BoLD will require a new `ChallengeManager` written in Solidity and deployed to Ethereum. The challenge manager contains entry points for making challenge moves, opening leaves, creating sub-challenges, and confirming challenges.

- **Challenge manager <a data-quicklook-from="client">client</a>:** Software that can manage the life cycles of challenges that an <a data-quicklook-from="active-validator">active validator</a> participates in. Validators need to be able to participate in multiple challenges at once and manage individual challenge vertices correctly to act upon, confirm, or reject them.

- **Challenge period:** Window of time ([6.4 days on Arbitrum One](/build-decentralized-apps/reference/03-chain-params.mdx)) over which an assertion can be challenged, after which the assertion can be confirmed. This is configurable by the DAO.

- **<a data-quicklook-from="challenge-protocol">challenge protocol</a>:** A set of rules through which a disagreement on an assertion is resolved using Ethereum as the final arbiter. Ethereum's VM can verify one-step proofs of deterministic computation that will confirm a challenge winner in Arbitrum's Rollup contracts once the challenge period has elapsed.

- **Delay attacks:** In a delay attack, a malicious party (or group of parties) acts within the challenge protocol and tries to delay the <a data-quicklook-from="confirmation">confirmation</a> of results back to the parent chain. Before BoLD, the previous challenge protocol allowed adversaries to do this by forcing the honest party to play 1-vs-1 games against them to delay confirmation. In contrast, BoLD has a proven, constant upper bound on confirmation times for assertions in Arbitrum, addressing the biggest flaw of the current challenge mechanism. BoLD validators don’t need to play 1-vs-1 challenges and instead can defend a single assertion against many malicious claims. With delay attacks solved, Arbitrum will be able to allow permissionless validation.

- **Edge:** Edges are a portion of a claim made by a validator about the history of the chain from some end state all the way back to some initial state. Edges are the fundamental unit in a challenge.

- **Fraud proofs:** Proofs that prove or disprove that an invalid state transition has taken place. These proofs are generated by challenge participants and are submitted to a chain's parent chain. For example, Arbitrum Rollups that settle onto Ethereum will have their proofs submitted to Ethereum and verified via a <a data-quicklook-from="smart-contract">smart contract</a>. In this case, these proofs allow Ethereum to be the final arbiter of disagreements over assertions in the Rollup contracts, which cannot be falsified by any parties as there is only a single, correct result of executing a `<a data-quicklook-from="wasm">WASM</a>` instruction on a pre-state. WASM is the assembly language that is used to represent programs whose execution is being disputed. In fact, Arbitrum, before and after BoLD, uses a slightly different language called WAVM when executing challenges. The difference is not important to this discussion, but for details, see the page outlining the differences between [WASM and WAVM](/how-arbitrum-works/05-validation-and-proving/03-proving-and-challenges.mdx#7-wasm-to-wavm-transformation).

- **Honest validator:** An entity that knows the correct state of the Arbitrum child chain and who may want to participate in creating assertions, confirming assertions, and/or challenging invalid assertions if they exist. More specifically, this entity must run an <a data-quicklook-from="arbitrum-full-node">Arbitrum full node</a> in `MakeNodes`, `Defensive`, `StakeLatest`, or `ResolveNodes` mode as described in the [How to run a validator](/run-arbitrum-node/more-types/02-run-validator-node.mdx). Note that there must always be an active proposer (i.e., a validator who actively submits new assertions) to advance the chain and who will need to run a validator in `MakeNodes` mode.

- **OneStepProver:** A set of existing contracts that implement a miniature WASM VM capable of executing one-step-proofs of computation of the child <a data-quicklook-from="chain-state">chain state</a> transition function. This is implemented in Solidity and exists on Ethereum. No changes to the `OSP` contracts are needed for BoLD.

- **Permissionless validation:** The ability for anyone to interact with the Arbitrum Rollup contracts on Ethereum to both post assertions and challenge others' assertions without needing permission. With the release of BoLD, the Rollup contracts on Arbitrum will no longer have a permissioned list of validators.

- **Rollup contract:** This smart contract living on Ethereum allows validators to bond on state assertions about Arbitrum. It's known as [`RollupCore.sol`](https://github.com/OffchainLabs/nitro-contracts/blob/main/src/rollup/RollupCore.sol), and Arbitrum chains use it to post assertions. BoLD requires several changes to how assertions work in this contract, and it now contains a reference to another contract called a `ChallengeManager`, which is new in BoLD.

- **State manager backend:** Software that can retrieve child chain states and produce commitments to `WAVM` histories for Arbitrum based on an execution server. The validator client, described below, will have access to a state manager backend in order to make moves on challenge vertices.

- **Timers:** Each unrivaled edge (that is, an edge without another competing edge) will have a timer that _ticks up_. Time in the protocol is measured using blocks of the first non-Arbitrum ancestor chain (i.e., Ethereum blocks for L2 chains and L3 chains settling to an Arbitrum chain, and parent chain blocks for chains settling to a non-Arbitrum L2 chain), and block numbers are used. An edge's timer stops ticking when a rival edge is created onchain. We'll give a more detailed definition later, but two edges are considered rivals if they agree on some prefix of a computation starting from the same state but disagree on the remainder of that computation. Most importantly, timers are used to confirm assertions when an unrivaled edge's timer and associated assertion reaches the specified challenge period. See the section on [Timers and Edges](./bold-technical-deep-dive.mdx#timers) for more details.

- **Validating <a data-quicklook-from="bridge">bridge</a>:** The smart contract that leverages Ethereum's security and censorship-resistance to unlock bridged assets from the child chain back to the parent. Assets can be unlocked after an assertion (showing that those assets have been bridged from the child chain) has been posted and confirmed.

- **Validator client:** A validator client is software that knows the correct history of the Arbitrum child chain via a state manager backend and can create assertions on the parent chain about them by bonding a claim. A validator is also active in ensuring honest assertions get confirmed and participating in challenging those it disagrees with. In BoLD, an honest validator will also participate in challenges other validators are a part of to support other honest participants. It interacts with the onchain components via chain bindings described above.

- **Validator software:** Software that has knowledge of the correct Arbitrum child chain state at any point. It tracks the onchain rollup contracts for assertions posted and will automatically initiate challenges on malicious assertions if configured to do so by the user. It will participate in new and existing challenges and make moves as required by the protocol to win against any number of malicious entities. Its goals are to ensure only honest assertions about Arbitrum's state are confirmed on Ethereum, and that honest assertions always get confirmed on Ethereum in a timely manner (within at most two challenge periods). All Arbitrum full nodes are watchtower validators by default. This means they do not post claims or assertions unless configured to do so but will warn in case invalid assertions are detected onchain.

### How BoLD uses Ethereum

When it comes to implementing the protocol, BoLD needs to be deployed on a credibly-neutral, censorship-resistant backend to be fair to all participants. As such, Ethereum was chosen as the perfect candidate. Ethereum is currently the most decentralized, secure, smart contract <a data-quicklook-from="blockchain">blockchain</a> to which the full protocol can be deployed, with challenge moves performed as transactions to the protocol’s smart contracts.

A helpful mental model for understanding the system is that it uses Ethereum itself as the ultimate referee for deciding assertion results. Participants in the challenge protocol can disagree over the _results of child chain state transitions_ and provide proofs to the protocol's smart contracts on Ethereum to determine which result is correct. Because computation is deterministic, there will always be a single correct result.

<ImageZoom
  src="/img/haw-transaction-lifecycle.svg"
  alt="Transaction lifecycle diagram showing various pathways for submitting transactions"
  className="img-600px"
/>

_From the **[Nitro whitepaper](https://github.com/OffchainLabs/nitro/blob/master/docs/Nitro-whitepaper.pdf)**. Parent chain blocks are “settled to the parent chain” after a 6.4 day period has elapsed and nobody has challenged their validity on Ethereum._

In effect, there is a miniature Arbitrum state-transition VM [deployed as an Ethereum smart contract](https://github.com/OffchainLabs/nitro-contracts/blob/main/src/osp/OneStepProofEntry.sol) to prove which assertions are correct. However, computation on Ethereum is expensive, which is why this mini-VM is built to handle “one-step proofs” consisting of a single step of WebAssembly (WASM) code. The Arbitrum state transition logic, written in Golang, is also compiled to WASM and will therefore obtain the same results as the VM found in the onchain smart contract. The soundness of the protocol depends on the assumptions that computation is deterministic and equivalent between the onchain VM and the Golang state transition compiled to WASM.

All actors in the protocol have a local state from which they can produce valid proofs, and all honest parties will have the same local state. Malicious entities, however, can deviate from the honest parties in attempts to confirm invalid states through the protocol. Both the protocol and the honest validator client’s job is then to allow honest parties to always win against any number of malicious participants by always claiming the absolute truth.

## Assertions

A key responsibility for Arbitrum proposers is to regularly post claims about the Arbitrum chains’ state to Ethereum at certain checkpoints. These are known as assertions (and are sometimes called child chain state roots). Assertions contain information, most critically:

1. The child chain block hash being claimed

2. A "history commitment" combining hashes of the chain's intermediate state after every block the assertion covers. This is essentially a Merkle-like data structure, a tree from which the asserter only needs to provide the root when posting the assertion.

3. The batch number it corresponds to for the Arbitrum chain

4. The number of messages in the Arbitrum <a data-quicklook-from="sequencer">Sequencer</a> inbox at the time the assertion is created

The following assertion to be posted onchain must consume the specified number of inbox messages from the previous assertion. There is a required delay measured in blocks of the first non-Arbitrum ancestor chain (i.e., Ethereum blocks for L2 chains and L3 chains settling to an Arbitrum chain, and parent chain blocks for chains settling to a non-Arbitrum L2 chain) for assertion posting. Currently, this value is set to equal one hour for BoLD.

Anyone can confirm assertions after a period of 6.4 days if they have not been challenged. In particular, assertions facilitate the process of withdrawing from Arbitrum back to Ethereum. Arbitrum withdrawals require specifying a blockhash, which must be confirmed as an assertion onchain. This is why withdrawals have a delay of 6.4 days if they are not actively challenged.

Validators must become proposers in the rollup contract before being allowed to post assertions. For <a data-quicklook-from="arbitrum-one">Arbitrum One</a> and <a data-quicklook-from="arbitrum-nova">Arbitrum Nova</a>, this involves placing a one-time bond of 3600 `WETH` that is locked in the contract until they choose to withdraw. Validators can only withdraw their bond if their latest posted assertion gets confirmed. Every assertion a validator posts will become their latest bonded assertion. Subsequent bonds are not needed to post more assertions, instead, the protocol “moves” a validator’s bonds to their latest posted assertion.

Assertions form a chain where there can be forks. For instance, a validator might disagree on the history commitment of block state hashes, which an assertion contains. All <a data-quicklook-from="arbitrum-nitro">Arbitrum Nitro</a> nodes are configured to warn users if they observe an assertion they disagree with posted onchain. However, suppose a node is configured as a validator and has deposited a bond to the [`Rollup` contract](https://github.com/OffchainLabs/nitro-contracts/blob/main/src/rollup/RollupCore.sol). In that case, that validator can post the correct rival assertion to any invalid one it just observed. The validator will also be able to initiate a challenge by posting a challenge bond and other data to the `ChallengeManager`, signaling it is disputing an assertion.

#### Overflow assertions

Given the mandatory delay of one hour between assertions posted onchain, and each assertion is a claim to a specific Arbitrum batch, there could be a very large number of blocks in between assertions. However, a single assertion only supports a maximum of 2^26 Arbitrum blocks since the previous assertion. If this value is overflowed, one or more follow-up overflow assertions needs to be posted to consume the rest of the blocks above the maximum. This overflow assertion will not be subject to the mandatory one-hour delay between assertions.

#### Trustless bonding pools

A large upfront assertion bond is critical for discouraging malicious actors from attacking Arbitrum and spamming the network (e.g., delay attacks), especially because malicious actors will always lose challenges and their entire bond. On the other hand, requiring such a high upfront assertion bond may be prohibitive for a single honest entity to put up—especially since the cost to defend Arbitrum is proportional to the number of malicious entities and ongoing challenges at any given point in time.

To address this, there is a [contract](https://github.com/OffchainLabs/bold/blob/main/contracts/src/assertionStakingPool/AssertionStakingPoolCreator.sol) that anyone can use to deploy a <a data-quicklook-from="trustless">trustless</a>, bonding (or staking) pool as a way of crowdsourcing funds from others who wish to help defend Arbitrum, but who may otherwise not individually be able to put up the sizeable upfront bond itself.

Anyone can deploy an assertion bonding pool using the `AssertionStakingPoolCreator.sol` contract as a means to crowdsource funds for bonding funds to an assertion. To defend Arbitrum using one of these pools, an entity would first deploy this pool with the assertion they believe is correct and wish to bond on to challenge an adversary's assertion. Then, anyone can verify that the claimed assertion is correct by running the inputs through their node's <a data-quicklook-from="state-transition-function">State Transition Function</a> (STF). If other parties agree that the assertion is correct, they can deposit their funds into the contract. When enough funds have been deposited, anyone can trigger the creation of the assertion onchain to start the challenge in a trustless manner. Finally, once the dispute protocol confirms the honest parties' assertion, all involved entities will get their funds reimbursed and can withdraw.

Note that with bonding pools, there is no minimum `WETH` requirement and once the entire bond amount is raised (either 3600, 555, or 79 `ETH` for Arbitrum One), then the assertion can be posted by anyone trustlessly. Additionally, there is an optional feature in the Nitro node validator software that enables both the automatic deployment of a bonding pool contract and depositing of funds to challenge an observed invalid assertion.

Trustless bonding pools can also be created to open challenges
and make moves on challenges without sacrificing decentralization.

### Opening challenges

To initiate a challenge, there must first be a fork in the assertion chain within the Arbitrum Rollup contracts. However, a challenge's actual start involves creating an edge claim and posting it to the `ChallengeManager` contract on the parent chain.

If there is a fork in the assertion chain (a "top-level" dispute), anyone can begin a challenge by creating a challenge edge. No bond is needed: each branch of the fork has a unique assertion, with an attached bond, and that assertion contains enough information to uniquely determine its challenge edge; that is, there is no "wiggle room" that an adversary can use to create multiple challenge edges from a single assertion (if this were possible, it could lead to spam attacks on the protocol).

Anyone can open a sub-challenge on an assertion without needing to be a bonder in the Rollup contract, so long as they post a challenge bond and an edge claiming intent to start the challenge. This challenge bond is much lower than the one required to become an assertion proposer. Challenges are not tied to specific addresses or parties - instead, anyone can participate.

Recall that a challenge is a fundamental disagreement about an assertion posted to the Arbitrum chain. At its core, validators essentially disagree about the blockhash at a certain block number, and the BoLD protocol allows them to interactively narrow down their disagreement via fraud proofs such that Ethereum can be the final referee and claim a winner.

At its core, the disagreement between validators looks something like this:

- Common parent assertion: `batch 5, blockhash 0xabc`

- Alice’s assertion: `batch 10, blockhash 0x123`

- Bob’s assertion: `batch 10, blockhash 0x456`

Their disagreement is about an Arbitrum block somewhere between batch 5 and batch 10. Here’s how the actual challenge begins in this example:

Validators have to fetch all blocks between batch 5 and batch 10 and create a Merkle commitment out of them as a Merkle tree with 2^26+1 leaves. If there are fewer than 2^26 blocks in between the assertions, the last block is repeated to pad the leaves of the tree to that value. Validators then create an “edge” data structure, which contains the following fields:

- **start_hash:** the start_hash of the claimed assertion and is also the end_hash of the previous assertion

- **end_hash:** the end hash of the last block in the child assertion that a validator claims is correct.

- **merkle_root:** the Merkle root that results from committing to a Merkle tree from the start block hash to the end block hash

- **inclusion_proofs:** Merkle proofs that the end hashes are indeed leaves of the Merkle tree committing to a root

The concept of a history commitment is at the core of challenges and BoLD itself.

The validators above provide a Merkle proof of their commitment to some history. In this case, all the Arbitrum block hashes from batch 5 to batch 10. Using this tree, validators can narrow their disagreement to a single block using Merkle proofs by iteratively bisecting the Merkle root and creating edges which have their own history commitments to each half of the tree.

### Challenge resolution

The fundamental unit in a challenge is an edge data structure.

#### Initiation

The first validator to create an edge initiates a challenge. The smart contracts validate the Merkle inclusion proofs and hashes provided to prove this challenge is about a specific fork in the assertion chain in the Rollup contract.

#### Bisections

When an edge is created, it claims some history from point A to B, with which validators can agree or disagree. Other validators can claim some history from point A to B’, where B’ is a different end state. A history commitment is a Merkle commitment to a list of hashes.

To narrow down a disagreement, validators have to figure out what exact hash they disagree with. To do this, the game essentially takes turns between validators playing binary search. Each move here is known as a “bisection” because each move splits a history commitment in half.

For instance:

Alice commits to 33 hashes with start = A, end = B

Bob commits to 33 hashes with start = A, end = B’

Either of them can perform a “bisection” move on their edge. For instance, if Alice “bisects” her edge E, the bisection <a data-quicklook-from="transaction">transaction</a> will produce two children, E_1 and E_2. E_1 commits to 17 hashes from height A to B/2, and E_2 commits to 17 hashes from height B/2 to B.

A validator can make a bisection move on an edge as long as that edge is “rivaled”, meaning that there is another edge with a conflicting claim.

#### Sub-challenges

The number of steps of execution at which validators could disagree within a single Arbitrum block has a max of 2^42. To play a game of bisections on this amount of hashes would be unreasonable from a space requirement, as each history commitment would require 4.35Tb worth of hashes. Instead, BoLD plays the bisection game over different levels of granularity of this space of 2^42 hashes that we call sub-challenges that can be viewed as recursive execution of the dispute resolution process.

As a reminder, the bisection game is an iterative and interactive process. The first sub-challenge is at the block level and is where validators disagree over Arbitrum blocks between two assertions. The disagreeing validators create “edges” containing history commitments to all the blocks in between those two assertions, which is a max of 2^26 child chain blocks, and commence the bisection game. As they progressively narrow down to a single block of disagreement, the validators then begin the next phase of the challenge process by opening a sub-challenge over up to 2^19 **BigSteps**, which are each 2^23 steps of WASM execution. Once they reach a single disagreement at the BigStep level, they open a final sub-challenge over a maximum of 2^23 SmallSteps, which are each a single step of WASM execution. The bisection game is the same at each sub-challenge level, and opening a sub-challenge requires placing another “challenge bond”. The magnitudes of challenge bonds are different at each sub-challenge level.

#### One step proof

Once validators reach a single step of disagreement at the deepest sub-challenge level, they need to provide something called a **<a data-quicklook-from="one-step-proof">One Step Proof</a>**, or OSP for short. This is a proof of WASM execution showing that executing the Arbitrum state transition function at machine hash A leads to machine hash B. The parent chain, like Ethereum is for Arbitrum One, then actually runs a WASM emulator using a smart contract for this step and will declare a winner. An evil party cannot forge a one-step proof, and unless there is a critical bug in the smart contract, the honest party will always win. At this point, the honest party’s one-step proven edge will be confirmed, and the evil party has no more moves to make. Next, the honest party’s “branch” of edges all the way from the top to the one-step proven edge will have an ever-increasing timer until the top edge is confirmed by time.

#### Timers

Once a validator creates an edge, and if it does not have any rival edge contesting it, that edge will have a timer that ticks up called its **unrivaled timer**. Time in the protocol is measured in blocks of the first non-Arbitrum ancestor chain (i.e., Ethereum blocks for L2 chains and L3 chains settling to an Arbitrum chain, and parent chain blocks for chains settling to a non-Arbitrum L2 chain), and block numbers are used. An edge's timer stops ticking when a rival edge is created onchain.

Edges also have an **inherited timer**, which is the sum of its unrivaled timer + the minimum inherited timer of an edge's children (recursive definition). Once one of the top-level edges that initiated a challenge has achieved an inherited `timer >= a CHALLENGE_PERIOD (6.4 days)``, it can be confirmed. At this point, its assertion can also be confirmed as its associated challenge has completed. A minor but important detail is that edges also inherit the time their claimed assertion was unrivaled.

Feel free to read the [BoLD whitepaper](https://arxiv.org/abs/2404.10491) for more details around how timers are tracked.

#### Cached timer updates

An edge's "inherited timer" value exists onchain and can be updated via a transaction. Given it is a recursive definition, it can be updated via multiple transactions. First, the lowermost edges have their timers updated, then their parents, etc., up to the top. Validators can track information locally to avoid sending wasteful transactions and only propagate updates once they are confident their edge is confirmable by time.

#### Confirmation

Once an edge has a total onchain timer greater than or equal to a challenge period, it can be confirmed via a transaction. Not all edges need to be confirmed onchain, as simply the top-level block challenge edge is enough to confirm the claimed assertion and resolve a dispute. A challenge is not complete at the one-step proof. It is only complete once the claimed assertion of a challenge is confirmed by time.

### Bonding in challenges

To create a challenge, there must be a fork in the Arbitrum assertion chain smart contract. A validator that wishes to initiate a challenge must then post an “edge” claiming a history of block hashes from the previous assertion to the claimed assertion they believe is correct. To do so, they need to put up some value called a "challenge bond". Note that to open a new assertion-level challenge, no challenge bond needs to be posted. This is because top-level assertions already contain enough information to uniquely determine their corresponding challenge edge (which contain a hash of this history), and have already been bonded on.

Challenge bonds are named as such because they are bonds required
for opening challenges. The mechanism of how challenge bond economics are decided is contained in the
[Economics of Disputes](../../how-arbitrum-works/bold/bold-economics-of-disputes.mdx), which also explains
the cost profile and spam prevention in BoLD. In short, the actual cost of a bond encompasses many costs
associated with participating in the dispute game. More information on the bond sizes and how they were
calculated can be found in the [Economics of Disputes](../../how-arbitrum-works/bold/bold-economics-of-disputes.mdx)
document mentioned above.

Each sub-challenge that is created requires depositing a challenge bond. For Arbitrum One, the first unrivaled edge’s bond is kept in the challenge manager contract on Ethereum, while any subsequent rival bonds are kept in an excess bond receiver address. Once a challenge is complete, all bonds for an honest party are automatically refunded in-protocol while all confiscated bonds are sent to the ArbitrumDAO treasury. It is important to not offer the majority of the bonds confiscated from dishonest parties to honest parties to avoid perverse incentives, such as grieving attacks in self-challenges or to discourage needless competition between honest parties.

### Reimbursements of bonds

The reimbursement of assertion bonds and challenge bonds for honest parties will be handled “in-band” by the protocol. Please see [Economics of Disputes](../../how-arbitrum-works/bold/bold-economics-of-disputes.mdx) for more information about this topic.

### Upgrade mechanism

For BoLD to be deployed on an Arbitrum chain, an upgrade admin action needs to be taken using an `UpgradeExecutor` pattern. This is a smart contract that executes actions as the rollup owner. At the upgrade, the `RollupCore.sol` contract will be updated to a new BoLD one, and additional contracts needed for BoLD challenges, such as an `EdgeChallengeManager.sol`, will also be deployed to the parent chain.

Next, assertions will then be posted to the new Rollup contract. During the upgrade period, there could have been a very large number of blocks posted in Arbitrum batches. For this purpose, BoLD assertions support the concept of an **overflow**, allowing us to efficiently handle this situation.

:::caution Withdrawals leading up to a BoLD upgrade

The confirmation timing on any withdrawal that is in-flight when the BoLD upgrade is activated will be delayed until the first BoLD assertion is confirmed. This means that for any Arbitrum chain that upgrades to use BoLD, including Arbitrum One and Arbitrum Nova, all pending withdrawals to the parent chain, Ethereum, that were initiated _before_ the upgrade will be delayed by one challenge period, plus the time between the withdrawal was initiated and the time that the BoLD upgrade takes place. This is because the upgrade effectively "resets" the challenge period for that are not yet finalized.

For example, if the upgrade happened at time _t_, then a withdrawal initiated at a time _t-2_ days will need to wait an additional _6.4_ days for their withdrawal to be finalized, totaling 8.4 days of maximum delay. Withdrawals that finalize before the upgrade takes place at time _t_ will be unaffected. In other words, the maximum delay a withdrawal will experience leading up to the upgrade is 12.8 days (two challenge periods).
:::

The upgrade pattern for an existing Arbitrum Rollup to a BoLD-enabled one is tested extensively and run as part of each of our pull requests in the BoLD repository [upgrade workflow on GitHub](https://github.com/OffchainLabs/bold/blob/c4e068b568ff662f49ed191c5c3188ea7b6138b2/.github/workflows/go.yml#L209).

---

## .mdx (bold/gentle-introduction.mdx)
---
title: 'A gentle introduction: BoLD'
sidebar_label: 'A gentle introduction'
description: 'An educational introduction that provides a high-level understanding of BoLD, a new dispute protocol to enable permissionless validation for Arbitrum chains.'
user_story: 'As a user or researcher of the Arbitrum product suite, I want to learn about BoLD, a next-generation dispute protocol that enables permissionless validation.'
content_type: gentle-introduction
author: leeederek
sme: leeederek
---

import ImageZoom from '@site/src/components/ImageZoom';

This introduction is for those who want to learn about <a data-quicklook-from="bold">BoLD</a>: a new dispute protocol for optimistic rollups that enables **permissionless validation for <a data-quicklook-from="arbitrum">Arbitrum</a> chains**. BoLD stands for Bounded Liquidity Delay and is currently deployed on a <a data-quicklook-from="arbitrum-one">Arbitrum One</a>, <a data-quicklook-from="arbitrum-nova">Arbitrum Nova</a>, and Arbitrum Sepolia.

This next-generation dispute protocol technology is now available for any <a data-quicklook-from="arbitrum-chain">Arbitrum chain</a> to upgrade to and is live in production on Arbitrum One, Nova, and Arbitrum Sepolia.

BoLD replaces the previous, permissioned <a data-quicklook-from="fraud-proof">fraud proof</a> protocol for Arbitrum One and Arbitrum Nova, as well as for any Arbitrum chain (who wishes to adopt BoLD).

## In a nutshell:

- Validation for Arbitrum One and Arbitrum Nova is a privileged action currently limited to an [allow-listed set of parties, maintained by the Arbitrum DAO](https://docs.arbitrum.foundation/state-of-progressive-decentralization#allowlisted-validators) to reduce the risks of _[delay attacks](https://medium.com/offchainlabs/solutions-to-delay-attacks-on-rollups-434f9d05a07a)_. _Delay attacks_ are a class of attacks where malicious entities can open as many disputes as they are willing to forfeit bonds during the <a data-quicklook-from="challenge">challenge</a> period to delay confirmations of assertions (equal to the time needed to resolve those disputes one by one).
- BoLD, an acronym for Bounded Liquidity Delay, is a new challenge resolution protocol for Arbitrum chains that enables permissionless validation by mitigating the risks of delay attacks against [Optimistic rollups like Arbitrum](/how-arbitrum-works/05-validation-and-proving/02-rollup-protocol.mdx). This is possible because BoLD's design ensures disputes will be resolved within a fixed time window, currently set to equal one <a data-quicklook-from="challenge-period">challenge period</a> (~6.4 days) for Arbitrum One and Arbitrum Nova. If there is a dispute, BoLD guarantees the maximum total time to be equal to two challenge periods (one for raising disputes, one for resolving disputes), a two day grace period for the Security Council to intervene if necessary, and a small delta for computing challenges.
- Enabling permissionless validation is key milestone on [Arbitrum’s journey to becoming a Stage 2 Rollup](https://docs.arbitrum.foundation/state-of-progressive-decentralization) - the most advanced and mature rollup technology categorization, according to [L2Beat](https://medium.com/l2beat/introducing-stages-a-framework-to-evaluate-rollups-maturity-d290bb22befe). With BoLD, **any honest party can validate and bond their funds to post a correct child chain state assertions to win disputes against malicious entities.**

## What _exactly_ is BoLD?

BoLD, is an upgrade to Arbitrum's existing dispute protocol. Specifically, BoLD changes some of the rules used by validators to open and resolve disputes about Arbitrum’s state to ensure only valid states get confirmed on an Arbitrum chain’s <a data-quicklook-from="parent-chain">parent chain</a>, such as Ethereum.

The current dispute protocol has working fraud proofs and is used in production today by Arbitrum chains. The changes BoLD brings enable anyone to participate in the validation of the state of the chain and enhance security around all child to parent chain messages (including withdrawals).

Under BoLD, a bonded validator’s responsibilities are to:

- Post claims about an Arbitrum chain’s state to its parent chain (for Arbitrum One, the parent chain is Ethereum),
- Open challenges to dispute invalid claims made by other validators, and
- Confirm valid claims by participating in and winning challenges.

The goal of BoLD is to unlock permissionless validation by ensuring that disputes are resolved within a fixed period (currently equivalent to two challenge periods, plus a two-day grace period for the Security Council to intervene if necessary and a small delta for computation), effectively removing the risk of delay attacks and making withdrawals to a parent chain more secure. BoLD accomplishes this by introducing a new dispute system that lets any single entity defend Arbitrum against malicious parties - effectively allowing anyone to validate, propose, and defend an Arbitrum chain’s state without needing permission to do so.

## Why does Arbitrum need a new dispute protocol?

While Arbitrum chains today benefit from working fraud proofs, BoLD introduces a few subtle but innovative changes that let _anyone_ challenge and win disputes - all within a fixed time period. In other words, Arbitrum chains will continue to be secured with an interactive proving game between validators using fraud proofs, but with the added benefit of this game being completely permissionless and time-bounded to the same length as one challenge period (or 6.4 days, by default).

Under the hood, the reason why BoLD can offer time-bound, permissionless validation is because a correct Arbitrum state <a data-quicklook-from="assertion">assertion</a> is **not tied to the entity that bonds their capital to a claim**. This property, coupled with the fact that the <a data-quicklook-from="child-chain">child chain</a> states are completely deterministic and can be proven on Ethereum, means that **any number of honest parties** can rely on BoLD to prove that their claim is correct. Lastly, a property that will not change with BoLD is the fact that there needs to only be one honest party defending Arbitrum.

### BoLD brings Arbitrum closer to being recognized as a Stage 2 rollup

Inspired by [Vitalik’s proposed milestones](https://ethereum-magicians.org/t/proposed-milestones-for-rollups-taking-off-training-wheels/11571), the team over at L2Beat has assembled a widely recognized framework for evaluating the development Ethereum Rollups. Both Vitalik and the [L2Beat framework](https://medium.com/l2beat/introducing-stages-a-framework-to-evaluate-rollups-maturity-d290bb22befe) refer to the final stage of rollup development as _“Stage 2 - No Training Wheels”_. A critical criterion for being considered a Stage 2 rollup is to allow anyone to validate the child <a data-quicklook-from="chain-state">chain state</a> and post fraud proofs to Ethereum without restraints. This is considered a key requirement for Stage 2 because it ensures _[“that the system is not controlled by a limited set of entities and instead is subject to the collective scrutiny of the entire community”](https://medium.com/l2beat/introducing-stages-a-framework-to-evaluate-rollups-maturity-d290bb22befe)._

BoLD enables permissionless validation by allowing _anyone_ to challenge incorrect Arbitrum state assertions and therefore unlocks new avenues for participation in securing the network, fostering greater inclusivity and resilience. This is made possible because BoLD guarantees that a single, honest entity who has their capital bonded to the correct Arbitrum state assertion will always win against malicious adversaries. The research and work to bring BoLD to life underscores Arbitrum's commitment to scaling Ethereum without compromising on security.

<ImageZoom src="/img/bold-l2beat-pie-chart.png" alt="Pie slice" className="img-600px" />

With BoLD at its core, Arbitrum charts a course towards being recognized as a Stage 2 rollup by addressing the currently yellow (above) State Validation wedge in [L2Beat's risk analysis pie chart](https://l2beat.com/scaling/summary). BoLD contributes to a more permissionless, efficient, and robust rollup ecosystem. Additionally, BoLD will be available as an upgrade for all Arbitrum chains who wish to adopt it to reap the aforementioned benefits.

### BoLD makes withdrawals to parent chain Ethereum safer

Today, there is a period of time, following a state assertion, called the “challenge period,” where any validator can open a dispute over the validity of a given the child chain state root. If there are no disputes during the challenge period, the protocol confirms the state root and considers it to be valid - this property is what makes Arbitrum an optimistic rollup. This challenge period is why you must wait ~1 week (6.4 days to be exact) to withdraw assets from Arbitrum One, for example. While this design is secured with working fraud proofs, it is susceptible to [delay attacks](https://medium.com/offchainlabs/solutions-to-delay-attacks-on-rollups-434f9d05a07a), where malicious actors continuously open disputes to extend that challenge period for as long as they’re willing to sacrifice bonds - effectively extending the challenge period indefinitely by an amount equal to the time it takes to resolve each dispute, one by one. This risk is not ideal nor safe, and is why validation for Arbitrum One and Nova is confined to a permissioned set of entities overseen by the Arbitrum DAO.

<ImageZoom
  src="/img/bold-safer-withdrawals-with-bold-2.png"
  alt="BoLD safer withdrawals"
  className="img-600px"
/>

BoLD addresses these challenges head-on by introducing a time limit on the existing rollup protocol for resolving disputes, effectively ensuring that challenges conclude within a 6.4-day window (this window can changed by the DAO for Arbitrum One and Nova). This is possible due to two reasons: (1) BoLD’s design allows for challenges between the honest party and any number of malicious adversaries to happen in parallel, and (2) the use of a time limit that will automatically confirm the honest party’s claims if the challenger fails to respond.

To summarize with an analogy and the diagram below: Arbitrum’s current dispute protocol assumes that any assertion that gets challenged must be defended against each unique challenger sequentially, like in a _“1v1 tournament”_. BoLD, on the other hand, enables any single honest party to defend the correct state and be guaranteed to win, similar to an _“all-vs-all battle royale”_ where there must and will always be a single winner in the end.

<ImageZoom
  src="/img/bold-before-vs-after-with-bold.png"
  alt="Before and after with BoLD"
  className="img-600px"
/>

:::note

The timer/clocks above are arbitrary and instead represent the duration of challenges and how challenges are sequential today but can take place in parallel with BoLD. The duration of challenges are independent from one another.

:::

## How is this possible?

The BoLD protocol provides the guardrails and rules for how validators challenge claims about the state of an Arbitrum chain. Since Arbitrum’s state is deterministic, there will always be only one correct state for a given input of onchain operations and transactions. The beauty of BoLD’s design guarantees that disputes will be resolved within a fixed time window, removing the risk of delay attacks and ultimately enabling anyone to bond their funds to and successfully defend that singular correct state of Arbitrum.

Let’s dive in to an overview of how BoLD actually works.

1. **An assertion is made:** Validators begin by taking the most recent confirmed [assertion](/how-arbitrum-works/05-validation-and-proving/02-rollup-protocol.mdx), called `Block A`, and assert that some number of transactions afterward, using Nitro’s deterministic <a data-quicklook-from="state-transition-function">State Transition Function</a> (STF), will result in an end state, `Block Z`. If a validator claims that the end state represented by `Block Z` is correct, they will bond their funds to `Block Z` and propose that state to it's parent chain. (For more details on how bonding works, see [Bold technical deep dive](/how-arbitrum-works/bold/bold-technical-deep-dive.mdx)). If nobody disagrees after a certain amount of time, known as the challenge period, then the state represented by the assertion `Block Z` is confirmed as the correct state of an Arbitrum chain. However, if someone disagrees with the end state `Block Z`, they can submit a challenge. This is where BoLD comes into play.
2. **A challenge is opened:** When another validator observes and disagrees with the end state represented by `Block Z`, they can permissionlessly open a challenge by asserting and bonding capital to a claim on a different end state, represented by an assertion `Block Y`. At this point in time, there are now two asserted states: `Block A → Block Z` and `Block A → Block Y`. Each of these asserted states, at this point in time now that there's a challenge, are referred to _edges_ while a Merkle tree of asserted states from some start to endpoint (e.g., `Block A → Block Z`) is more formally known as a _history commitment._ It is important to note that Ethereum at this point in time has no notion of which edge(s) is correct or incorrect - edges are simply a portion of a claim made by a validator about the history of the chain from some end state all the way back to some initial state. Also note that because a bond put up by a validator is tied to an assertion rather than the party who put up that bond, there can be any number of honest, anonymous parties that can open challenges against incorrect claims. It is important to note that the bonds put up to open challenges are held in the rollup contract. There is a prescribed procedure for what the Arbitrum Foundation is expected to do with these funds; see Step 5 below for a summary.
3. **Multi-level, interactive <a data-quicklook-from="dissection">dissection</a> begins:** To resolve the dispute, the disagreeing entities will need to come to an agreement on what the _actual, correct_ asserted state should be. [It would be tremendously expensive to re-execute](/how-arbitrum-works/05-validation-and-proving/03-proving-and-challenges.mdx) and compare everything from `Block A → Block Z` and `Block A → Block Y`, especially since there could be potentially millions of transactions in between `A`, `Z`, and `Y`. Instead, entities take turns bisecting their respective _history commitments_ until they arrive at a single step of instruction where an arbiter, like Ethereum, can declare a winner. Note that [this system is very similar to how challenges are resolved on Arbitrum chains today](/how-arbitrum-works/05-validation-and-proving/01-validation-and-proving.mdx) - BoLD only changes some minor, but important, details in the resolution process. Let’s dive into what happens next:
   - **Block challenges**: When a challenge is opened, the edges are called _level-zero edges_ since they are at the granularity of Arbitrum blocks. The disputing parties take turns bisecting their history commitments until they identify the specific block that they disagree on.
   - **Big-step challenge:** Now that the parties have narrowed down their dispute to a single block, the back-and-forth bisection exercise continues within that block. Note that this block is agreed by all parties to be some state after the initial state, but before the final states. This time, however, the parties will narrow down on a specific _range_ of instructions for the State Transition Function within the block - essentially working towards identifying a set of instructions within which their disagreement lies. This range is currently defined as 2^20 steps of `WASM` instructions, which is the assembly of choice for validating Arbitrum chains.
   - **One-step challenge:** Within that range of 2^20 instructions, the back-and-forth bisecting continues until all parties arrive at a single step of instruction that they disagree on. At this point in time, parties agree on the initial state of Arbitrum before the step but disagree on the end state one step immediately after. Remember that since Arbitrum’s state is entirely deterministic, there is only one correct end state.
4. **One-step proof:** Once a challenge is isolated down to a dispute about a single step, both parties run that step to produce, and then submit, a one-step proof to the OneStepProof <a data-quicklook-from="smart-contract">smart contract</a> on the parent chain (e.g., Ethereum). A one-step proof is a proof that a single step of computation results in a particular state. The smart contract on the parent chain will execute the disputed step to validate the correctness of a submitted proof from the two parties. It is at this point that the honest party's proof will be deemed valid and its tree of edges will be confirmable by time, while the dishonest party will have their edges rejected by timeout.
5. **<a data-quicklook-from="confirmation">Confirmation</a>:** Once the honest one-step edge is confirmed, the protocol will work on confirming or rejecting the parent edges until it reaches the level-zero edge of the honest party. With the honest party’s level-zero edge now confirmed, the honest party’s assertion bond can be withdrawn. Meanwhile, the dishonest party has their bonds taken away to ensure the dishonest party is always punished.
   - There is another way that a level-zero edge can get confirmed: time. At each of the mini-stages of the challenge (block challenge, big-step challenge, one-step challenge), a timer increments upwards towards some challenge period, _T_ defined by BoLD. This timer begins ticking for a party when they submit their bisected history commitment until their challenger submits their bisected history commitment in response. An edge is automatically confirmed if the timer reaches _T._
6. Reimbursements for the honest party's parent chain gas costs and mini-bonds made at the other challenge levels are handled by the Arbitrum Foundation.

That’s it! We’ve now walked through each of the steps that validators will take to dispute challenges with the BoLD protocol. One final note here is that each of the steps explained above can take place concurrently and this is one of the reasons why BoLD can guarantee that disputes are resolved within a fixed time frame.

## Frequently asked questions about BoLD (FAQ):

#### Q: How does bonding work?

The entities responsible for posting assertions about Arbitrum state to Ethereum are called validators. If posting assertions were free, anyone could create conflicting assertions to always delay withdrawals by 14 days instead of seven. As such, Arbitrum requires validators to put in a “security deposit”, known as a bond, to be allowed to post assertions. Validators can withdraw their bond as soon as their latest posted assertion has been confirmed, and end their responsibilities. These bonds can be any `ERC-20` token and should be set to a large enough value (e.g., 200 `WETH`) to make it economically infeasible for an adversary to attack an Arbitrum chain and to mitigate against spam (that would otherwise delay confirmations). Requiring a high bond to post assertions about Arbitrum seems centralizing, as we are replacing an allowlist of validators with instead a system that requires a lot of money to participate in. To address this, there is a [contract](https://github.com/OffchainLabs/BoLD/blob/main/contracts/src/assertionStakingPool/AssertionStakingPoolCreator.sol) that anyone can use to deploy a bonding pool as a way of crowdsourcing funds from others who wish to help defend Arbitrum but who may not individually be able to put up the large upfront bond itself. The use of bonding pools, coupled with the fact that there can be any number of honest anonymous parties ready to defend Arbitrum, means that these high bond values do not harm decentralization.

#### Q: Why are the bond sizes so high for Arbitrum One?

There are two types of “bonds” in BoLD: **assertion and challenge.** The below sizes are carefully calculated and set for Arbitrum One using a variety of factors, including TVL and optimizing for a balance between cost for honest parties and security of the protocol. As always, the exact bond sizes for an Arbitrum chain using BoLD is entirely up to the <a data-quicklook-from="chain-owner">chain owner</a> to decide, if they choose to adopt BoLD at all.

**Assertion bond sizes**

Assertion bond sizes can be thought of as a “security deposit” that an entity puts down to fulfill the role of a proposer (i.e., a validator who proposes state assertions to the parent chain). The bond sizes are high because the role of a proposer assumes a big responsibility - their role is to ensure that the chain progresses. Accordingly, the bond also acts as a deterrence to _delay attacks_, where the attacker would sacrifice the bond in order to cause roughly a week of delay in a group of withdrawals. If the bond is too small or free, there may not be enough deterrence against this type of attack. Validators who choose to be proposers can withdraw their bond as soon as their most recent posted assertion has been confirmed by the protocol. We expect there to be very few proposers for Arbitrum One as only one is sufficient for safety and full functioning of the chain.

**Challenge bond sizes**

If someone disagrees with a posted assertion from a proposer, they can pool funds together to propose their own assertion that represents the correct history of the chain. Upon doing so, a challenge between the two claims will begin. Anyone can participate in the challenge, as it is not tied up to specific addresses. To resolve a challenge, participants will incur compute and gas costs due to the [interactive fraud proof game](/how-arbitrum-works/05-validation-and-proving/03-proving-and-challenges.mdx), and certain moves within a challenge have an additional bond required to prevent resource exhaustion and spam from adversaries. These moves within a challenge require smaller, **challenge bonds**. The proposed challenge bonds for Arbitrum One are 1110 `ETH` to fully resolve a dispute, which will also get reimbursed upon the confirmation of assertions by the protocol.

The rationale behind the specific challenge bond size was made using something called a “resource ratio” - defined as the cost ratio between an adversary and an honest party when participating in the interactive fraud proof game. The value was chosen to ensure that the malicious party will pay 10x the marginal costs of the honest party. This resource ratio, coupled with the fact that an honest party will always get their bonds refunded while a malicious party loses everything, helps prevent and deter attacks to begin with.

To summarize with a scenario, this effectively means that defending against a \$1B dollar attack would require ~\$100M of bonds. The ~\$100M would be reimbursed upon winning a challenge, where the \$1B put up by an adversary would be lost. The proposal aims to send the confiscated funds to the treasury by setting the “excess state receiver” address to the [DAO’s treasury address](https://arbiscan.io/address/0xF3FC178157fb3c87548bAA86F9d24BA38E649B58). The tradeoff here is that the higher the resource ratio we want, the more expensive it is for both honest and evil parties to make claims in disputes.

**Bonding pools as a way to allow people to participate in assertion posting**

BoLD ships with <a data-quicklook-from="trustless">trustless</a> bonding pools that allow any group of participants to pool their funds together to challenge a dishonest proposer, and win. That is, any group of entities can pool funds into a simple contract that will post an assertion to Ethereum without needing to trust each other. Upon observation of an invalid assertion, validators have one challenge period (~6.4 days) to pool funds in the contract and respond with a counter assertion. We believe that making it easy to pool the funds to participate in the defense of Arbitrum trustlessly and improves decentralization and the safety of BoLD.

#### Q: Does the bond requirement only mean that whales can validate Arbitrum One?

Validating Arbitrum One is **free and accessible**. All Arbitrum One nodes, by default, are watchtower validators meaning they can detect and report invalid assertions posted to Ethereum.

However, becoming an assertion proposer requires a bond, as without it, anyone could delay all Arbitrum bridged assets by one week. However, BoLD allows for anyone to propose assertions and also challenge invalid assertions via pool contracts, helping keep proposers accountable for their actions.

#### Q: How does BoLD disincentivize malicious actors from attacking an Arbitrum chain?

Bonds put up by honest parties will always be refunded while malicious actors always stand to lose 100% of their bond. Malicious actors stand to lose everything at each challenge. BoLD delay is bounded and additional challenges would not increase the delay of a particular assertion.

#### Q: In the event of a challenge, what happens to the confiscated funds from malicious actors for Arbitrum One?

Recall that BoLD enables any validator to put up a bond to propose assertions about the child chain state. These assertions about the child chain state are deterministic and so an honest party who puts up a bond on the correct assertion will always win in disputes. In these scenarios, the honest party will eventually have their bonds reimbursed while the malicious actor will lose all of their funds.

In BoLD, all costs spent by malicious actors are confiscated and sent to the Arbitrum DAO treasury. A small reward, called the Defender's Bounty, of 1% will be awarded to entities who put down challenge bonds in defense of Arbitrum One. For the remainder of the funds, the Arbitrum DAO will have full discretion over what to do with the funds confiscated from a malicious actor. This includes, but is not limited to:

- Using the confiscated funds to refund the parent chain gas costs to honest parties,
- Rewarding or reimbursing the honest parties with some, or all, of the confiscated funds in excess of the 1% Defender's Bounty,
- Burning some, or all, of the confiscated funds, or
- Keep some, or all, of the confiscated funds within the Arbitrum DAO Treasury

As always, an Arbitrum chain can choose how they wish to structure and manage confiscated funds from dishonest parties.

#### Q: Why are honest parties not automatically rewarded with confiscated funds from a malicious actor?

It’s tempting to think that rewarding the honest proposer in a dispute can only make the protocol stronger, but this turns out not to be true, because an adversary can sometimes profit by placing the honest bonds themselves.

This creates perverse incentives that threaten the security of BoLD. Here’s an example, from Ed Felten:

<aside>
💡 *Suppose the top-level assertion bond is \$5M. A delay attack, where the attacker wants to cause one week of delay at minimum cost, costs the attacker \$5M. If we change the protocol to give the honest proposer 20% of the confiscated bond, then the attack only costs \$4M, because the attacker can post both bonds (\$10M total) and get back the honest \$5M bond, plus a \$1M reward. So, the protocol is weaker against delayed griefing. We can compensate by increasing the top-level assertion bond to \$6.25M, so delay griefing still costs \$5M, but that increases the cost, to an honest party, required to defend against other attacks. The intuition that giving bigger rewards for honest actions can only make the protocol stronger, though natural, turns out not to be correct. The reason for this is that large rewards create new strategic options for the attacker which might enable them to reduce their cost.*

</aside>

That said, there’s no harm in paying the honest proposer a fair interest rate on their bond, so they don’t suffer for having helped the protocol by locking up their capital in a bond.

Therefore, the BoLD AIP proposes that the honest parties be rewarded 1% of confiscated bonds from a dishonest party, in the event of a challenge. This reward applies only to entities who deposit challenge bonds and participate in defending Arbitrum against a challenge. The exact amount rewarded to honest parties will be proportional to the amount defender’s deposited into the protocol during a challenge, making bonding pool participants eligible. The process by which this reward is calculated will be done offchain and payouts will require a DAO vote because the confiscated funds are always sent to a DAO-controlled address.

#### Q: Why is `ARB` not the bonding token used in BoLD? on Arbitrum One?

Although BoLD supports using an `ERC-20` token, Ethereum, specifically `WETH`, was chosen over `ARB` for a few reasons:

1. **Arbitrum One & Arbitrum Nova both inherit their security from Ethereum already,** Arbitrum One and Nova rely on Ethereum for both data availability and as the referee for determining winners during fraud-proof disputes. It follows then that Ethereum continues to be used in BoLD, which is meant to permissionlessly secure Arbitrum even further. Ethereum’s value is also relatively independent of Arbitrum, especially when compared to `ARB`.
2. Adversaries might be able to exploit the potential instability of `ARB` when trying to win challenges. Suppose an adversary deposits their bond in `ARB`, and can create an impression that they have a nontrivial chance of winning the challenge. This might drive down the value of `ARB`, which would decrease the adversary's cost to create more bonds (i.e., more spam) during the challenge, which in turn could increase the adversary's chances of winning (which would drive `ARB` lower, making the attack cheaper still for the adversary, etc.)
3. **Access to liquidity**: Ethereum has greater liquidity than `ARB`. In the event of an attack on Arbitrum, access and ease of pooling funds may become crucial.
4. **Fraud proofs are submitted to, and arbitrated on the parent chain (Ethereum).** The bonding of capital to make assertions is done so on the parent chain (Ethereum), since Ethereum is the arbitrator of disputes. If BoLD were to use `ARB` instead of Ethereum, a large amount of `ARB` must be pre-positioned on the parent chain which is more difficult to do when compared to pre-positioning Ethereum on the parent chain (Ethereum).

An Arbitrum chain owner may choose to use any token they wish for bonding, if they adopt and use BoLD permissionless validation.

#### Q: Can the required token for the validator be set to `ARB` and can network `ETH` revenues be distributed for validator incentives for Arbitrum One?

Yes. The asset that a validator uses to become a proposer in BoLD can be set to any `ERC-20` token, including `ARB`. For Arbitrum One, `ETH` is used for bonds for various reasons mentioned above. The Arbitrum DAO can change this asset type at any time via a governance proposal. Should such an economic incentive model exist, the source and denomination of funds used to incentivize validators will be at the discretion of the Arbitrum DAO. Again, though, we don't see `ARB`-based bonding as a good idea at present; see the last question.

#### Q: How are honest parties reimbursed for bonding their capital to help secure Arbitrum One?

The Arbitrum DAO reimburses “active” proposers with a fair interest rate, as a way of removing the disincentive to participate, by reimbursing honest parties who bond their own capital and propose assertions for Arbitrum One. The interest rate should be denominated in `ETH` and should be equal to the annualized yield that Ethereum mainnet validators receive, which at the time of writing, is an APR between 3% to 4% (based on [CoinDesk Indices Composite Ether Staking Rate (CESR)](https://www.coindesk.com/indices/ether/cesr) benchmark and [_Rated.Network_](https://explorer.rated.network/network?network=mainnet&timeWindow=all&rewardsMetric=average&geoDistType=all&hostDistType=all&soloProDist=stake)). This interest is considered a reimbursement because this payment reimburses the honest party for the opportunity cost of locking up their capital and should not be perceived as a “reward” - for the same [reasons why the protocol does not reward honest parties with the funds confiscated from a malicious actor](#q-why-are-honest-parties-not-automatically-rewarded-with-confiscated-funds-from-a-malicious-actor)). These reimbursement payments can be paid out upon an active proposer’s honest assertion being confirmed on Ethereum and will be calculated and handled offchain by the Arbitrum Foundation.

BoLD makes it permissionless for any validator to become a proposer and also introduces a way to pay a service fee to honest parties for locking up capital to do so. Validators are not considered active proposers until they successfully propose an assertion _with_ a bond. In order to become an active proposer for Arbitrum One, post-BoLD, a validator has to propose a child chain state assertion to Ethereum. If they do not have an active bond on the parent chain, they then need to attach a bond to their assertion in order to successfully post the assertion. Subsequent assertions posted by the same address will simply move the already-supplied bond to their latest proposed assertion. Meanwhile, if an entity, say Bob, has posted a successor assertion to one previously made by another entity, Alice, then Bob would be considered by the protocol to be the current active proposer. Alice would no longer be considered by the protocol as the active proposer and once Alice’s assertion is confirmed, then Alice gets her assertion bond refunded. There can only be one “active” proposer at any point in time.

The topic of economic and incentive models for BoLD on Arbitrum One is valuable and we believe it deserves the full focus and attention of the community via a separate proposal/discussion - decoupled from this proposal to bring BoLD to mainnet. Details around proposed economic or incentive models for BoLD will need continued research and development work, but the deployment of BoLD as-is represents a substantial improvement to the security of Arbitrum even without economic-related concerns being resolved. The DAO may choose, via governance, to fund other parties or change this reimbursement model at any time.

For Arbitrum chains, any economic model can be implemented alongside BoLD, if chain owners decide to adopt BoLD.

#### Q: For Arbitrum One proposers, is the service fee applied to the amount bonded? If that’s the case, the `ETH` would be locked and thus unable to be used to generate yield elsewhere. So, which assets are used to generate this yield for the service fee? Would it involve some `ETH` from the Arbitrum bridge?

The proposed service fee should correlate to the annualized income that Ethereum mainnet validators receive, over the same time period. At the time of writing, the estimated annual income for Ethereum mainnet validators is approximately 3% to 4% of their bond (based on [CoinDesk Indices Composite Ether Staking Rate (CESR)](https://www.coindesk.com/indices/ether/cesr) benchmark and [_Rated.Network_](https://explorer.rated.network/network?network=mainnet&timeWindow=all&rewardsMetric=average&geoDistType=all&hostDistType=all&soloProDist=stake)).

The fee is applied to the total amount bonded over the duration of time that a proposer is active. A validator will need to deposit `ETH` into the contracts on the parent chain to become a proposer and so those deposited funds will indeed be unable to be used for yield in other scenarios. The decision on the source of funds for the yield is entirely up to the ArbitrumDAO to decide.

#### Q: For Arbitrum One, will the offchain compute costs be reimbursed? (i.e., the costs for a validator computing the hashes for a challenge)

Reimbursement will not be made for offchain compute costs as we view these to be costs borne by all honest operators, alongside the maintenance and infra costs that regularly arise from running a node.

Our testing has demonstrated that the cost of running a sub-challenge in BoLD, the most computationally-heavy step, on an AWS r5.4xlarge EC2 instance, costs around \$2.50 USD (~\$1 hour for one challenge with 2.5 hour duration) using [on-demand prices for U.S. East (N. Virginia)](https://instances.vantage.sh/aws/ec2/r5.4xlarge). Therefore, the additional costs from offchain compute is assumed to be negligible relative to the regular infra costs of operating a node.

#### Q: How will BoLD impact Arbitrum Nova?

Although this AIP proposes that both Arbitrum One and Nova upgrade to use BoLD, we recommend for the removal of the [allowlist of validators for Arbitrum One while keeping Nova permissioned with a DAO-controlled allowlist of entities](https://docs.arbitrum.foundation/state-of-progressive-decentralization#allowlisted-validators) - unchanged from today.

This decision was made for two reasons. First, Arbitrum Nova’s TVL is much lower than Arbitrum One’s TVL, (~\$17B vs. ~\$46M at the time of writing, from [L2Beat](https://l2beat.com/scaling/summary)). This means that the high bond sizes necessary for preventing spam and delay attacks would make up a significant proportion of Nova’s TVL - which we believe introduces a centralization risk as very few parties would be incentivized to secure Nova. A solution here would be to lower the bond sizes, which brings us to the second reason: lower bond sizes reduce the costs of delay griefing attacks (where malicious actors delay the chain’s progress) and therefore hurt the security of the chain. We believe enabling permissionless validation for Nova is not worth the capital requirement tradeoffs, given the unique security model of AnyTrust chains.

Notably, since Arbitrum Nova's security already depends on at least one DAC member providing honest data availability, trusting the same committee to have at least one member provide honest validation does not add a major trust assumption. This requires all DAC members also to run validators. If the DAC is also validating the chain, a feature the <a data-quicklook-from="offchain-labs">Offchain Labs</a> team has been working on, Fast Withdrawals, would allow users to withdraw assets from Nova in ~15 minutes, or the time it takes to reach parent chain finality. This is made possible by the DAC attesting to and instantly confirming an assertion. Fast Withdrawals will be the subject of future forum post and snapshot vote.

#### Q: When it comes to viewing the upfront assertion bond (to be a proposer) as the security budget for Arbitrum One, is it possible for an attacker to go above the security budget and, if yes, what happens then?

The upfront capital to post assertions (onchain action) is 3600 `ETH`, with subsequent sub-challenge assertions requiring 555/79 `ETH` (per level) - this applies to honest proposers as well as malicious entities. A malicious entity can post multiple invalid top level assertions and/or open multiple challenges and the honest entity can

It is critical to note that Arbitrum state transitions are entirely deterministic. An honest party bonded to the correct state assertion, the honest party will get all their costs refunded while a malicious entity stands to lose everything. Additionally, BoLD’s design ensures that any party bonded to the correct

If a malicious entity wanted to attack Arbitrum, they would need to deposit 3600 `ETH` to propose an invalid state assertion.

:::info Node Running Info

- Anyone can run an Arbitrum node (today and post-BoLD)
- The default mode is `watchtower`, which chills out and watches the chain in action. It will alert you if it sees something wrong on chain but no action is taken. It requires **no** funds and **doesn't** take any onchain action.
- Other "modes" that nodes can run are: `stakeLatest`, `resolveNodes`, `makeNodes`, and `defensive`. All of these modes **require** funds and **will** take onchain action.
  - Nodes running in this mode are considered validators because they validate what they see and take onchain action.
- Running a `watchtower` node is **not** a validator.
- Proposers are a special role, they strictly run in `makeNodes` mode. This means a proposer is someone running an Arbitrum node in `makeNodes` mode, also making them a validator.
- For more information about see [Running a Node](/run-arbitrum-node/01-overview.mdx) and [Validation strategies](/run-arbitrum-node/more-types/02-run-validator-node.mdx#validation-strategies).
  :::

#### Q: How do BoLD-based L3s challenge periods operate, considering the worst-case scenario?

To recap, both Arbitrum’s current dispute protocol and BoLD require assertions to be posted to the parent chain and employ interactive proving, which involves a back-and-forth between two entities until a single step of disagreement is reached. That single step (of disagreement) is then submitted to contracts on the parent chain. Those contracts are used to declare a winner. For L2s, like Arbitrum One, BoLD must be deployed on a credibly-neutral, censorship-resistant backend to ensure disputes are fairly resolved. Ethereum, therefore, is the perfect candidate for deployment of the BoLD protocol for L2s.

But you might now be wondering: what about L3 Arbitrum chains that don’t settle to Ethereum? Unlike L2s that settle to Ethereum, assertions on an L3’s state need to be posted to an L2 either via (A) the L3 <a data-quicklook-from="sequencer">Sequencer</a> or (B) the <a data-quicklook-from="delayed-inbox">Delayed Inbox</a> queue managed by the L2 sequencer on L2. In the event that the parent chain (in this case, L2) is being repeatedly censored or if the L2 sequencer is offline, every block level assertion and/or sub-challenge assertion would need to wait 24 hours before they can bypass the sequencer (using the the`SequencerInbox`’s `forceInclusion` method described [here](/how-arbitrum-works/03-sequencer.mdx)). If this were to happen, challenge resolution would be delayed by a time _t_ where _t_ = (24 hours) \* number of moves for a challenge. To illustrate with sample numbers, if a challenge takes 50 sequential moves to resolve, then the delay would be 50 days!

To mitigate the risk of this issue manifesting for Arbitrum chains, Offchain Labs has included a feature called _Delay Buffer_ as part of BoLD’s 1.0.0 release. The _Delay Buffer_ feature aims to limit the negative effects of: prolonged parent chain censorship, prolonged sequencer censorship, and/or unexpected sequencer outages. This is accomplished by implementing some time threshold that is decremented when unexpected delays occur. Once that time threshold is met, the force inclusion window is lowered - effectively enabling entities to make moves without the 24 hour delay-per-move.

Under reasonable parameterization, the sequencer could be offline / censoring for 24 hours twice, before the force inclusion window is effectively dropped from 24 hours to a minimum inclusion time. The force inclusion window gradually (over weeks) replenishes to it's original value over time as long as the sequencer is on "good behavior" - regularly sequencing messages without unexpected delays. We believe that the Delay Buffer feature provides stronger guarantees of censorship resistance for Arbitrum chains.

The methodology for calculating the parameters, specifically for L3 Arbitrum chains, will be made available at a later date for teams who wish to use BoLD.

#### Q: What is the user flow for using the assertion bonding pool contract?

:::warning

The autopooling feature is not available on <a data-quicklook-from="arbitrum-nitro">Arbitrum Nitro</a> as of yet.

:::

Anyone can deploy an assertion bonding pool using [`AssertionStakingPoolCreator.sol`](https://github.com/OffchainLabs/BoLD/blob/main/contracts/src/assertionStakingPool/AssertionStakingPoolCreator.sol) as a means to crowdsource funds to put up a bond for an assertion. To defend Arbitrum using a bonding pool, an entity would first deploy this pool with the assertion they believe is correct and wish to put up a bond to challenge an adversary's assertion. Then, anyone can verify that the claimed assertion is correct by running the inputs through their node's State Transition Function (STF). If other parties agree that the assertion is correct, then they can deposit their funds into the contract. When enough funds have been deposited, anyone can permissionlessly trigger the creation of the assertion onchain to start the challenge. Finally, once the honest parties' assertion is confirmed by the dispute protocol, all involved entities can get their funds reimbursed and can withdraw. The Arbitrum Nitro node validation software also comes with an optional feature called "auto pooling," where the entire workflow of assertion bonding pool deployment and depositing funds into the said pool are automated. If "auto pooling" is activated & the private key controlling the validator has funds, a pool will be trustlessly deployed alongside an assertion with the available funds. If a validator with the "auto pooling" feature enabled sees an assertion onchain that it agrees with _and_ a bonding pool already exists for that assertion, then the validator will automatically deposit funds into the bonding pool to "join" the others who are backing that onchain assertion in a trustless manner.

#### Q: What type of hardware will be necessary to run a BoLD validator?

The minimum hardware requirements for running a BoLD validator is still being researched and finalized. The goal, however, is that regular consumer hardware (i.e., laptop) can effectively be used by an honest party to secure an Arbitrum chain using BoLD in the average case.

#### Q: How do BoLD validators communicate with one another? Is it over a P2P network?

BoLD validators for Arbitrum chains communicate directly with smart contracts on the parent chain (Ethereum). This means that opening challenges, submitting bisected history commitments, one-step proofs, and confirmations are all refereed on Ethereum. There is no P2P between validators.

#### Q: For an L3 Arbitrum chain, secured using BoLD, that settles to Arbitrum One, does the one-step proof happen on the parent chain?

Yes; it happens on Arbitrum One.

#### Q: For Arbitrum One, does implementing BoLD reduce the scope or remove the need for the Arbitrum Security Council?

BoLD can limit the scope of Arbitrum One and Nova’s reliance on the Security Council as it takes Arbitrum chains one-step closer to full decentralization.

---

## .mdx (bold/partials/_bold-public-preview-banner-partial.mdx)
:::caution ALPHA RELEASE, PUBLIC PREVIEW DOCS

The BoLD dispute protocol is currently deployed on a public testnet (that posts assertions to Ethereum Sepolia) and is tagged as an `alpha` release. The code has been audited by [Trail of Bits](https://github.com/trailofbits/publications/blob/master/reviews/2024-04-offchainbold-securityreview.pdf) and in a [public audit competition with Code4rena](https://code4rena.com/audits/2024-05-arbitrum-bold), but **should not be used in production scenarios**. Please note that the public testnet is intended for Arbitrum users and researchers to test and experiment with the BoLD dispute protocol for the purposes of education and hardening the protocol via the surfacing of bugs. The public testnet may be paused, and its parameters may be updated at any time in response to scenarios that impact the network and its ability to fulfill its function as a working, reliable test environment. This documentation is currently in [public preview](../public-preview-expectations.md).

To provide feedback, click the _Request an update_ button at the top of this document, [join the Arbitrum Discord](https://discord.gg/arbitrum), or reach out to our team directly by completing [this form](http://bit.ly/3yy6EUK).

:::

---

## .mdx (bold/public-preview-expectations.mdx)
---
title: 'Public preview: What to expect'
sidebar_label: 'Public preview'
description: 'BoLD is currently tagged as an `alpha` release supported by *public preview* documentation. This concept document explains what this means, and what to expect.'
author: symbolpunk
sidebar_position: 10
---

BoLD, Arbitrum's new dispute protocol, is currently tagged as an `alpha` release supported by _public preview_ documentation. This concept document explains what "public preview" means, what to expect from public preview capabilities, and how to engage with our team as you tinker.

### How products are developed at Offchain Labs

Offchain Labs builds products in a way that aligns loosely with the spirit of "building in public". We like to release things **early and often** so that we can capture feedback and iterate in service of your needs, as empirically as possible.

To do this, some of our product offerings are documented with **public preview** disclaimers that look like this:

This banner's purpose is to set expectations while inviting readers like you to express your needs so that we can incorporate them into the way that we iterate on product.

### What to expect when using public preview offerings

As you tinker and provide feedback, we'll be listening. Sometimes, we'll learn something non-obvious that will result in a significant change. More commonly, you'll experience incremental improvements to the developer experience as the offering grows out of its **public preview** status, towards **stable** status.

Public preview offerings are evolving rapidly, so don't expect the degree of release notes discipline that you'd expect from a stable offering. Keep your eyes open for notifications regarding patch, minor, and major changes, along with corresponding relnotes that highlight breaking changes and new capabilities.

### How to provide feedback

Our product team primarily uses three feedback channels while iterating on public preview capabilities:

1.  **Docs**: Click on the **Request an update** button located in the top-right corner of any document to provide feedback on the docs and/or developer experience. This will lead you to a prefilled GitHub issue that members of our product team periodically review.
2.  **Discord**: [Join the Arbitrum Discord](https://discord.gg/arbitrum) to engage with members of the Arbitrum community and product team.
3.  **Google form**: Complete [this form](http://bit.ly/3yy6EUK) to ask for support.

### What to expect when providing feedback

Our ability to respond to feedback is determined by our ever-evolving capacity and priorities. We can't guarantee responses to all feedback submissions, but our small-but-mighty team is listening, and we'll try our best to acknowledge and respond to your feedback. No guarantees though!

_PS, [our small-but-mighty team is hiring](https://jobs.lever.co/offchainlabs)._

### Thank you!

Thanks for helping us build things that meet your needs! We're excited to engage with OGs and newcomers alike; please don't hesitate to reach out.

---

## .mdx (timeboost/gentle-introduction.mdx)
---
title: 'A gentle Introduction: Timeboost'
sidebar_label: 'A gentle introduction'
description: 'Learn how Timeboost works and how it can benefit your Arbitrum-based project.'
author: leeederek
sme: leeederek
user_story: As a current or prospective Arbitrum user, I want to understand how Timeboost works and how to use it.
content_type: gentle-introduction
---

import { FlowChart } from '@site/src/components/InteractiveDiagrams/Timeboost/CentralizedAuction';
import ImageWithCaption from '@site/src/components/ImageCaptions/';
import ImageZoom from '@site/src/components/ImageZoom';

This introduction will walk you through <a data-quicklook-from='timeboost'>Arbitrum Timeboost</a>: a novel <a data-quicklook-from='transaction-ordering-policy'>transaction ordering policy</a> for Arbitrum chains that allows chain owners to capture the Maximal Extractable Value (MEV) on their chain and reduce spam, all while preserving fast block times and protecting users from harmful types of MEV, such as sandwich attacks and front-running.

Timeboost is the culmination of over a year of [research and development](https://arxiv.org/abs/2306.02179) by the team at Offchain Labs. It is currently live on Arbitrum One and Arbitrum Nova, and is available for Arbitrum Orbit chains, whose owners can adopt and customize it as they choose.

### In a nutshell

- The old <a data-quicklook-from='first-come-first-serve-fcfs'>"First-Come, First-Serve (FCFS)"</a> ordering policy has many benefits, including a great UX and protection from harmful types of MEV. However, nearly all MEV on the chain is extracted by searchers who invest wastefully in hardware and spamming to win latency races (which negatively strains the network and leads to congestion). Timeboost is a new transaction ordering policy that preserves many of the great benefits of FCFS while unlocking a path for chain owners to capture some of the available MEV on their network and introducing an auction to reduce latency, racing, and, ultimately, spam.
- Timeboost introduces a few new components to an Arbitrum chain’s infrastructure: a sealed-bid second-price auction and a new <a data-quicklook-from='express-lane'>"express lane"</a> at an Arbitrum chain’s sequencer. Valid transactions submitted to the express lane will be sequenced immediately with no delay, while all other transactions submitted to the chain’s sequencer will experience a nominal delay (default: 200ms). The auction winner is granted the sole right to control the express lane for pre-defined, temporary intervals. The default block time for Arbitrum chains will continue to be industry-leading at 250ms, even with Timeboost enabled. What will change with Timeboost is that some transactions not in the express lane will be delayed to the next block.
- Timeboost is an optional feature for Arbitrum chains aimed at two types of groups of entities: (1) chain owners and their ecosystems and (2) sophisticated onchain actors and searchers. Chain owners can use Timeboost to capture additional revenue from the MEV their chain generates already, and sophisticated onchain actors and searchers will spend their resources on buying rights for the express lane (instead of spending those resources on winning latency races, which otherwise leads to spam and congestion on the network).
- Timeboost will work with both centralized and [decentralized sequencer setups](https://medium.com/@espressosys/espresso-systems-and-offchain-labs-publish-decentralized-timeboost-specification-b29ff20c5db8). The specification for a centralized sequencer is public ([here](https://github.com/OffchainLabs/timeboost-design)); implementing this spec [allowed us to deliver Timeboost to market](https://forum.arbitrum.foundation/t/constitutional-aip-proposal-to-adopt-timeboost-a-new-transaction-ordering-policy/25167/1) sooner, instead of waiting for the completion of the design and implementation of a decentralized sequencer.

## Why do Arbitrum chains need Timeboost?

Today, Arbitrum chains order incoming transactions on a <a data-quicklook-from='first-come-first-serve-fcfs'>"First-Come, First-Serve (FCFS)"</a> basis. This ordering policy was chosen as the default for Arbitrum chains because it is simple to understand and implement, enables fast block times (starting at 250ms and down to 100ms if desired), and protects users from harmful types of MEV like front-running & sandwich attacks.

However, there are a few downsides to an FCFS ordering policy. Under FCFS, searchers are incentivized to participate in and try to win latency races through investments in offchain hardware. This means that for searchers on Arbitrum chains, generating a profit from arbitrage and liquidation opportunities involves a lot of spam, placing stress on chain infrastructure and contributing to congestion. Additionally, all of the captured MEV on an Arbitrum chain under FCFS goes to searchers - returning none of the available MEV to the chain owner or the applications on the chain.

Timeboost retains most FCFS benefits while addressing FCFS limitations.

#### Timeboost preserves the great UX that Arbitrum chains are known for

- The default block time for Arbitrum chains continues to be industry-leading at 250ms, even with Timeboost. With Timeboost, some transactions not in the express lane may experience a delay to the next block.

#### With Timeboost, Arbitrum chains will continue to protect users from harmful types of MEV

- Timeboost only grants the auction winner a _temporary time advantage_ - not the power to view or reorder incoming transactions or to be the first in every block. Furthermore, the transactions' mempool will continue to be private, which means users with Timeboost enabled will continue to be protected from harmful MEV-like front-running and sandwich attacks.

#### Timeboost unlocks a new value accrual path for chain owners

- Chain owners may use Timeboost to capture a portion of the available MEV on their chain that would have otherwise gone entirely to searchers. There are many flavors of this, too - including custom gas tokens and/or redistribution of these proceeds back to the applications and users on the chain.

#### Timeboost may help reduce spam and congestion on a network

- By introducing the ability to “purchase a time advantage” through the Timeboost auction, it is expected that rational, profit-seeking actors will spend on auctions _instead of_ investing in hardware or infrastructure to win latency races. This diversion of resources by these actors is expected to reduce FCFS MEV-driven spam on the network.

## What is Timeboost, and how does it work?

Timeboost is a _transaction ordering policy_. It's a set of rules that the sequencer of an Arbitrum chain is trusted to follow when ordering transactions submitted by users. In the near future, multiple sequencers will be able to enforce those rules with decentralized Timeboost.

For Arbitrum chains, the sequencer’s sole job is to take arriving, valid transactions from users, place them into an order dictated by the transaction ordering policy, and then publish the final sequence to a real-time feed and in compressed batches to the chain’s data availability layer. Prior to Timeboost, the transaction ordering policy was FCFS, and Timeboost is a modified FCFS ordering policy.

Timeboost is implemented using three separate components that work together:

- **A special “express lane”** which allows valid transactions to be sequenced as soon as the sequencer receives them for a given round.
- **An offchain auction** to determine the controller of the express lane for a given round. This auction is managed by an <a data-quicklook-from='autonomous-auctioneer'>autonomous auctioneer</a>.
- **An <a data-quicklook-from='auction-contract'>auction contract</a>** deployed on the target chain to serve as the canonical source of truth for the auction results and handling of auction proceeds.

To start, the default duration of a round is 60 seconds. Transactions not in the express lane will be subject to a default 200-millisecond artificial delay to their arrival timestamp before their transaction is sequenced, which means that some non-express lane transactions may get delayed to the next block. It’s important to note that the default Arbitrum block time will remain at 250 milliseconds (which can be adjusted to 100 milliseconds if desired). Let’s dive into how each of these components works.

### The express lane

<ImageZoom
  src="/img/timeboost-centralized-timeboost-express-lane-workflow.jpg"
  alt="using the express lane"
  className="img-900px"
/>

The express lane is implemented using a special endpoint on the sequencer, formally titled `timeboost_sendExpressLaneTransaction`. This endpoint is special because transactions submitted to it will be sequenced immediately by the sequencer, hence the name, express lane. The sequencer will only accept valid transaction payloads to this endpoint if they are correctly signed by the current round’s <a data-quicklook-from='express-lane-controller'>express lane controller</a>. Other transactions can still be submitted to the sequencer as normal, but these will be considered non-express lane transactions and will, therefore, have their arrival timestamp delayed by 200 milliseconds. It is important to note that transactions from both the express and non-express lanes are eventually sequenced into a single, ordered stream of transactions for the sequencer to post to a data availability layer (and for node operations to then construct the chain's state). The express lane controller does _not_:

- Have the right to re-order transactions.
- Have a guarantee that their transactions will always be first at the “top-of-the-block.”
- Guarantee a profit at all.

The value of the express lane will be the sum of how much MEV the express lane controller predicts they can extract during the upcoming round (i.e., MEV opportunity estimates made before the auction closes) _plus_ the amount of MEV extracted by the express lane controller while they are in control (that they otherwise did not predict). Understanding how the value of the express lane is determined can be useful for chain owners when adjusting to the artificial delay and the time before the auction closes.

### The Timeboost auction

Control of the express lane in each round (default: 60 seconds) is determined by a per-round auction, which is a sealed-bid, second-price auction. This auction is held to determine the express lane controller for the next round. In other words, the express lane controller was determined at any point in time in the previous auction round. Bids for the auction can be made with any `ERC-20` token, in any amount, and can be collected by any address - at the full discretion of the chain owner.

<FlowChart />

The auction for a round has a closing time that is `auctionClosingSeconds` (default: 15) seconds before the beginning of the round. This means that, in the default parameters, parties have 45 seconds to submit bids before the auction will no longer accept bids. In the 15 seconds between when bids are no longer accepted and when the new round begins, the autonomous auctioneer will verify all bids, determine the winner, and make a call to the on-chain auction contract to formally resolve the auction.

### Auction contract

Before placing a bid in the auction, a party must deposit funds into the `Auction` Contract. At any time, deposits can be made or funds can be added to an existing deposit. These deposits are fully withdrawable, with some nominal delay (two rounds or two minutes by default), so as not to impact the outcome of an existing round. There is no minimum deposit amount, but there is a starting minimum bid of 0.001 `WETH` (default amount and token) called the "minimum reserve price". The chain owner sets the minimum reserve price, which can be updated at any time up to 30 seconds (default) before the start of the next round to ensure the auction participants will always know the reserve price at least 30 seconds before they must submit their bids. A reserve price can also be set by the chain owner (or by an address designated by the chain owner) as a way to raise the minimum bid, as the `Auction` Contract enforces that the reserve price is never less than the minimum reserve price.

Once the autonomous auctioneer determines an auction winner, the `Auction` contract will deduct the second-highest bid amount from the account of the highest bidder and transfer those funds to a `beneficiary` account designated by the chain owner by default. The `expressLaneControllerAddress` specified in the highest bid will become the express lane controller for the round.

### Default parameters

Below are a few of the default Timeboost parameters mentioned earlier. All these parameters and more are configurable by the chain owner.

| Parameter name          | Description                                                                                                                                                                                                                  | Recommended default value                  |
| ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------ |
| `roundDurationSeconds`  | Duration of time that the sequencer will honor the express lane privileges for transactions signed by the current round’s express lane controller.                                                                           | 60 seconds                                 |
| `auctionClosingSeconds` | Time before the start of the next round. The autonomous auctioneer will not accept bids during this time interval.                                                                                                           | 15 seconds                                 |
| `beneficiary`           | Address where proceeds from the Timeboost auction are sent to when `flushBeneficiaryBalance()` gets called on the auction contract.                                                                                          | An address controlled by the chain's owner |
| `_biddingToken`         | Address of the token used to make bids in the Timeboost auction. It can be any `ERC-20` token (assuming the token chosen does not have fee-on-transfer, rebasing, transfer hooks, or otherwise non-standard `ERC-20` logic). | `WETH`                                     |
| `nonExpressDelayMsec`   | The artificial delay applied to the arrival timestamp of non-express lane transactions _before_ the non-express lane transactions are sequenced.                                                                             | 0.2 seconds, or 200 milliseconds           |
| `reservePrice`          | The minimum bid amount accepted by the auction contract for the current Timeboost auction, denominated in `_biddingToken`.                                                                                                   | None                                       |
| `_minReservePrice`      | A value that must be equal to or below the `reservePrice` to act as a "floor minimum" for Timeboost bids. Enforced by the auction contract.                                                                                  | 0.001 `WETH`                               |

## Who is Timeboost for, and how do I use it?

Timeboost is an optional addition to an Arbitrum chain’s infrastructure, meaning that enabling Timeboost is at the discretion of the chain owner and that an Arbitrum chain can fully function normally without Timeboost.

When enabled, Timeboost is meant to serve different groups of parties with varying degrees of impact and benefits. Let’s go through them below:

#### For regular users:

Timeboost will have a minimal impact. Non-express lane transactions will be delayed by a nominal 200ms, which means to the average user, their transactions will take approximately 450ms to be sequenced and included into a block (up from approximately 200ms). All users will remain protected from harmful MEV activity, such as sandwich attacks and front-running, through the continued use of a private mempool.

#### For chain owners:

Timeboost represents a unique way to accrue value to their token and generate revenue for the chain. Explicitly, chain owners can set up their Timeboost auction to collect bid proceeds in the same token used for gas on their network and then choose what to do with these proceeds afterward.

#### For searchers/arbitrageurs:

Timeboost adds a unique twist to your existing or prospective MEV strategies that may become more profitable than before. For instance, purchasing the time advantage offered by Timeboost’s auction may end up costing _less_ than the costs to invest in hardware and win latency races. Another example is the potential new business model of reselling express lane rights to other parties in time slots or on a granular, per-transaction basis.

### Special note on Timeboost for chain owners

As with many new features and upgrades to Arbitrum Nitro, Timeboost is an optional feature that chain owners may choose to deploy and customize however they see fit. Deploying and enabling/disabling Timeboost on a live Arbitrum chain will not halt or impact the chain but will instead influence the chain's transaction ordering policy. An Arbitrum chain will, by default, fall back to FCFS if Timeboost is deployed but disabled or if there is no express lane controller for a given round.

It is recommended that Arbitrum teams holistically assess the applicability and use cases of Timeboost for their chain before deploying and enabling Timeboost. This is because some Arbitrum chains may not have that much MEV (e.g., arbitrage) to begin with. Furthermore, we recommend that Arbitrum chains start with the default parameters recommended by Offchain Labs and closely monitor the results and impacts on your chain’s ecosystem over time before considering adjusting any of the parameters.

---

## .mdx (timeboost/how-to-use-timeboost.mdx)
---
title: 'How to use Timeboost'
description: Learn how to use timeboost
author: jose-franco
content_type: how-to
---

import { VanillaAdmonition } from '@site/src/components/VanillaAdmonition/';

Timeboost is a new transaction ordering policy for Arbitrum chains. With Timeboost, anyone can bid for the right to access an express lane on the **Sequencer** faster transaction inclusion.

In this how-to, you'll learn how to bid for the right to use the express lane and submit transactions through the express lane. To learn more about Timeboost and the key terms used on this page, refer to the [gentle introduction](/how-arbitrum-works/timeboost/gentle-introduction.mdx).

This how-to assumes that you're familiar with the following:

- [How Timeboost works](/how-arbitrum-works/timeboost/gentle-introduction.mdx)
- [viem](https://viem.sh/), since the snippets of code present in the how-to use this library

<VanillaAdmonition type="note"  title="Note about transferring express lane rights">

Please note that in the initial release of Timeboost, transferring control of the express lane via either the `setTransferor` or the `transferExpressLaneController` will not be supported by the Arbitrum Nitro node software at launch and may be implemented at a future date via a regular node upgrade. Calls made to these two functions on the auction contract will be successful, but the node software (including the Sequencer) will not recognize the actual rights transfer.

A round's express lane controller, at their choice, can still send transactions signed by others on a per-transaction basis, as explained later in this guide.

</VanillaAdmonition>

## How to submit bids for the right to be the express lane controller

To use the express lane for faster transaction inclusion, you must win an auction for the right to be the express lane controller for a specific round.

<VanillaAdmonition type="note">

Remember that, by default, each round lasts 60 seconds, and the auction for a specific round closes 15 seconds before the start of the round. These default values can be configured on a chain using the `roundDurationSeconds` and `auctionClosingSeconds` parameters.

</VanillaAdmonition>

An auction contract facilitates auctions, and bids get submitted to an autonomous auctioneer that interacts with the contract. Let's examine the process of submitting bids and determining the winner of an auction.

### Prerequisites: Gather the required information

import { AddressExplorerLink as AEL } from '@site/src/components/AddressExplorerLink';

Before we begin, make sure you have:

- Address of the auction contract
- Endpoint of the autonomous auctioneer

The following table shows this information for the Arbitrum DAO-owned chains:

#### Table 1: Timeboost reference URLs and addresses

| Network          | Auction contract                                                              | Autonomous auctioneer endpoint             |
| ---------------- | ----------------------------------------------------------------------------- | ------------------------------------------ |
| Arbitrum Sepolia | <AEL address="0x991DbEDf388CB5925318f06362D4fCa7b040527D" chainID={421614} /> | https://arbsepolia-auctioneer.arbitrum.io/ |
| Arbitrum One     | <AEL address="0x5fcb496a31b7AE91e7c9078Ec662bd7A55cd3079" chainID={42161} />  | https://arb1-auctioneer.arbitrum.io/       |
| Arbitrum Nova    | <AEL address="0xa5aBADAF73DFcf5261C7f55420418736707Dc0db" chainID={42170} />  | https://nova-auctioneer.arbitrum.io/       |

### Step 1: Deposit funds into the auction contract

Before bidding on an auction, we need to deposit funds in the auction contract. These funds are in the form of the `ERC-20` tokens used to bid, also known as the `bidding token`. We will be able to bid for an amount that is equal to or less than the tokens we have deposited in the auction contract.

To see the amount of tokens we have deposited in the auction contract, we can call the function `balanceOf` in the `Auction` contract:

```tsx
const depositedBalance = await publicClient.readContract({
  address: auctionContractAddress,
  abi: auctionContractAbi,
  functionName: 'balanceOf',
  args: [userAddress],
});
console.log(`Current balance of ${userAddress} in auction contract: ${depositedBalance}`);
```

If we want to deposit more funds to the `Auction` contract, we first need to know what the bidding token is. To obtain the address of the bidding token, we can call the function `biddingToken` in the `Auction` contract:

```tsx
const biddingTokenContractAddress = await publicClient.readContract({
  address: auctionContractAddress,
  abi: auctionContractAbi,
  functionName: 'biddingToken',
});
console.log(`biddingToken: ${biddingTokenContractAddress}`);
```

<VanillaAdmonition type="note"  title="Bidding token in Arbitrum chains">

On Arbitrum One and Arbitrum Nova, the default bidding token is `WETH`.

</VanillaAdmonition>

Once we know what the bidding token is, we can deposit funds to the auction contract by calling the function `deposit` of the contract after having it approved as spender of the amount we want to deposit:

```tsx
// Approving spending tokens
const approveHash = await walletClient.writeContract({
  account,
  address: biddingTokenContractAddress,
  abi: parseAbi(['function approve(address,uint256)']),
  functionName: 'approve',
  args: [auctionContract, amountToDeposit],
});
console.log(`Approve transaction sent: ${approveHash}`);

// Making the deposit
const depositHash = await walletClient.writeContract({
  account,
  address: auctionContractAddress,
  abi: auctionContractAbi,
  functionName: 'deposit',
  args: [amountToDeposit],
});
console.log(`Deposit transaction sent: ${depositHash}`);
```

### Step 2: Submit bids

Once we have deposited funds into the auction contract, we can submit bids for the current auction round.

We can obtain the current round by calling the function `currentRound` in the `Auction` contract:

```tsx
const currentRound = await publicClient.readContract({
  address: auctionContractAddress,
  abi: auctionContractAbi,
  functionName: 'currentRound',
});
console.log(`Current round: ${currentRound}`);
```

The above shows the current round that's running. At the same time, the auction for the next round might be open. For example, if the `currentRound` is 10, the auction for round 11 is currently happening. To check whether or not that auction is open, we can call the function `isAuctionRoundClosed` of the `Auction` contract:

```tsx
let currentAuctionRoundIsClosed = await publicClient.readContract({
  address: auctionContractAddress,
  abi: auctionContractAbi,
  functionName: 'isAuctionRoundClosed',
});
```

:::note

Remember that, by default, auctions for a given round open 60 seconds before that round starts and close 15 seconds before the round starts, so there might be no auctions opened at certain times.

:::

Once we know the current round, we can bid for (`currentRound + 1`) and verify that the auction is still open (`!currentAuctionRoundIsClosed`), then we can submit a bid.

When bids get submitted to the autonomous auctioneer endpoint, we need to send an `auctioneer_submitBid` request with the following information:

- chain id
- address of the express lane controller candidate (for example, our address if we want to be the express lane controller)
- address of the auction contract
- round we are bidding for (in our example, `currentRound + 1`)
- the amount in `wei` of the deposit `ERC-20` token to bid
- signature (explained below)

<VanillaAdmonition type="info"  title="Minimum reserve price">

The amount to bid must be above the minimum reserve price at the moment you are bidding. This parameter is configurable per chain. You can obtain the minimum reserve price by calling the method `minReservePrice()(uint256)` in the `Auction` contract.

</VanillaAdmonition>

Let's see an example of a call to this RPC method:

```tsx
const currentAuctionRound = currentRound + 1;
const hexChainId: `0x${string}` = `0x${Number(publicClient.chain.id).toString(16)}`;

const res = await fetch(<AUTONOMOUS_AUCTIONEER_ENDPOINT>, {
  method: 'POST',
  headers: { 'content-type': 'application/json' },
  body: JSON.stringify({
    jsonrpc: '2.0',
    id: 'submit-bid',
    method: 'auctioneer_submitBid',
    params: [
      {
        chainId: hexChainId,
        expressLaneController: userAddress,
        auctionContractAddress: auctionContractAddress,
        round: `0x${currentAuctionRound.toString(16)}`,
        amount: `0x${Number(amountToBid).toString(16)}`,
        signature: signature,
      },
    ],
  }),
});
```

The signature that needs to be sent is an [EIP-712](https://eips.ethereum.org/EIPS/eip-712) signature over the following typed structure data:

- Domain: `Bid(uint64 round,address expressLaneController,uint256 amount)`
- `round`: auction round number
- `expressLaneController`: address of the express lane controller candidate
- `amount`: amount to bid

Here's an example to produce that signature with viem:

```tsx
const currentAuctionRound = currentRound + 1;

const signatureData = hashTypedData({
  domain: {
    name: 'ExpressLaneAuction',
    version: '1',
    chainId: Number(publicClient.chain.id),
    verifyingContract: auctionContractAddress,
  },
  types: {
    Bid: [
      { name: 'round', type: 'uint64' },
      { name: 'expressLaneController', type: 'address' },
      { name: 'amount', type: 'uint256' },
    ],
  },
  primaryType: 'Bid',
  message: {
    round: currentAuctionRound,
    expressLaneController: userAddress,
    amount: amountToBid,
  },
});
const signature = await account.sign({
  hash: signatureData,
});
```

<VanillaAdmonition type="note">

You can also call the function `getBidHash` in the auction contract to obtain the `signatureData`, specifying the `round`, `userAddress`, and `amountToBid`.

</VanillaAdmonition>

When sending the request, the autonomous auctioneer will return an empty result with an HTTP status `200` if received correctly. If the result returned contains an error message, something went wrong. Following are some of the error messages that can help us understand what's happening:

#### Table 2: Errors relating to bid submission

| Error                   | Description                                                                                                 |
| ----------------------- | ----------------------------------------------------------------------------------------------------------- |
| `MALFORMED_DATA`        | Wrong input data, failed to deserialize, missing certain fields, etc.                                       |
| `NOT_DEPOSITOR`         | The address is not an active depositor in the auction contract                                              |
| `WRONG_CHAIN_ID`        | Wrong chain id for the target chain                                                                         |
| `WRONG_SIGNATURE`       | Signature failed to verify                                                                                  |
| `BAD_ROUND_NUMBER`      | Incorrect round, such as one from the past                                                                  |
| `RESERVE_PRICE_NOT_MET` | Bid amount does not meet the minimum required reserve price onchain                                         |
| `INSUFFICIENT_BALANCE`  | The bid amount specified in the request is higher than the deposit balance of the depositor in the contract |

### Step 3: find out the winner of the auction

After the auction closes and before the round starts, the autonomous auctioneer will call the auction contract with the two highest bids received, allowing the contract to declare the winner and deduct the second-highest bid from the winner's deposited funds. After this, the contract will emit an event with the new Express Lane Controller address.

We can use this event to determine whether or not we've won the auction. The event signature is:

```solidity
event SetExpressLaneController(
    uint64 round,
    address indexed previousExpressLaneController,
    address indexed newExpressLaneController,
    address indexed transferor,
    uint64 startTimestamp,
    uint64 endTimestamp
);
```

Here's an example to get the log from the auction contract to determine the new express lane controller:

```tsx
const fromBlock = <any recent block, for example during the auction>
const logs = await publicClient.getLogs({
  address: auctionContractAddress,
  event: auctionContractAbi.filter((abiEntry) => abiEntry.name === 'SetExpressLaneController')[0],
  fromBlock,
});

const newExpressLaneController = logs[0].args.newExpressLaneController;
console.log(`New express lane controller: ${newExpressLaneController}`);
```

If you won the auction, congratulations! You are the express lane controller for the next round, which, by default, will start 15 seconds after the auction closes. The following section explains how we can submit a transaction to the express lane.

## How to submit transactions to the express lane

The sequencer immediately sequences transactions sent to the express lane, while regular transactions are delayed 200ms by default. However, only the express lane controller can send transactions to the express lane. The previous section explained how to participate in the auction as the express lane controller for a given round.

The sequencer handles the express lane. When sending transactions to the Sequencer endpoint, we need to send a `timeboost_sendExpressLaneTransaction` request with the following information:

- chain id
- current round (following the example above, `currentRound`)
- address of the auction contract
- sequence number: a per-round nonce of express lane submissions, which resets to 0 at the beginning of each round. You can also use the special "dontcare" sequence number (2^64 - 1) to indicate that you don't care about ordering relative to other `ExpressLaneSubmissions` (normal nonce ordering within transactions for an account is still respected)
- RLP-encoded transaction payload
- conditional options for Arbitrum transactions ([more information](https://github.com/OffchainLabs/go-ethereum/blob/48de2030c7a6fa8689bc0a0212ebca2a0c73e3ad/arbitrum_types/txoptions.go#L71))
- signature (explained below)

:::info Timeboost-ing third party transactions

Notice that while the express lane controller must sign the `timeboost_sendExpressLaneTransaction` request, any party can sign the transaction for execution. In other words, the express lane controller can receive transactions signed by other parties and sign them to apply the time advantage offered by the express lane to those transactions.

:::

:::info Support for `eth_sendRawTransactionConditional`

Timeboost doesn't currently support the `eth_sendRawTransactionConditional` method.

:::

Let's see an example of a call to this RPC method:

```tsx
const hexChainId: `0x${string}` = `0x${Number(publicClient.chain.id).toString(16)}`;

const transaction = await walletClient.prepareTransactionRequest(...);
const serializedTransaction = await walletClient.signTransaction(transaction);

const res = await fetch(<SEQUENCER_ENDPOINT>, {
  method: 'POST',
  headers: { 'content-type': 'application/json' },
  body: JSON.stringify({
    jsonrpc: '2.0',
    id: 'express-lane-tx',
    method: 'timeboost_sendExpressLaneTransaction',
    params: [
      {
        chainId: hexChainId,
        round: `0x${currentRound.toString(16)}`,
        auctionContractAddress: auctionContractAddress,
        sequenceNumber: `0x${sequenceNumber.toString(16)}`,
        transaction: serializedTransaction,
        options: {},
        signature: signature,
      },
    ],
  }),
});
```

The required signature is an Ethereum signature that needs to be sent with the following information:

- Hash of `keccak256("TIMEBOOST_BID")`
- Chain id in hexadecimal, padded to 32 bytes
- Auction contract address
- Round number in hexadecimal, padded to 8 bytes
- Sequence number in hexadecimal, padded to 8 bytes
- Serialized transaction

Here's an example to produce that signature:

```tsx
const hexChainId: `0x${string}` = `0x${Number(publicClient.chain.id).toString(16)}`;

const transaction = await walletClient.prepareTransactionRequest(...);
const serializedTransaction = await walletClient.signTransaction(transaction);

const signatureData = concat([
  keccak256(toHex('TIMEBOOST_BID')),
  pad(hexChainId),
  auctionContract,
  toHex(numberToBytes(currentRound, { size: 8 })),
  toHex(numberToBytes(sequenceNumber, { size: 8 })),
  serializedTransaction,
]);
const signature = await account.signMessage({
  message: { raw: signatureData },
});
```

When sending the request, the sequencer will return an empty result with an HTTP status `200` if it received it correctly. If the result returned contains an error message, something went wrong. Following are some of the error messages that can help us understand what's happening:

#### Table 3: Errors relating to express lane transaction submission

Note that if you get any of the errors below, then the sequence number used in your express lane transaction was _not_ consumed.
| Error | Description |
| ------------------------------ | --------------------------------------------------------------------- |
| `MALFORMED_DATA` | wrong input data, failed to deserialize, missing certain fields, etc. |
| `WRONG_CHAIN_ID` | wrong chain id for the target chain |
| `WRONG_SIGNATURE` | signature failed to verify |
| `BAD_ROUND_NUMBER` | incorrect round, such as one from the past |
| `NOT_EXPRESS_LANE_CONTROLLER` | the sender is not the express lane controller |
| `NO_ONCHAIN_CONTROLLER` | there is no defined, on-chain express lane controller for the round |
| `SEQUENCE_NUMBER_ALREADY_SEEN` | the sequence number used for the given transaction was already consumed, try resubmitting with a new sequence number |
| `SEQUENCE_NUMBER_TOO_LOW` | the sequencer number used for the given transaction is numerically lower than the expeected sequence number, try resubmitting with the expected sequence number
| `sequence number has reached max allowed limit` | the limit on the number of buffered express lane transactions was reached. Read more about this on our [Troubleshoot Timeboost page](/how-arbitrum-works/timeboost/troubleshoot-timeboost.mdx#the-block-based-timeout-for-express-lane-transactions) |

:::info What happens if you're not the express lane controller?

If you are not the express lane controller and you try to submit a transaction to the express lane, the sequencer will respond with the error `NOT_EXPRESS_LANE_CONTROLLER` or `NO_ONCHAIN_CONTROLLER`.

:::

<!--

## How to transfer the right to use the express lane to someone else

If you are the express lane controller, you also have the right to transfer the right to use the express lane to someone else.

To do that, you can call the function `transferExpressLaneController` in the auction contract:

```tsx
const transferELCTransaction = await walletClient.writeContract({
  currentELCAccount,
  address: auctionContractAddress,
  abi: auctionContractAbi,
  functionName: 'transferExpressLaneController',
  args: [currentRound, newELCAddress],
});
console.log(`Transfer EL controller transaction hash: ${transferELCTransaction}`);
```

From that moment, the previous express lane controller will not be able to send new transactions to the express lane.

### Setting a transferor account

A `transferor` is an address with the right to transfer express lane controller rights on behalf of the express lane controller. This function (`setTransferor`) ensures that the express lane controller has a way of nominating an address that can transfer rights to anyone they see fit to improve the user experience of reselling/transferring the control of the express lane.

We can set a transferor for our account using the auction contract. Additionally, we can fix that transferor account until a specific round to guarantee other parties that we will not change the transferor until the specified round finishes.

To set a transferor, we can call the function `setTransferor` in the auction contract:

```tsx
// Fixing the transferor for 10 rounds
const fixedUntilRound = currentRound + 10n;

const setTransferorTransaction = await walletClient.writeContract({
  currentELCAccount,
  address: auctionContractAddress,
  abi: auctionContractAbi,
  functionName: 'setTransferor',
  args: [
    {
      addr: transferorAddress,
      fixedUntilRound: fixedUntilRound,
    },
  ],
});
console.log(`Set transferor transaction hash: ${setTransferorTransaction}`);
```

From that moment on (until the transferor is changed or disabled), the transferor will be able to call `transferExpressLaneController` while the express lane controller is `currentELCAccount` to transfer the rights to use the express lane to a different account.

-->

## How to withdraw funds deposited in the auction contract

Funds are deposited in the auction contract to have the right to bid in auctions. Withdrawing funds is possible through a two-step process: initiate the withdrawal, wait for two rounds, and then finalize the withdrawal.

To initiate a withdrawal, we can call the function `initiateWithdrawal` in the `Auction` contract:

```tsx
const initWithdrawalTransaction = await walletClient.writeContract({
  account,
  address: auctionContractAddress,
  abi: auctionContractAbi,
  functionName: 'initiateWithdrawal',
});
console.log(`Initiate withdrawal transaction sent: ${initWithdrawalTransaction}`);
```

This transaction will initiate a withdrawal of all funds deposited by the sender's account. When executing it, the contract will emit a `WithdrawalInitiated` event with the following structure:

```solidity
event WithdrawalInitiated(
    address indexed account,
    uint256 withdrawalAmount,
    uint256 roundWithdrawable
);
```

In this event, the `account` is the address from which we will withdraw funds, `withdrawalAmount` specifies the amount we will take from the contract, and `roundWithdrawable` indicates the specific round during which we can finalize the withdrawal.

After two rounds have passed, we can call the method `finalizeWithdrawal` in the `Auction` contract to finalize the withdrawal:

```tsx
const finalizeWithdrawalTransaction = await walletClient.writeContract({
  account,
  address: auctionContractAddress,
  abi: auctionContractAbi,
  functionName: 'finalizeWithdrawal',
});
console.log(`Finalize withdrawal transaction sent: ${finalizeWithdrawalTransaction}`);
```

## How to identify timeboosted transactions

Transactions sent to the express lane by the express lane controller and that have been executed (regardless of whether they were successful or reverted) can be identified by examining their receipts or the message broadcast by the Sequencer feed.

Transaction receipts now include a new field, `timeboosted`, which will be `true` for timeboosted transactions and `false` for regular non-timeboosted transactions. For example:

```shell
blockHash               0x56325449149b362d4ace3267681c3c90823f1e5c26ccc4df4386be023f563eb6
blockNumber             105169374
contractAddress
cumulativeGasUsed       58213
effectiveGasPrice       100000000
from                    0x193cA786e7C7CC67B6227391d739E41C43AF285f
gasUsed                 58213
logs                    []
logsBloom               0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
root
status                  1 (success)
transactionHash         0x62ea458ad2bb408fab57d1a31aa282fe3324b2711e0d73f4777db6e34bc1bef5
transactionIndex        1
type                    2
blobGasPrice
blobGasUsed
to                      0x0000000000000000000000000000000000000001
gasUsedForL1            "0x85a5"
l1BlockNumber           "0x6e8b49"
timeboosted             true
```

In the sequencer feed, the `BroadcastFeedMessage` struct now contains a `blockMetadata` field that represents whether a particular transaction in the block was timeboosted or not. The field `block metadata` is an array of bytes, and it starts with a byte representing the version (`0`), followed by `ceil(N/8)` bytes, where `N` is the number of transactions in the block. If a particular transaction were time-boosted, the bit representing its position in the block would be set to `1`, while the rest would reset to `0`. For example, if the `blockmetadata` of a particular message, viewed as bits, is `00000000 01100000`, then the 2nd and 3rd transactions in that block were time boosted.

## How to view historical bid data

In the current implementation, information about the winning bid for a resolved auction emits via the `AuctionResolved` event ([sample interface](https://github.com/OffchainLabs/nitro-contracts/blob/main/src/express-lane-auction/IExpressLaneAuction.sol#L95-L103)). Historical bid information, including the round number and bid amounts, are published to a public S3 bucket at a regular cadence. The domain for the S3 bucket where historical bids get saved is:

#### Table 4: S3 URLs for historical bid data

:::info URL updates for historical bid data
On June 9 2025, at around 17:27 ET (UTC−05:00), the region in which the Amazon S3 bucket for historical bid data on Arbitrum One was _changed_ from `s3://timeboost-auctioneer-arb1/uw2/validated-timeboost-bids/` to `s3://timeboost-auctioneer-arb1/ue2/validated-timeboost-bids/`. The below table has been updated for the new, correct URL to access bid data after June 9, 2025 17:27 ET, but if you require data from before June 9, 2025 17:27 ET, then please use `s3://timeboost-auctioneer-arb1/uw2/validated-timeboost-bids/`.

:::

| Chain            | S3 bucket URL                                                   |
| ---------------- | --------------------------------------------------------------- |
| Arbitrum Sepolia | s3://timeboost-auctioneer-sepolia/ue2/validated-timeboost-bids/ |
| Arbitrum One     | s3://timeboost-auctioneer-arb1/ue2/validated-timeboost-bids/    |
| Arbitrum Nova    | s3://timeboost-auctioneer-nova/ue2/validated-timeboost-bids/    |

:::note

Make sure you use `--no-sign-request` with the [AWS S3 CLI](https://docs.aws.amazon.com/cli/latest/reference/s3/).

:::

Here is an example query on how to look up and download historical bid data:

```shell
➜  ~ aws s3 ls s3://timeboost-auctioneer-arb1/ue2/validated-timeboost-bids/2025/06/10/  --no-sign-request --recursive

2025-06-09 18:21:47      12553 ue2/validated-timeboost-bids/2025/06/09/0130304-0130343.csv.gzip
2025-06-09 18:36:46       4725 ue2/validated-timeboost-bids/2025/06/09/0130344-0130358.csv.gzip
...
2025-06-09 17:23:28       3407 uw2/validated-timeboost-bids/2025/06/09/0130264-0130284.csv.gzip
2025-06-09 17:27:32       1228 uw2/validated-timeboost-bids/2025/06/09/0130285-0130288.csv.gzip

➜ ~ aws s3 cp s3://timeboost-auctioneer-arb1/ue2/validated-timeboost-bids/2025/06/09/0130304-0130343.csv.gzip local.csv.gzip --no-sign-request
download: s3://timeboost-auctioneer-arb1/ue2/validated-timeboost-bids/2025/06/09/0130304-0130343.csv.gzip to ./local.csv.gzip
```

## Troubleshooting and best practices

Our guide on [Troubleshooting Timeboost](/how-arbitrum-works/timeboost/troubleshoot-timeboost.mdx) provides more information on how response times work and how express lane transactions are sequenced, along with common errors and best practices for using Timeboost.

---

## .mdx (timeboost/timeboost-faq.mdx)
---
title: 'Frequently Asked Questions (FAQs) about Timeboost'
description: Timeboost FAQ
author: dlee
content_type: faq
user_story: I have questions about Timeboost
---

Below are some common and frequently asked questions about Timeboost. This list of questions is in no particular order and will be updated periodically as new questions arise.

## Using Timeboost

#### As a typical user, will I notice any difference in my experience?

The only difference users should experience is a small delay when submitting their transactions. The default configuration for this delay is 200ms, and a chain's owner can adjust it.

The delay intends to give the express lane controller an advantage, allowing them to include transactions slightly quicker than others. Importantly, user transactions will remain private until after they are sequenced, meaning that the express lane controller cannot frontrun or sandwich other users.

#### How can I participate in Timeboost directly?

Interested parties can participate in the Timeboost auctions by depositing funds in the auction contract and sending bids to the autonomous auctioneer. Feel free to refer to [this guide](../timeboost/how-to-use-timeboost.mdx) for more information.

The Timeboost auction is open to everyone; however, since auctions require a non-zero bid to win, only parties that can generate a return from capturing arbitrage opportunities, backrunning opportunities, or reselling the express lane rights will benefit from participating.

The Timeboost protocol operates behind the scenes with minimal impact on normal users, generating revenue for the chain owners and opening up an additional revenue stream for sophisticated searchers.

#### What is the goal of Timeboost?

The goal of Timeboost is to provide chain owners with a way to capture available MEV on their chain and reduce spam from FCFS arbitrage while preserving a best-in-class user experience with both fast block times and protecting users from harmful MEV (e.g., frontrunning, sandwich attacks).

#### Does it work with Arbitrum chains?

Arbitrum chains can adopt Timeboost, and Arbitrum chain owners can also choose to use any `ERC-20` token for making bids. For example, a chain could decide to accept its (or any other) token for the auction.

#### How do I change or cancel my bid after I have submitted it?

The autonomous auctioneer will consider only an address’s most recent bid, meaning that if you have placed a bid and wish to change it, you may re-submit a bid to “update it.” To cancel a bid, place a new bid that is significantly lower than your original bid or bid below the minimum reserve price. Remember that there is a maximum of five bids per round per address to mitigate DDoS risks.

## Security questions

#### Does Timeboost create new types of MEV extraction vectors?

Timeboost does not create new types of Maximum Extractable Value (MEV). Instead, it introduces slight adjustments to when and how existing forms of MEV operate. Timeboost's design strikes a balance between capturing MEV value for the chain without introducing additional externalities.

For example, Timeboost does not enable transaction reordering in a way that facilitates sandwich attacks. The protocol does allow users to attempt to process their transactions earlier by gaining control of the express lane. Still, it doesn't permit them to manipulate the order in which trades occur relative to others in the same block. This ordering means the fast lane controller [at any given time] cannot be certain of how their transactions will get ordered relative to others' transactions.

#### Does Timeboost give the auction winner an unfair advantage or power around transaction ordering?

Winning a Timeboost auction gives you a time advantage — specifically, a proposed 200ms “head start” — but it does not ensure your transaction will always be the first in every block. The perceived value of the express lane is determined by its holder and the amount they choose to bid to win control of it; it’s a use-it-or-lose-it privilege. Let’s be clear on what Timeboost does not do:

- It does not give anyone the right to reorder transactions. It does not allow you to view others’ transactions until they are sequenced (because the mempool remains private).
- It does not ensure your transaction will always be the first in every block.
- It does not mean your transaction will have absolutely zero total time delay. Winning the bid means you won’t experience the 200ms artificial delay others face, but natural delays — such as processing time or network distance — still apply.

#### Is it expected for powerful, centralized entities to monopolize the Timeboost express lane? Could this lead to harmful outcomes?

Timeboost's design is an auction-based system that encourages open competition. Although the idea of a monopoly can be intimidating, the auction process remains competitive. If one player dominates, they will be required to outbid other users, which prevents them from maintaining complete static control continuously. Additionally, the express lane only gives a 200ms time advantage. The system is designed to incentivize rational actors to participate when they believe there is an advantage to controlling the express lane and only bid up to the value they are willing to pay for that advantage (since it is a sealed-bid auction).

Finally, Timeboost is entirely optional, meaning that Arbitrum chains can still function normally without it. Should Timeboost need to be disabled, the network would smoothly revert to FCFS transaction ordering, maintaining its current security and efficiency. Every chain can make its own decision about whether to enable Timeboost–your chain, your rules.

## Technical questions

#### Does Timeboost mean an expectation for searchers to bid continuously in advance, expecting opportunities to happen one minute later, rather than “in real time” opportunities (I see something → I submit an arbitrage tx with priority)?

Before answering this question, it is worth clarifying that the participant will likely attempt to predict the amount of MEV generated between 15 seconds and 1 minute 15 seconds in the future, not 1 minute later. This assumption is because the auction is closed and resolved within a maximum of 15 seconds before the start of the next round (as proposed in the current proposal).

The expectation is willing participants will bid continuously for the right to use the express lane in advance so that they (the participant) can profit from both (1) MEV opportunities they predict between 15s and 1min 15s in the future **and** (2) MEV opportunities in real-time during the period that the participant is in control of the express lane that they didn’t otherwise predict in advance (proposed duration: 1 minute). Suppose the participant does not win control of the express lane. In that case, opportunities that they see in real-time are still exploitable, but with a 200ms delay, similar to all other transactions (since only the express lane controller’s transactions get sequenced with no delay).

#### What are the different variations of Timeboost?

Timeboost is implemented by modifying the sequencer to add an express lane and deploying an autonomous auctioneer service to facilitate the auction (sealed-bid, second-price) for temporary rights to control the express lane. Timeboost was designed and developed with decentralization in mind, including one that is compatible with decentralized sequencers (full specification [here](https://github.com/OffchainLabs/decentralized-timeboost-spec)).

#### Will Timeboost work with future decentralized Arbitrum sequencers?

Yes. Timeboost is compatible with both the current centralized sequencer and a future design that allows Arbitrum chains to benefit from a decentralized group of sequencers. The current approach allowed us to deliver Timeboost sooner rather than waiting until the decentralized sequencer design and implementation are complete. A full specification of Timeboost with decentralized sequencers can be found [here](https://github.com/OffchainLabs/decentralized-timeboost-spec)).

#### Will there be plans for a clean user interface that allows users to understand the logic of how their transactions get sorted, as well as an optional setting to adjust the sensitivity to the time factor?

For the first point about a more straightforward user interface, users can subscribe to the sequencer feed to view, in real time, the final order of transactions. Using the sequencer feed is a sufficient solution for helping users understand the logic behind how their transactions get sorted. Additional documentation and diagrams will be forthcoming to help illustrate this workflow.

To the second point, chain owners can adjust the amount of time that non-express lane transactions get delayed. This parameter, defined as `NonExpressDelayMsec', is denominated in milliseconds and is proposed to be 200ms initially.

#### How will Timeboost affect block time finality on Arbitrum chains? Does this mean that an Arbitrum chain’s new block time will be 450ms

Recall that Arbitrum chains have two types of finality: (1) a trusted or soft confirmation and (2) Ethereum-equivalent finality. A trusted or soft confirmation for a user’s transaction relies on the user trusting the sequencer and the near-instant transaction receipt issued by the sequencer, which takes approximately 250ms. For (2), the user can use the Ethereum-equivalent finality heuristic once their child chain transaction becomes finalized on the parent chain as part of a batch of transactions posted to Ethereum, which can take two epochs, or roughly 13 minutes, in today’s Proof-of-Stake Ethereum. Read more about these two types of finality [here](https://docs.arbitrum.io/how-arbitrum-works/tx-lifecycle).

With Timeboost, both finality timelines for non-express lane transactions (250ms for soft finality and ~13minutes for Ethereum-equivalent finality) will extend by the default 200ms delay proposed in Timeboost, which will be roughly ~450ms and ~13 minutes & 0.2 seconds for soft finality and Ethereum-equivalent finality, respectively.

For express lane transactions, there will be no impact on transaction finality, meaning that finality will remain at 250ms and ~13 minutes for soft finality and Etheruem-equivalent finality, respectively.

#### Is there a way to track the time (milliseconds, etc.) it takes for a transaction to be sent and received by the sequencer?

Yes! Measure the time between when you send your transaction and when you see it in the sequencer feed. Here, we assume that “accepted” refers to the point at which the sequencer has seen your transaction and gets processed into a block. This number is not uniformly consistent because different teams will have access to different hardware and setups, which may affect how quickly they can send messages over the public internet to the sequencer and also how quickly they can read the sequencer’s feed for state updates and transaction receipts.

#### Does gas have any effect on the transaction ordering in the sequencer?

Yes, because if your transaction did not provide enough gas, it might get rejected outright. If you specify insufficient gas, your transaction may be excluded from an upcoming block because it does not meet the network's requirements for processing, which include child chain execution and parent chain data posting costs.

#### Does Arbitrum support non-JSON formats for submitting transactions?

No.

#### How does the sequencer handle raw transaction requests that contain incorrect data, like an inconsistent nonce? For example, multiple transactions by same wallet with same nonce are submitted to the sequencer. Would there be any penalty for that wallet/IP address?

You should expect to get a nonce error. This behavior will work today, but this is considered abuse and we provide no guarantees on how this behavior will be treated in the future.

#### Does the sequencer have any rate limit? If it has, what is the limit, and is it per IP address or wallet?

Yes, but these limits are not published, and we don’t expect anyone to reach them. The limits are per IP address.

#### Is it recommended to send requests directly to the sequencer IP address(es) instead of the sequencer domain if we want the lowest latency?

No, it is not recommended.

#### Is there a more efficient way to track if our transaction has been accepted or rejected by the sequencer than listening to smart contract events or transaction counts?

Yes. We recommend that teams monitor and verify transaction receipts to obtain formal confirmation that their transaction gets included in a block.

#### For Timeboost, is there a limit on the number of transactions that can receive a boost from the winner in a round?

There is no transaction limit, but there is a block-based limit: if your Timeboosted transactions do not get sequenced within five Arbitrum blocks (1250ms) from the time that the sequencer received them, then they will get dropped. The 200ms Timeboost time advantage only lasts for 1000ms anyway, so this is a safe limit that people will not hit. Note that the block gas limit may be a reason why your transaction(s) doesn't get included in an Arbitrum block. More details on this limit can be found in this section of our docs [here](../timeboost/troubleshoot-timeboost.mdx)

#### If a transaction is Timeboosted and we assume it arrived in the current block creation period, which includes other non-express lane transactions, would that Timeboosted transaction queue in front of those other non-express lane transactions that have already arrived but not included yet?

It depends on the arrival timestamp of all the transactions. "Regular" transactions will receive a 200ms delay before being sequenced, while Timeboosted transactions will receive zero delay before being sequenced. In this scenario, if the "regular" transactions had arrived 200 milliseconds earlier than the Timeboosted transaction, they would get sequenced alongside the Timeboosted transaction based on their arrival timestamps. In this case, for that block, some "regular" transactions may indeed be ahead of Timeboosted transactions due to the timestamps. For additional information read this [document that explains how Timeboost works](../timeboost/gentle-introduction.mdx).

#### Apart from Timeboost, are there any other mechanisms or factors that can affect transaction priority or latency?

There are many factors, including how your infrastructure is set up and built, as well as where you are sending transactions from. Latency is something that top teams will optimize for, so spend time focusing on this to ensure you can maximize the benefits of Timeboost. The 200ms time advantage you receive from using Timeboost is likely more than enough to exploit the arbitrage opportunities onchain (ahead of competitors who are not using Timeboost).

#### Are there any recommendations for submitting transactions to Arbitrum with lower latency in addition to Timeboost?

Yeah! We recommend:

- Doing adequate testing of your infrastructure to optimize for the geographical latency considerations,
  A solid setup for bid submission if using Timeboost,
- Running your transaction pre-checker, Arbitrum full node, and a client to subscribe to the sequencer feed for the fastest on-chain updates, the ability to send transactions fast, robust nonce management, and
- Build a good observability and monitoring stack to watch and take action on events from the auction and also to review & process transaction receipts quickly to confirm behavior

---

## .mdx (timeboost/troubleshoot-timeboost.mdx)
---
title: 'Troubleshoot Timeboost'
description: A guide on common errors & best practices when using Timeboost
user_story: As a developer, I want to understand how to troubleshoot common issues when using Timeboost
author: dlee
content_type: how-to
---

This is a short guide on how response times work and how <a data-quicklook-from="express-lane">express lane</a> <a data-quicklook-from="transaction">transactions</a> are sequenced, alongside common errors and best practices for using <a data-quicklook-from="timeboost">Timeboost</a>. This guide assumes you have reviewed our guide on [How to use Timeboost](/how-arbitrum-works/timeboost/how-to-use-timeboost.mdx).

## How express lane transactions are ordered into blocks

The express lane time advantage is currently set to 200ms, while the current block creation time is 250ms. Both express lane transactions and regular transactions are processed together in a single queue after taking into account the timeboost time advantage and artificial delay. This means that if an express lane transaction and a normal transaction both arrive at the <a data-quicklook-from="sequencer">sequencer</a> at the same time, but _before 50ms have passed since the last block was produced_, then both transactions may appear in the same block, though the express lane transaction would be sequenced ahead of the normal transaction (assuming that the block's gas limit has not yet been reached).

Express lane transactions are processed in the order of their `sequenceNumber`, which is a field in every express lane transaction. The `sequenceNumber` field is important because transactions with `sequenceNumber = n` can only be sequenced after all the transactions from `sequenceNumber = 0` to `sequenceNumber = n-1` have been sequenced. The first expected sequence number for a new round is zero and increments for each accepted transaction.

There is a special "dontcare" sequence number (2^64 - 1) that can be used to indicate that you don't care about ordering of this express lane submission relative to others. Transactions with the "dontcare" sequence number will be sequenced right away without waiting for any other transactions. Note that normal nonce ordering within transactions for an account is still respected, so transactions from the same account will still be ordered by their nonce regardless of sequence number. The express lane controller can send `ExpressLaneSubmissions` with both "dontcare" and normal sequence numbers within the same round.

## How response times work

The response for a transaction submission to the express lane is returned immediately once received by the sequencer. For example, if an express lane transaction is sent to the sequencer at `t=0ms` and it took 50ms to arrive at the sequencer (defined as `time_to_arrive`), then the expected response time is at `t=50ms`. Note that an accepted transaction is defined as an express lane transaction submission where the sequencer returns an empty result with an HTTP status of `200` and will always have their `sequenceNumber` consumed. You can read more about how to submit express lane transactions in: [How to submit transactions to the express lane](/how-arbitrum-works/timeboost/how-to-use-timeboost.mdx#how-to-submit-transactions-to-the-express-lane).

## Errors relating to the `sequenceNumber`

When it comes to submitting express lane transactions, there are a few scenarios to consider. Note that if your use case doesn't require an ordering between `ExpressLaneSubmissions` beyond the usual per account nonce ordering, then you can use the "dontcare" sequence number described above.

### Scenario 1: You get an error response immediately

In this scenario, an error response is immediately returned after you send an express lane transaction. In most cases the transaction's `sequenceNumber` will not be consumed if the error is Timeboost-related or if the transaction was invalid (e.g., nonce too low, malformed transaction). If the error contains "Error queuing expressLane transaction" you need to re-submit your transaction with the same sequence number after rectifying any errors, or submit a different transaction with that sequence number. See [Common Timeboost error responses](#common-error-responses) below for a full list of Timeboost-related error responses and how to interpret them.

### Scenario 2: Your transaction got an empty response with an HTTP status of `200`

In this scenario, a `null` response is immediately returned after you send an express lane transaction. This means that your transaction's `sequenceNumber` was consumed and your transaction was accepted. However, this does not mean that your transaction was sequenced into a block due to a block-based timeout explained below. We recommend checking transaction receipts for confirmation on whether your transactions were sequenced into a block or not.

#### The block-based timeout for express lane transactions

If the express lane controller decides to send a burst of transactions to the express lane with ascending values for the `sequenceNumber`, then the sequencer will attempt to process them in the order defined by the `sequenceNumber` (as explained above). However, if the transactions arrive out-of-order at the sequencer, then the transactions that do not have the expected `sequenceNumber` will be buffered (up to a limit) to be processed until the sequencer receives the transaction with the expected `sequenceNumber`. Once the sequencer receives the transaction with the expected `sequenceNumber`, then the sequencer will begin processing the buffered transaction with the next `sequenceNumber`. In other words, a transaction will only be sequenced into a block once transactions with the other, missing sequence numbers arrive to fill in the “gap” between the expected `sequencerNumber` and a given transaction’s `sequenceNumber`.

A block-based timeout is applied to all express lane transactions, even those in the buffer, such that any transactions accepted (meaning `sequenceNumber` is consumed) by the sequencer will be dropped if they are not sequenced into a block within 5 blocks. This timeout can occur if the cummulative gas usage of transactions (express lane or otherwise) fill up 5 blocks worth of transactions _before_ all of the buffered express lane transactions are sequenced. No timeout error will be returned in this case and we recommend checking transaction receipts for confirmation on whether your transactions were sequenced into a block or not. Note that each Arbitrum block has a gas limit of 32 million gas and 1 Arbitrum block is produced every 250ms. This block-based timeout is likely to be reached before the limit on buffered transactions is hit in almost all cases.

## Common error responses

The below two tables can also be found on our guide on [How to use Timeboost](/how-arbitrum-works/timeboost/how-to-use-timeboost.mdx).

#### Table 1: Errors relating to bid submission

| Error                   | Description                                                                                                 |
| ----------------------- | ----------------------------------------------------------------------------------------------------------- |
| `MALFORMED_DATA`        | wrong input data, failed to deserialize, missing certain fields, etc.                                       |
| `NOT_DEPOSITOR`         | the address is not an active depositor in the auction contract                                              |
| `WRONG_CHAIN_ID`        | wrong chain id for the target chain                                                                         |
| `WRONG_SIGNATURE`       | signature failed to verify                                                                                  |
| `BAD_ROUND_NUMBER`      | incorrect round, such as one from the past                                                                  |
| `RESERVE_PRICE_NOT_MET` | bid amount does not meet the minimum required reserve price on-chain                                        |
| `INSUFFICIENT_BALANCE`  | the bid amount specified in the request is higher than the deposit balance of the depositor in the contract |

#### Table 2: Errors relating to express lane transaction submission

Note that if you get any of the errors below or errors related to invalidity of the transaction (e.g., nonce too low, malformed transaction), then the sequence number used in your express lane transaction was _not_ consumed.
| Error | Description |
| ------------------------------ | --------------------------------------------------------------------- |
| `MALFORMED_DATA` | wrong input data, failed to deserialize, missing certain fields, etc. |
| `WRONG_CHAIN_ID` | wrong chain id for the target chain |
| `WRONG_SIGNATURE` | signature failed to verify |
| `BAD_ROUND_NUMBER` | incorrect round, such as one from the past |
| `NOT_EXPRESS_LANE_CONTROLLER` | the sender is not the express lane controller |
| `NO_ONCHAIN_CONTROLLER` | there is no defined, on-chain express lane controller for the round |
| `SEQUENCE_NUMBER_ALREADY_SEEN` | the sequence number used for the given transaction was already consumed, try resubmitting with a new sequence number |
| `SEQUENCE_NUMBER_TOO_LOW` | the sequencer number used for the given transaction is numerically lower than the expeected sequence number, try resubmitting with the expected sequence number
| `sequence number has reached max allowed limit` | the limit on the number of buffered express lane transactions was reached |

## Notes on Timeboost's implementation for Arbitrum One & Arbitrum Nova

Although the auctioneer will function autonomously, please note that the ArbitrumDAO (formal owners of Arbitrum One and Arbitrum Nova) has granted the sequencer operator with:

1. The right to pause the acceptance and verification of bids. This is to allow the current sequencer operator to provide reliable, consistent UX and maximize infrastructure stability, and
2. The right to disable Timeboost entirely in the event of a security risk or otherwise malicious attempt to harm Arbitrum One and Arbitrum Nova node operators, existing deployed applications, and/or end users. The Arbitrum Foundation and Offchain Labs commits to sharing publicly post-mortems and analyses should this scenario arise.

These rights, among a few others as described in the [original AIP](https://forum.arbitrum.foundation/t/constitutional-aip-proposal-to-adopt-timeboost-a-new-transaction-ordering-policy/25167), are expected to only be exercised in circumstances where doing so would enhance Timeboost’s long-term stability, preserve or improve the user experience for those using Timeboost-enabled Arbitrum chains, increase the security posture, resiliency, or stability of the chain, and/or otherwise help increase revenue for the ArbitrumDAO.

It is important to emphasize that for Arbitrum One and Arbitrum Nova, the DAO-elected Arbitrum Security Council can, at any time, perform either Emergency Actions or Non-Emergency Actions to execute software upgrades, perform routine maintenance, and other parameter adjustments to Timeboost, in each case in accordance with its existing powers. These actions can include, but are not limited solely to, exercising the rights proposed above for the current sequencer operator. More information about the Arbitrum Security Council and their scope of powers can be found in the [ArbitrumDAO Constitution](https://docs.arbitrum.foundation/dao-constitution).

---

